<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbstractCore Troubleshooting Guide - AbstractCore</title>
    <meta name="description" content="Complete troubleshooting guide for AbstractCore core library and server, including common mistakes and how to avoid them.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">AbstractCore Troubleshooting Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">Complete troubleshooting guide for AbstractCore core library and server, including common mistakes and how to avoid them.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#table-of-contents" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Table of Contents</a>
<a href="#common-mistakes-to-avoid" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Common Mistakes to Avoid</a>
<a href="#quick-diagnosis" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Diagnosis</a>
<a href="#installation-issues" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Installation Issues</a>
<a href="#core-library-issues" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Library Issues</a>
<a href="#server-issues" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Server Issues</a>
<a href="#provider-specific-issues" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Provider-Specific Issues</a>
<a href="#performance-issues" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Performance Issues</a>
<a href="#best-practices" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Best Practices</a>
<a href="#debug-techniques" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Debug Techniques</a>
<a href="#common-error-messages" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Common Error Messages</a>
<a href="#getting-help" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Getting Help</a></div>

            <div class="doc-content">


<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#common-mistakes-to-avoid">Common Mistakes to Avoid</a></li>
<li><a href="#quick-diagnosis">Quick Diagnosis</a></li>
<li><a href="#installation-issues">Installation Issues</a></li>
<li><a href="#core-library-issues">Core Library Issues</a></li>
<li><a href="#server-issues">Server Issues</a></li>
<li><a href="#provider-specific-issues">Provider-Specific Issues</a></li>
<li><a href="#performance-issues">Performance Issues</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#debug-techniques">Debug Techniques</a></li>
</ul>
<hr/>
<h2 id="common-mistakes-to-avoid">Common Mistakes to Avoid</h2>
<p>Understanding common pitfalls helps prevent issues before they occur.</p>
<h3 id="top-3-critical-mistakes">Top 3 Critical Mistakes</h3>
<ol>
<li>
<p><strong>üîë Incorrect Provider Configuration</strong>
   - <em>Symptom</em>: Authentication failures, no model response
   - <em>Quick Fix</em>: Always set API keys as environment variables
   - See: <a href="#issue-authentication-errors">Authentication Errors</a></p>
</li>
<li>
<p><strong>üß© Mishandling Tool Calls</strong>
   - <em>Symptom</em>: Tools not executing, streaming interruptions
   - <em>Quick Fix</em>: Use <code>@tool</code> decorator and handle tool calls properly
   - See: <a href="#issue-tool-calls-not-working">Tool Calls Not Working</a></p>
</li>
<li>
<p><strong>üíª Provider Dependency Confusion</strong>
   - <em>Symptom</em>: <code>ModuleNotFoundError</code> for providers
   - <em>Quick Fix</em>: Install provider-specific packages with <code>pip install "abstractcore[provider]"</code>
   - See: <a href="#issue-modulenotfounderror">ModuleNotFoundError</a></p>
</li>
<li>
<p><strong>üñ•Ô∏è LM Studio Server Not Enabled</strong>
   - <em>Symptom</em>: Connection refused, no response from LM Studio
   - <em>Quick Fix</em>: Enable "Status: Running" toggle in LM Studio GUI
   - See: <a href="#issue-lm-studio-server-not-enabled">LM Studio Server Not Enabled</a></p>
</li>
<li>
<p><strong>üìè Context Length Too Small (LM Studio/Ollama)</strong>
   - <em>Symptom</em>: 400 Bad Request, truncated responses, errors with long inputs
   - <em>Quick Fix</em>: Set "Default Context Length" to "Model Maximum" in LM Studio
   - See: <a href="#issue-context-length-too-small-400-bad-request-truncated-responses">Context Length Too Small</a></p>
</li>
</ol>
<h3 id="common-mistake-patterns">Common Mistake Patterns</h3>
<h4 id="mistake-missing-or-incorrect-api-keys">Mistake: Missing or Incorrect API Keys</h4>
<p><strong>You'll See:</strong>
- <code>ProviderAPIError: Authentication failed</code>
- No response from the model
- Cryptic error messages about credentials</p>
<p><strong>Why This Happens:</strong>
- API keys not set as environment variables
- Whitespace or copying errors in key
- Incorrect key permissions or expired credentials</p>
<p><strong>Solution:</strong> See <a href="#issue-authentication-errors">Authentication Errors</a> for complete fix.</p>
<p><strong>Prevention:</strong>
- Use environment variables for sensitive credentials
- Store keys in <code>.env</code> files (add to <code>.gitignore</code>)
- Regularly rotate and update API keys
- Use secret management tools for production</p>
<h4 id="mistake-incorrect-tool-call-handling">Mistake: Incorrect Tool Call Handling</h4>
<p><strong>You'll See:</strong>
- Tools not executing during generation
- Partial or missing tool call results
- Streaming interruptions</p>
<p><strong>Why This Happens:</strong>
- Not using <code>@tool</code> decorator
- Incorrect tool definition format
- Not handling tool responses</p>
<p><strong>Solution:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

# Use @tool decorator for automatic tool definition
@tool
def get_weather(city: str) -&gt; str:
    """Get current weather for a city."""
    return f"Weather in {city}: sunny, 72¬∞F"

llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate(
    "What's the weather in Tokyo?",
    tools=[get_weather]  # Pass decorated function directly
)
</code></pre></div>
<p><strong>Prevention:</strong>
- Always use <code>@tool</code> decorator for automatic tool definitions
- Use type hints for all parameters
- Add clear docstrings for tool descriptions
- Handle tool execution errors gracefully
- See: <a href="#issue-tool-calls-not-working">Tool Calls Not Working</a></p>
<h4 id="mistake-overlooking-error-handling">Mistake: Overlooking Error Handling</h4>
<p><strong>You'll See:</strong>
- Unhandled exceptions
- Silent failures in tool or generation calls
- Unexpected application crashes</p>
<p><strong>Why This Happens:</strong>
- Not catching provider-specific exceptions
- Assuming 100% reliability of LLM responses
- No retry or fallback mechanisms</p>
<p><strong>Solution:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.exceptions import ProviderAPIError, RateLimitError

providers = [
    ("openai", "gpt-4o-mini"),
    ("anthropic", "claude-haiku-4-5"),
    ("ollama", "qwen3-coder:30b")
]

def generate_with_fallback(prompt):
    for provider, model in providers:
        try:
            llm = create_llm(provider, model=model)
            return llm.generate(prompt)
        except (ProviderAPIError, RateLimitError) as e:
            print(f"Failed with {provider}: {e}")
            continue
    raise Exception("All providers failed")
</code></pre></div>
<p><strong>Prevention:</strong>
- Always use try/except blocks
- Implement provider fallback strategies
- Log and monitor errors systematically
- Design for graceful degradation</p>
<h4 id="mistake-memory-and-performance-bottlenecks">Mistake: Memory and Performance Bottlenecks</h4>
<p><strong>You'll See:</strong>
- High memory consumption
- Slow response times
- Out-of-memory errors during long generations</p>
<p><strong>Why This Happens:</strong>
- Not managing token limits
- Generating overly long responses
- Inefficient streaming configurations</p>
<p><strong>Solution:</strong></p>
<div class="code-block"><pre><code class="language-python"># Optimize memory and performance
response = llm.generate(
    "Complex task",
    max_tokens=1000,  # Limit response length
    timeout=30,       # Set reasonable timeout
    temperature=0.7   # Control creativity/randomness
)
</code></pre></div>
<p><strong>Prevention:</strong>
- Always set <code>max_tokens</code>
- Use streaming for long responses
- Monitor memory usage in production
- See: <a href="#performance-issues">Performance Issues</a></p>
<h4 id="mistake-hardcoding-credentials">Mistake: Hardcoding Credentials</h4>
<p><strong>You'll See:</strong>
- Exposed API keys in code
- Inflexible configuration management
- Security vulnerabilities</p>
<p><strong>Why This Happens:</strong>
- Copying example code directly
- Not understanding configuration best practices
- Lack of environment-based configuration</p>
<p><strong>Solution:</strong></p>
<div class="code-block"><pre><code class="language-python">import os
from abstractcore import create_llm

# Best practice: Load from environment
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
DEFAULT_MODEL = os.getenv('DEFAULT_LLM_MODEL', 'gpt-4o-mini')

llm = create_llm(
    "openai",
    model=DEFAULT_MODEL,
    api_key=OPENAI_API_KEY
)
</code></pre></div>
<p><strong>Prevention:</strong>
- Never hardcode API keys or sensitive data
- Use environment variables
- Implement configuration management libraries
- Follow 12-factor app configuration principles</p>
<hr/>
<h2 id="quick-diagnosis">Quick Diagnosis</h2>
<p>Run these checks first:</p>
<div class="code-block"><pre><code class="language-bash"># Check Python version
python --version  # Should be 3.9+

# Check AbstractCore installation
pip show abstractcore

# Test core library
python -c "from abstractcore import create_llm; print('‚úì Core library OK')"

# Test server (if installed)
curl http://localhost:8000/health  # Should return {"status":"healthy"}
</code></pre></div>
<hr/>
<h2 id="installation-issues">Installation Issues</h2>
<h3 id="issue-modulenotfounderror">Issue: ModuleNotFoundError</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>ModuleNotFoundError: No module named .abstractcore.
ModuleNotFoundError: No module named 'openai'
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Install AbstractCore
pip install abstractcore

# Install with specific provider
pip install "abstractcore[openai]"
pip install "abstractcore[anthropic]"
# Local OpenAI-compatible servers (Ollama, LMStudio, vLLM, llama.cpp, ...) work with the core install.

# Install the full feature set (pick one)
pip install "abstractcore[all-apple]"    # macOS/Apple Silicon (includes MLX, excludes vLLM)
pip install "abstractcore[all-non-mlx]"  # Linux/Windows/Intel Mac (excludes MLX and vLLM)
pip install "abstractcore[all-gpu]"      # Linux NVIDIA GPU (includes vLLM, excludes MLX)

# Verify installation
pip list | grep abstract
</code></pre></div>
<h3 id="issue-dependency-conflicts">Issue: Dependency Conflicts</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>ERROR: pip's dependency resolver does not currently take into account all the packages...
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Create clean environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# OR
.venv\Scripts\activate  # Windows

# Fresh install
pip install --upgrade pip
pip install "abstractcore[all-apple]"    # macOS/Apple Silicon
# or: pip install "abstractcore[all-non-mlx]"  # Linux/Windows/Intel Mac
# or: pip install "abstractcore[all-gpu]"      # Linux NVIDIA GPU

# If still failing, try one provider at a time
pip install "abstractcore[openai]"
</code></pre></div>
<hr/>
<h2 id="core-library-issues">Core Library Issues</h2>
<h3 id="issue-authentication-errors">Issue: Authentication Errors</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>Error: OpenAI API key not found
Error: Authentication failed
Error: Invalid API key
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check if API key is set
echo $OPENAI_API_KEY  # Should show your key
echo $ANTHROPIC_API_KEY

# Set API key
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# Add to shell profile for persistence
echo 'export OPENAI_API_KEY="sk-..."' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Verify key format
# OpenAI: starts with "sk-"
# Anthropic: starts with "sk-ant-"

# Test authentication
python -c "from abstractcore import create_llm; llm = create_llm('openai', model='gpt-4o-mini'); print(llm.generate('test').content)"
</code></pre></div>
<h3 id="issue-model-not-found">Issue: Model Not Found</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>Error: Model 'qwen3-coder:30b' not found
Error: Unsupported model
</code></pre></div>
<p><strong>Solutions:</strong></p>
<p><strong>For Ollama:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check available models
ollama list

# Pull missing model
ollama pull qwen3-coder:30b

# Verify Ollama is running
ollama serve
</code></pre></div>
<p><strong>For LMStudio:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check LMStudio server
curl http://localhost:1234/v1/models

# In LMStudio GUI:
# 1. Go to "Local Server" tab
# 2. Select model from dropdown
# 3. Click "Start Server"
</code></pre></div>
<p><strong>For OpenAI/Anthropic:</strong></p>
<div class="code-block"><pre><code class="language-python"># Use correct model names
llm = create_llm("openai", model="gpt-4o-mini")  # ‚úì Correct
llm = create_llm("openai", model="gpt4")  # ‚úó Wrong

llm = create_llm("anthropic", model="claude-haiku-4-5")  # ‚úì Correct
llm = create_llm("anthropic", model="claude-3")  # ‚úó Wrong
</code></pre></div>
<h3 id="issue-connection-errors">Issue: Connection Errors</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>Connection refused
Timeout error
Network error
</code></pre></div>
<p><strong>Solutions:</strong></p>
<p><strong>For Ollama:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Start Ollama service
ollama serve

# Check if running
curl http://localhost:11434/api/tags

# If using custom host
export OLLAMA_HOST="http://localhost:11434"
</code></pre></div>
<p><strong>For LMStudio:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Verify server is running
curl http://localhost:1234/v1/models

# Check port in LMStudio GUI (usually 1234)
</code></pre></div>
<p><strong>For Cloud Providers:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Test network connection
ping api.openai.com
ping api.anthropic.com

# Check proxy settings
echo $HTTP_PROXY
echo $HTTPS_PROXY

# Disable proxy if needed
unset HTTP_PROXY
unset HTTPS_PROXY
</code></pre></div>
<h3 id="issue-tool-calls-not-working">Issue: Tool Calls Not Working</h3>
<p><strong>Symptoms:</strong>
- Tools not being called
- Empty tool responses
- Tool format errors</p>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

# Ensure @tool decorator is used
@tool
def get_weather(city: str) -&gt; str:
    """Get weather for a city."""
    return f"Weather in {city}: sunny, 72¬∞F"

# Use tool correctly
llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate(
    "What's the weather in Paris?",
    tools=[get_weather]  # Pass as list
)

# Check if tool was called
if hasattr(response, 'tool_calls') and response.tool_calls:
    print("Tools were called")
</code></pre></div>
<hr/>
<h2 id="server-issues">Server Issues</h2>
<h3 id="issue-server-wont-start">Issue: Server Won't Start</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>Address already in use
Port 8000 is already allocated
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check what's using port 8000
lsof -i :8000  # Linux/Mac
netstat -ano | findstr :8000  # Windows

# Kill process on port
kill -9 $(lsof -t -i:8000)  # Linux/Mac

# Use different port
uvicorn abstractcore.server.app:app --port 3000
</code></pre></div>
<h3 id="issue-abstractcoreapikey-error">Issue: ABSTRACTCORE_API_KEY Error</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>Error: Missing environment variable: ABSTRACTCORE_API_KEY
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Set the required variable
export ABSTRACTCORE_API_KEY="unused"

# For Codex CLI, set ALL three:
export OPENAI_BASE_URL="http://localhost:8000/v1"
export OPENAI_API_KEY="unused"
export ABSTRACTCORE_API_KEY="unused"

# Verify they're set
echo $OPENAI_BASE_URL
echo $OPENAI_API_KEY
echo $ABSTRACTCORE_API_KEY
</code></pre></div>
<h3 id="issue-server-running-but-no-response">Issue: Server Running but No Response</h3>
<p><strong>Symptoms:</strong>
- curl hangs
- No response from endpoints
- Timeout errors</p>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check server is actually running
curl http://localhost:8000/health

# Check server logs
tail -f logs/abstractcore_*.log

# Enable debug mode
export ABSTRACTCORE_DEBUG=true
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# Test with simple request
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "openai/gpt-4o-mini", "messages": [{"role": "user", "content": "test"}]}'
</code></pre></div>
<h3 id="issue-models-not-showing">Issue: Models Not Showing</h3>
<p><strong>Symptoms:</strong></p>
<div class="code-block"><pre><code>curl http://localhost:8000/v1/models returns empty list
</code></pre></div>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check if providers are configured
curl http://localhost:8000/providers

# Verify provider setup:

# For Ollama
ollama list  # Should show models
ollama serve  # Make sure it's running

# For OpenAI
echo $OPENAI_API_KEY  # Should be set

# For Anthropic
echo $ANTHROPIC_API_KEY  # Should be set

# For LMStudio
curl http://localhost:1234/v1/models  # Should return models
</code></pre></div>
<h3 id="issue-tool-calls-not-working-with-cli">Issue: Tool Calls Not Working with CLI</h3>
<p><strong>Symptoms:</strong>
- Codex/Crush/Gemini CLI not detecting tools
- Tool format errors in streaming</p>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Set correct tool format for your CLI

# For Codex CLI (qwen3 format - default)
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# For Crush CLI (llama3 format)
export ABSTRACTCORE_DEFAULT_TOOL_CALL_TAGS=llama3
export ABSTRACTCORE_DEFAULT_EXECUTE_TOOLS=false
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# For Gemini CLI (xml format)
export ABSTRACTCORE_DEFAULT_TOOL_CALL_TAGS=xml
export ABSTRACTCORE_DEFAULT_EXECUTE_TOOLS=false
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# Restart server after changing environment variables
pkill -f "abstractcore.server.app"
</code></pre></div>
<hr/>
<h2 id="provider-specific-issues">Provider-Specific Issues</h2>
<h3 id="ollama">Ollama</h3>
<p><strong>Issue: Ollama not responding</strong></p>
<div class="code-block"><pre><code class="language-bash"># Restart Ollama
pkill ollama
ollama serve

# Check status
curl http://localhost:11434/api/tags

# List models
ollama list

# Pull model if missing
ollama pull qwen3-coder:30b
</code></pre></div>
<p><strong>Issue: Out of memory</strong></p>
<div class="code-block"><pre><code class="language-bash"># Use smaller models
ollama pull gemma3:1b  # Only 1GB
ollama pull qwen3:4b-instruct-2507-q4_K_M  # 4GB

# Check system memory
free -h  # Linux
vm_stat  # macOS

# Close other applications
</code></pre></div>
<h3 id="openai">OpenAI</h3>
<p><strong>Issue: Rate limits</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check your rate limits
# https://platform.openai.com/account/rate-limits

# Implement backoff in code
import time
try:
    response = llm.generate("prompt")
except RateLimitError:
    time.sleep(20)  # Wait before retry
</code></pre></div>
<p><strong>Issue: Billing</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check billing dashboard
# https://platform.openai.com/account/billing

# Verify payment method is added
# Check usage limits aren't exceeded
</code></pre></div>
<h3 id="anthropic">Anthropic</h3>
<p><strong>Issue: API key format</strong></p>
<div class="code-block"><pre><code class="language-bash"># Anthropic keys start with "sk-ant-"
echo $ANTHROPIC_API_KEY  # Should start with sk-ant-

# Get key from console
# https://console.anthropic.com/
</code></pre></div>
<h3 id="lmstudio">LMStudio</h3>
<h4 id="issue-connection-refused">Issue: Connection refused</h4>
<div class="code-block"><pre><code class="language-bash"># Verify LMStudio server is running
# Check LMStudio GUI shows "Server running"

# Test connection
curl http://localhost:1234/v1/models

# Check port number in LMStudio (usually 1234)
</code></pre></div>
<h4 id="issue-lm-studio-server-not-enabled">Issue: LM Studio Server Not Enabled</h4>
<div class="code-block"><pre><code class="language-bash"># CRITICAL: Ensure LM Studio server is enabled in the GUI
# 1. Open LM Studio application
# 2. Look for "Status: Running" toggle switch in the interface
# 3. Make sure the toggle is switched to "ON" (green background, white handle on right)
# 4. If the toggle shows "OFF", click it to enable the server
# 5. Verify the server is running by checking the status indicator

# Test server availability
curl http://localhost:1234/v1/models

# If still failing, check LM Studio logs for any error messages
</code></pre></div>
<h4 id="issue-context-length-too-small-400-bad-request-truncated-responses">Issue: Context Length Too Small (400 Bad Request, Truncated Responses)</h4>
<div class="code-block"><pre><code class="language-bash"># Problem: LLM returns 400 Bad Request, truncated output, or errors with long inputs
# Root Cause: Insufficient context length configured for the model or server

# Solution 1: Increase Default Context Length (RECOMMENDED)
# This is the most robust way to ensure all models use maximum available context
# 1. Open LM Studio application
# 2. Go to "App Settings" ‚Üí "General" tab
# 3. Find "Model Defaults" ‚Üí "Default Context Length"
# 4. Set dropdown to "Model Maximum" (or highest available value like 131072)
# 5. Restart LM Studio server for changes to take effect

# Solution 2: Increase Context Length per Model (Alternative)
# This method applies context length setting to a specific model
# 1. Open LM Studio application
# 2. Go to "My Models" tab
# 3. Select the specific model you are using
# 4. Look for "Context Length" slider/input (usually under "Load" or "Context" tab)
# 5. Adjust slider to maximum value (e.g., 131072 tokens)
# 6. Reload the model for changes to take effect

# Solution 3: Increase Context Length via API Request (Advanced)
# For Ollama, or if you need to override settings for LM Studio via API
# For Ollama:
ollama run &lt;model_name&gt; -c &lt;context_length&gt;
# Example: ollama run llama2 -c 4096

# For LM Studio via API (often handled automatically by AbstractCore):
# Include in request payload:
# {
#   "model": "your-model-name",
#   "prompt": "Your long prompt here...",
#   "options": {
#     "num_ctx": 4096  # Or your desired context length
#   }
# }

# Verification:
# After adjusting, test with a long prompt that previously failed
# Check server logs for any warnings or errors related to context
</code></pre></div>
<hr/>
<h2 id="performance-issues">Performance Issues</h2>
<h3 id="issue-slow-responses">Issue: Slow Responses</h3>
<p><strong>Diagnosis:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Time a request
time python -c "from abstractcore import create_llm; llm = create_llm('ollama', model='qwen3:4b-instruct-2507-q4_K_M'); print(llm.generate('test').content)"
</code></pre></div>
<p><strong>Solutions:</strong></p>
<p><strong>Use Faster Models:</strong></p>
<div class="code-block"><pre><code class="language-python"># Faster cloud models
llm = create_llm("openai", model="gpt-4o-mini")  # Fast
llm = create_llm("anthropic", model="claude-haiku-4-5")  # Fast

# Faster local models
llm = create_llm("ollama", model="gemma3:1b")  # Very fast
llm = create_llm("ollama", model="qwen3:4b-instruct-2507-q4_K_M")  # Balanced
</code></pre></div>
<p><strong>Enable Streaming:</strong></p>
<div class="code-block"><pre><code class="language-python"># Improves perceived speed
for chunk in llm.generate("Long response", stream=True):
    print(chunk.content, end="", flush=True)
</code></pre></div>
<p><strong>Optimize Parameters:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(
    "prompt",
    max_tokens=500,      # Limit length
    temperature=0.3      # Lower = faster
)
</code></pre></div>
<h3 id="issue-high-memory-usage">Issue: High Memory Usage</h3>
<p><strong>Solutions:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Use smaller models
ollama pull gemma3:1b  # 1GB instead of 30GB

# Close other applications

# For MLX on Mac
# Use 4-bit quantized models
llm = create_llm("mlx", model="mlx-community/Llama-3.2-3B-Instruct-4bit")
</code></pre></div>
<hr/>
<h2 id="best-practices">Best Practices</h2>
<p>Follow these best practices to avoid issues:</p>
<h3 id="configuration-management">Configuration Management</h3>
<ul>
<li>Use environment variables for API keys</li>
<li>Never commit credentials to version control</li>
<li>Use <code>.env</code> files (add to <code>.gitignore</code>)</li>
<li>Implement configuration validation</li>
<li>Use secret management in production</li>
</ul>
<h3 id="tool-development">Tool Development</h3>
<ul>
<li>Always use <code>@tool</code> decorator</li>
<li>Add type hints to all parameters</li>
<li>Write clear docstrings</li>
<li>Handle edge cases and errors</li>
<li>Test tools independently first</li>
</ul>
<h3 id="error-handling">Error Handling</h3>
<ul>
<li>Always use try/except blocks</li>
<li>Implement provider fallback strategies</li>
<li>Log errors systematically</li>
<li>Design for graceful degradation</li>
<li>Monitor error rates in production</li>
</ul>
<h3 id="performance">Performance</h3>
<ul>
<li>Always set <code>max_tokens</code></li>
<li>Use streaming for long responses</li>
<li>Batch similar requests when possible</li>
<li>Monitor memory usage</li>
<li>Profile slow operations</li>
</ul>
<h3 id="security">Security</h3>
<ul>
<li>Validate all user inputs</li>
<li>Sanitize file paths and commands</li>
<li>Use least privilege principle</li>
<li>Regular security audits</li>
<li>Keep dependencies updated</li>
</ul>
<hr/>
<h2 id="debug-techniques">Debug Techniques</h2>
<h3 id="enable-debug-logging">Enable Debug Logging</h3>
<p><strong>Core Library:</strong></p>
<div class="code-block"><pre><code class="language-python">import logging
logging.basicConfig(level=logging.DEBUG)

from abstractcore import create_llm
llm = create_llm("openai", model="gpt-4o-mini")
</code></pre></div>
<p><strong>Server:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Enable debug mode
export ABSTRACTCORE_DEBUG=true

# Start with debug logging
uvicorn abstractcore.server.app:app --log-level debug

# Monitor logs
tail -f logs/abstractcore_*.log
</code></pre></div>
<h3 id="analyze-logs">Analyze Logs</h3>
<div class="code-block"><pre><code class="language-bash"># Find errors
grep '"level": "error"' logs/abstractcore_*.log

# Track specific request
grep "req_abc123" logs/abstractcore_*.log

# Monitor latency
cat logs/verbatim_*.jsonl | jq '.metadata.latency_ms'

# Token usage
cat logs/verbatim_*.jsonl | jq '.metadata.tokens | .input + .output' | \
  awk '{sum+=$1} END {print "Total:", sum}'
</code></pre></div>
<h3 id="test-in-isolation">Test in Isolation</h3>
<div class="code-block"><pre><code class="language-python"># Test provider directly
from abstractcore import create_llm

try:
    llm = create_llm("openai", model="gpt-4o-mini")
    response = llm.generate("Hello")
    print(f"‚úì Success: {response.content}")
except Exception as e:
    print(f"‚úó Error: {e}")
</code></pre></div>
<h3 id="collect-debug-information">Collect Debug Information</h3>
<div class="code-block"><pre><code class="language-bash"># Create debug report
echo "=== System ===" &gt; debug_report.txt
uname -a &gt;&gt; debug_report.txt
python --version &gt;&gt; debug_report.txt

echo "=== Packages ===" &gt;&gt; debug_report.txt
pip freeze | grep -E "abstract|openai|anthropic" &gt;&gt; debug_report.txt

echo "=== Environment ===" &gt;&gt; debug_report.txt
env | grep -E "ABSTRACT|OPENAI|ANTHROPIC|OLLAMA" &gt;&gt; debug_report.txt

echo "=== Tests ===" &gt;&gt; debug_report.txt
python -c "from abstractcore import create_llm; print('Core library: OK')" &gt;&gt; debug_report.txt 2&gt;&amp;1
curl http://localhost:8000/health &gt;&gt; debug_report.txt 2&gt;&amp;1

cat debug_report.txt
</code></pre></div>
<hr/>
<h2 id="common-error-messages">Common Error Messages</h2>
<table>
<thead>
<tr>
<th>Error</th>
<th>Meaning</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ModuleNotFoundError</code></td>
<td>Package not installed</td>
<td><code>pip install abstractcore</code> (then add provider extras as needed)</td>
</tr>
<tr>
<td><code>Authentication Error</code></td>
<td>Invalid API key</td>
<td>Check API key environment variable</td>
</tr>
<tr>
<td><code>Connection refused</code></td>
<td>Service not running</td>
<td>Start Ollama/LMStudio/server</td>
</tr>
<tr>
<td><code>LM Studio connection failed</code></td>
<td>LM Studio server not enabled</td>
<td>Enable "Status: Running" toggle in LM Studio GUI</td>
</tr>
<tr>
<td><code>400 Bad Request</code> (LM Studio)</td>
<td>Context length too small</td>
<td>Increase Default Context Length to "Model Maximum" in LM Studio</td>
</tr>
<tr>
<td><code>Model not found</code></td>
<td>Model unavailable</td>
<td>Pull model or check name</td>
</tr>
<tr>
<td><code>Rate limit exceeded</code></td>
<td>Too many requests</td>
<td>Wait or upgrade plan</td>
</tr>
<tr>
<td><code>Timeout</code></td>
<td>Request took too long</td>
<td>Use smaller model or increase timeout</td>
</tr>
<tr>
<td><code>Out of memory</code></td>
<td>Insufficient RAM</td>
<td>Use smaller model</td>
</tr>
<tr>
<td><code>Port already in use</code></td>
<td>Another process using port</td>
<td>Kill process or use different port</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="getting-help">Getting Help</h2>
<p>If you're still stuck:</p>
<ol>
<li>
<p><strong>Check Documentation:</strong>
   - <a href="getting-started.html">Getting Started</a> - Core library quick start
   - <a href="prerequisites.html">Prerequisites</a> - Provider setup
   - <a href="api-reference.html">Python API Reference</a> - Core library API
   - <a href="server.html">Server Guide</a> - Server setup
   - <a href="server.html">Server API Reference</a> - REST API endpoints</p>
</li>
<li>
<p><strong>Enable Debug Mode:</strong>
<code>bash
   export ABSTRACTCORE_DEBUG=true</code></p>
</li>
<li>
<p><strong>Collect Information:</strong>
   - Error messages
   - Debug logs
   - System information
   - Steps to reproduce</p>
</li>
<li>
<p><strong>Community Support:</strong>
   - GitHub Issues: <a href="https://github.com/lpalbou/AbstractCore/issues">github.com/lpalbou/AbstractCore/issues</a>
   - GitHub Discussions: <a href="https://github.com/lpalbou/AbstractCore/discussions">github.com/lpalbou/AbstractCore/discussions</a></p>
</li>
</ol>
<hr/>
<p><strong>Remember</strong>: Most issues are configuration-related. Double-check environment variables, API keys, and that services are running before diving deep into debugging.</p>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
