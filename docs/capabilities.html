<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbstractCore Capabilities - AbstractCore</title>
    <meta name="description" content="This document clearly explains what AbstractCore can and cannot do , helping you understand when to use it and when to look elsewhere.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">AbstractCore Capabilities</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">This document clearly explains what AbstractCore can and cannot do , helping you understand when to use it and when to look elsewhere.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#what-abstractcore-is" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">What AbstractCore IS</a>
<a href="#optional-capability-plugins-voiceaudiovision" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Optional capability plugins (voice/audio/vision)</a>
<a href="#what-abstractcore-does-well" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">What AbstractCore Does Well</a>
<a href="#what-abstractcore-does-not-do" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">What AbstractCore Does NOT Do</a>
<a href="#when-to-choose-abstractcore" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">When to Choose AbstractCore</a>
<a href="#abstractcore-in-the-ecosystem" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">AbstractCore in the Ecosystem</a>
<a href="#decision-tree" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Decision Tree</a>
<a href="#capabilities-summary" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Capabilities Summary</a>
<a href="#next-steps" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Next Steps</a></div>

            <div class="doc-content">


<h2 id="what-abstractcore-is">What AbstractCore IS</h2>
<p>AbstractCore is <strong>production-ready LLM infrastructure</strong>. It provides a unified, reliable interface to language models with essential features built-in.</p>
<h3 id="core-philosophy">Core Philosophy</h3>
<ul>
<li><strong>Infrastructure, not application logic</strong></li>
<li><strong>Reliability over features</strong></li>
<li><strong>Simplicity over complexity</strong></li>
<li><strong>Provider agnostic</strong></li>
</ul>
<h2 id="optional-capability-plugins-voiceaudiovision">Optional capability plugins (voice/audio/vision)</h2>
<p>AbstractCore stays dependency-light by default. Deterministic modality APIs (STT/TTS, generative vision) live in <strong>optional packages</strong> and are exposed through the capability plugin layer:</p>
<ul>
<li>Install <code>abstractvoice</code> → <code>llm.voice</code> / <code>llm.audio</code> (TTS/STT)</li>
<li>Install <code>abstractvision</code> → <code>llm.vision</code> (text→image, image→image, …)</li>
</ul>
<div class="code-block"><pre><code class="language-bash">pip install abstractvoice
pip install abstractvision
</code></pre></div>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")  # example; pick a provider/model you have access to
print(llm.capabilities.status())  # availability + selected backend ids + install hints

# Voice/audio
wav_bytes = llm.voice.tts("Hello", format="wav")
text = llm.audio.transcribe("speech.wav")

# Vision (requires vision_base_url / ABSTRACTVISION_BASE_URL)
# png_bytes = llm.vision.t2i("a red square")
</code></pre></div>
<h2 id="what-abstractcore-does-well">What AbstractCore Does Well</h2>
<h3 id="1-universal-llm-provider-interface">1. Universal LLM Provider Interface</h3>
<p><strong>What it does</strong>: Provides identical APIs across all major LLM providers.</p>
<div class="code-block"><pre><code class="language-python"># Same code works with any provider
def ask_llm(provider_name, question):
    llm = create_llm(provider_name, model="default")
    return llm.generate(question)

# All of these work identically
ask_llm("openai", "What is Python?")
ask_llm("anthropic", "What is Python?")
ask_llm("ollama", "What is Python?")
</code></pre></div>
<p><strong>Why this helps</strong>: Provides consistent tool calling, streaming, and structured output across all providers.</p>
<h3 id="2-production-grade-reliability">2. Production-Grade Reliability</h3>
<p><strong>What it does</strong>: Handles failures gracefully with retry logic, circuit breakers, and comprehensive error handling.</p>
<ul>
<li><strong>Automatic retries</strong> with exponential backoff for rate limits and network errors</li>
<li><strong>Circuit breakers</strong> prevent cascade failures when providers go down</li>
<li><strong>Smart error classification</strong> - retries recoverable errors, fails fast on auth errors</li>
<li><strong>Event system</strong> for monitoring and alerting</li>
</ul>
<p><strong>Why this helps</strong>: Includes production reliability features like retry logic and error handling.</p>
<h3 id="3-universal-tool-calling">3. Universal Tool Calling</h3>
<p><strong>What it does</strong>: Tools work consistently across ALL providers, even those without native tool support.</p>
<div class="code-block"><pre><code class="language-python">tools = [{"name": "get_weather", "description": "Get weather", ...}]

# Works with providers that have native tool support
openai_response = openai_llm.generate("Weather in Paris?", tools=tools)

# Also works with providers that don't (via intelligent prompting)
ollama_response = ollama_llm.generate("Weather in Paris?", tools=tools)
</code></pre></div>
<p><strong>Why this helps</strong>: Tools work with any provider, including those without native tool support.</p>
<h3 id="4-tool-call-tag-rewriting-for-agentic-cli-compatibility">4. Tool Call Tag Rewriting for Agentic CLI Compatibility</h3>
<p><strong>What it does</strong>: Automatically rewrites tool call tags to match different agentic CLI requirements in real-time.</p>
<div class="code-block"><pre><code class="language-python"># Rewrite tool calls for different CLIs
# Use a prompted-tools provider (tool-call markup lives in assistant content)
llm = create_llm("ollama", model="qwen3:4b-instruct")

# For Codex CLI (Qwen3 format)
response = llm.generate("Weather in Paris?", tools=tools, tool_call_tags="qwen3")
# Output: &lt;|tool_call|&gt;{"name": "get_weather", "arguments": {"location": "Paris"}}&lt;/|tool_call|&gt;

# For Crush CLI (LLaMA3 format)  
response = llm.generate("Weather in Paris?", tools=tools, tool_call_tags="llama3")
# Output: &lt;function_call&gt;{"name": "get_weather", "arguments": {"location": "Paris"}}&lt;/function_call&gt;

# For Gemini CLI (XML format)
response = llm.generate("Weather in Paris?", tools=tools, tool_call_tags="xml")
# Output: &lt;tool_call&gt;{"name": "get_weather", "arguments": {"location": "Paris"}}&lt;/tool_call&gt;
</code></pre></div>
<p><strong>Why this helps</strong>: Works with different agentic CLIs without code changes.</p>
<h3 id="5-tool-execution-control">5. Tool Execution Control</h3>
<p><strong>What it does</strong>: Control whether AbstractCore executes tools automatically or lets the agent handle execution.</p>
<div class="code-block"><pre><code class="language-python"># Default (recommended): passthrough mode (tools are *not* executed in AbstractCore)
llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate("Weather in Paris?", tools=tools)
# response.tool_calls contains structured tool call requests; host/runtime executes them

# Optional (deprecated): direct execution in AbstractCore for simple scripts only
# llm = create_llm("openai", model="gpt-4o-mini", execute_tools=True)
</code></pre></div>
<p><strong>Why this helps</strong>: Allows flexible tool execution control for different deployment scenarios.</p>
<h3 id="6-structured-output-with-automatic-retry">6. Structured Output with Automatic Retry</h3>
<p><strong>What it does</strong>: Gets typed Python objects from LLMs with automatic validation and retry on failures.</p>
<div class="code-block"><pre><code class="language-python">class Product(BaseModel):
    name: str
    price: float

# Automatically retries with error feedback if validation fails
product = llm.generate(
    "Extract: Gaming laptop for $1200",
    response_model=Product
)
</code></pre></div>
<p><strong>Why this helps</strong>: Built-in validation retry reduces manual error handling.</p>
<blockquote>
<p><strong>See</strong>: <a href="structured-output.html">Structured Output Guide</a> for native vs prompted strategies, schema design, and production deployment</p>
</blockquote>
<h3 id="5-streaming-with-tool-support">5. Streaming with Tool Support</h3>
<p><strong>What it does</strong>: Real-time response streaming that properly handles tool calls.</p>
<div class="code-block"><pre><code class="language-python"># Streams content in real-time, executes tools at the end
for chunk in llm.generate("Tell me about Paris weather", tools=tools, stream=True):
    print(chunk.content, end="", flush=True)
</code></pre></div>
<p><strong>Why this helps</strong>: Streaming works correctly with tool calls.</p>
<h3 id="6-event-driven-observability">6. Event-Driven Observability</h3>
<p><strong>What it does</strong>: Comprehensive events for monitoring, debugging, and control.</p>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global

def cost_monitor(event):
    if event.type != EventType.GENERATION_COMPLETED:
        return
    cost = event.data.get("cost_usd")
    if isinstance(cost, (int, float)) and cost &gt; 0.10:
        # NOTE: `cost_usd` is a best-effort estimate based on token usage.
        alert(f"High estimated cost: ${cost:.2f}")

on_global(EventType.GENERATION_COMPLETED, cost_monitor)
</code></pre></div>
<p><strong>Why this helps</strong>: Provides built-in observability for monitoring and debugging.</p>
<h3 id="7-built-in-production-applications">7. Built-in Production Applications</h3>
<p><strong>What it does</strong>: Provides ready-to-use command-line applications for common LLM tasks without any programming.</p>
<div class="code-block"><pre><code class="language-bash"># Document summarization with multiple strategies
summarizer document.pdf --style executive --length brief
summarizer report.txt --focus "technical details" --output summary.txt

# Entity and relationship extraction
extractor research_paper.pdf --format json-ld --focus technology
extractor article.txt --entity-types person,organization,location

# Text evaluation and scoring
judge essay.txt --criteria clarity,accuracy,coherence --context "academic writing"
judge code.py --context "code review" --format plain

# Intent analysis and deception detection
intent conversation.txt --focus-participant user --depth comprehensive
intent email.txt --format plain --context document --verbose
</code></pre></div>
<p><strong>Available Applications:</strong>
- <strong>Summarizer</strong>: Document summarization with customizable styles and focus areas
- <strong>Extractor</strong>: Entity and relationship extraction with multiple output formats
- <strong>Judge</strong>: Text evaluation with custom criteria and scoring rubrics
- <strong>Intent Analyzer</strong>: Psychological intent analysis with deception detection</p>
<p><strong>Why this helps</strong>: Provides ready-to-use CLI tools that work with any LLM provider.</p>
<h2 id="what-abstractcore-does-not-do">What AbstractCore Does NOT Do</h2>
<p>Understanding limitations is crucial for choosing the right tool.</p>
<h3 id="1-rag-pipelines-use-specialized-tools">1. RAG Pipelines (Use Specialized Tools)</h3>
<p><strong>What AbstractCore provides</strong>: Vector embeddings via <code>EmbeddingManager</code>
<strong>What it doesn't provide</strong>: Document chunking, vector databases, retrieval strategies</p>
<div class="code-block"><pre><code class="language-python"># AbstractCore gives you this
from abstractcore.embeddings import EmbeddingManager
embedder = EmbeddingManager()
similarity = embedder.compute_similarity("query", "document")

# You need to build this yourself
def rag_pipeline(query, documents):
    # 1. Chunk documents - YOU implement
    # 2. Store in vector DB - YOU implement
    # 3. Retrieve relevant chunks - YOU implement
    # 4. Construct prompt - YOU implement
    return llm.generate(prompt)
</code></pre></div>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://github.com/run-llama/llama_index">LlamaIndex</a></strong> - Full RAG framework
- <strong><a href="https://github.com/langchain-ai/langchain">LangChain</a></strong> - RAG components and chains</p>
<h3 id="2-complex-agent-workflows-use-agent-frameworks">2. Complex Agent Workflows (Use Agent Frameworks)</h3>
<p><strong>What AbstractCore provides</strong>: Single LLM calls with tool execution
<strong>What it doesn't provide</strong>: Multi-step agent reasoning, planning, memory persistence</p>
<div class="code-block"><pre><code class="language-python"># AbstractCore is great for this
response = llm.generate("What's 2+2?", tools=[calculator_tool])

# AbstractCore is NOT for this
def complex_agent():
    # 1. Plan multi-step solution - NOT provided
    # 2. Execute steps with memory - NOT provided
    # 3. Reflect and re-plan - NOT provided
    # 4. Persist agent state - NOT provided
    pass
</code></pre></div>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://github.com/lpalbou/AbstractAgent">AbstractAgent</a></strong> - Built on AbstractCore
- <strong><a href="https://github.com/langchain-ai/langgraph">LangGraph</a></strong> - Agent orchestration
- <strong><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a></strong> - Autonomous agents</p>
<h3 id="3-advanced-memory-systems-use-memory-frameworks">3. Advanced Memory Systems (Use Memory Frameworks)</h3>
<p><strong>What AbstractCore provides</strong>: Basic conversation history via <code>BasicSession</code>
<strong>What it doesn't provide</strong>: Semantic memory, long-term memory, knowledge graphs</p>
<div class="code-block"><pre><code class="language-python"># AbstractCore provides basic sessions
session = BasicSession(provider=llm)
session.generate("My name is Alice")
session.generate("What's my name?")  # Remembers within session

# For advanced memory, use specialized tools
temporal_graph = AbstractMemory()  # Persistent, semantic memory
temporal_graph.add_memory("Alice likes Python programming", context="conversation")
</code></pre></div>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://github.com/lpalbou/AbstractMemory">AbstractMemory</a></strong> - Temporal knowledge graphs
- <strong><a href="https://github.com/mem0ai/mem0">Mem0</a></strong> - Personalized memory layer</p>
<h3 id="4-prompt-template-management-use-template-libraries">4. Prompt Template Management (Use Template Libraries)</h3>
<p><strong>What AbstractCore provides</strong>: Direct prompt strings
<strong>What it doesn't provide</strong>: Template engines, prompt optimization, A/B testing</p>
<div class="code-block"><pre><code class="language-python"># AbstractCore expects you to handle prompts
prompt = f"Translate '{text}' to {language}"
response = llm.generate(prompt)

# For advanced templating, use other tools
template = PromptTemplate("Translate '{text}' to {language}")  # Not provided
</code></pre></div>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://jinja.palletsprojects.com/">Jinja2</a></strong> - Template engine
- <strong><a href="https://python.langchain.com/docs/modules/model_io/prompts/">LangChain Prompts</a></strong> - Prompt management
- <strong><a href="https://github.com/guidance-ai/guidance">Guidance</a></strong> - Prompt programming</p>
<h3 id="5-training-and-fine-tuning-use-ml-frameworks">5. Training and Fine-tuning (Use ML Frameworks)</h3>
<p><strong>What AbstractCore provides</strong>: Interface to existing models
<strong>What it doesn't provide</strong>: Model training, fine-tuning, or optimization</p>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://github.com/huggingface/transformers">Transformers</a></strong> - Model training
- <strong><a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a></strong> - Fine-tuning framework
- <strong><a href="https://github.com/unslothai/unsloth">Unsloth</a></strong> - Fast fine-tuning</p>
<h3 id="6-multi-agent-orchestration-use-orchestration-frameworks">6. Multi-Agent Orchestration (Use Orchestration Frameworks)</h3>
<p><strong>What AbstractCore provides</strong>: Single agent with tools
<strong>What it doesn't provide</strong>: Agent-to-agent communication, hierarchical agents</p>
<p><strong>Better alternatives</strong>:
- <strong><a href="https://github.com/joaomdmoura/crewAI">CrewAI</a></strong> - Multi-agent teams
- <strong><a href="https://github.com/microsoft/autogen">AutoGen</a></strong> - Agent conversations
- <strong><a href="https://github.com/langchain-ai/langgraph">LangGraph</a></strong> - Agent networks</p>
<h2 id="when-to-choose-abstractcore">When to Choose AbstractCore</h2>
<h3 id="choose-abstractcore-when-you-need">Choose AbstractCore When You Need:</h3>
<ol>
<li>
<p><strong>Reliable LLM Infrastructure</strong>
   - Production-ready error handling and retry logic
   - Consistent interface across different providers
   - Built-in monitoring and observability</p>
</li>
<li>
<p><strong>Provider Flexibility</strong>
   - Easy switching between OpenAI, Anthropic, Ollama, etc.
   - Provider-agnostic code that runs anywhere
   - Local and cloud provider support</p>
</li>
<li>
<p><strong>Universal Tool Calling</strong>
   - Tools that work across ALL providers
   - Consistent tool execution regardless of native support
   - Event-driven tool control and monitoring</p>
</li>
<li>
<p><strong>Structured Output Reliability</strong>
   - Type-safe responses with automatic validation
   - Built-in retry logic for validation failures
   - Production-grade error handling</p>
</li>
<li>
<p><strong>Streaming with Tools</strong>
   - Real-time responses that handle tools correctly
   - Proper streaming implementation across providers</p>
</li>
</ol>
<h3 id="dont-choose-abstractcore-when-you-need">Don't Choose AbstractCore When You Need:</h3>
<ol>
<li><strong>Full RAG Frameworks</strong> → Use LlamaIndex or LangChain</li>
<li><strong>Complex Agent Workflows</strong> → Use AbstractAgent or LangGraph</li>
<li><strong>Advanced Memory Systems</strong> → Use AbstractMemory or Mem0</li>
<li><strong>Prompt Template Management</strong> → Use Jinja2 or LangChain Prompts</li>
<li><strong>Model Training/Fine-tuning</strong> → Use Transformers or Axolotl</li>
<li><strong>Multi-Agent Systems</strong> → Use CrewAI or AutoGen</li>
</ol>
<h2 id="abstractcore-in-the-ecosystem">AbstractCore in the Ecosystem</h2>
<p>AbstractCore is designed to be <strong>the foundation</strong> that other tools build on:</p>
<div class="mermaid">graph TD
    A[Your Application] --&gt; B[AbstractAgent]
    A --&gt; C[AbstractMemory]
    A --&gt; D[Custom RAG Pipeline]

    B --&gt; E[AbstractCore]
    C --&gt; E
    D --&gt; E

    E --&gt; F[OpenAI]
    E --&gt; G[Anthropic]
    E --&gt; H[Ollama]
    E --&gt; I[MLX]

    style E fill:#e1f5fe
    style A fill:#fff3e0
</div>
<p><strong>AbstractCore</strong> = The reliable foundation
<strong>AbstractAgent</strong> = Agent workflows and planning
<strong>AbstractMemory</strong> = Advanced memory and knowledge graphs
<strong>Your Application</strong> = Business logic and user interface</p>
<h2 id="decision-tree">Decision Tree</h2>
<div class="code-block"><pre><code>Need LLM functionality?
├── Simple LLM calls with reliability? → AbstractCore ✅
├── Complex agents with planning? → AbstractAgent (built on AbstractCore)
├── Advanced memory/knowledge graphs? → AbstractMemory (with AbstractCore)
├── Full RAG with document management? → LlamaIndex or LangChain
├── Multi-agent conversations? → CrewAI or AutoGen
└── Just API compatibility? → LiteLLM
</code></pre></div>
<h2 id="capabilities-summary">Capabilities Summary</h2>
<table>
<thead>
<tr>
<th>Capability</th>
<th>AbstractCore</th>
<th>When You Need More</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LLM Provider Interface</strong></td>
<td>✅ Universal</td>
<td>Covers most use cases</td>
</tr>
<tr>
<td><strong>Production Reliability</strong></td>
<td>✅ Built-in</td>
<td>Covers most use cases</td>
</tr>
<tr>
<td><strong>Tool Calling</strong></td>
<td>✅ Universal</td>
<td>Multi-step reasoning → AbstractAgent</td>
</tr>
<tr>
<td><strong>Structured Output</strong></td>
<td>✅ With retry</td>
<td>Complex validation → Custom logic</td>
</tr>
<tr>
<td><strong>Streaming</strong></td>
<td>✅ With tools</td>
<td>Covers most use cases</td>
</tr>
<tr>
<td><strong>Basic Memory</strong></td>
<td>✅ Sessions</td>
<td>Semantic memory → AbstractMemory</td>
</tr>
<tr>
<td><strong>Vector Embeddings</strong></td>
<td>✅ SOTA models</td>
<td>Full RAG → LlamaIndex</td>
</tr>
<tr>
<td><strong>Events/Monitoring</strong></td>
<td>✅ Comprehensive</td>
<td>Covers most use cases</td>
</tr>
<tr>
<td><strong>Agent Workflows</strong></td>
<td>❌ Single calls</td>
<td>Complex agents → AbstractAgent</td>
</tr>
<tr>
<td><strong>Advanced Memory</strong></td>
<td>❌ Session only</td>
<td>Knowledge graphs → AbstractMemory</td>
</tr>
<tr>
<td><strong>RAG Pipelines</strong></td>
<td>❌ Embeddings only</td>
<td>Document processing → LlamaIndex</td>
</tr>
<tr>
<td><strong>Prompt Templates</strong></td>
<td>❌ Raw strings</td>
<td>Template management → Jinja2</td>
</tr>
</tbody>
</table>
<h2 id="next-steps">Next Steps</h2>
<p>Based on your needs:</p>
<ul>
<li><strong>Start with AbstractCore</strong>: <a href="getting-started.html">Getting Started Guide</a></li>
<li><strong>Need agents</strong>: Check out <a href="https://github.com/lpalbou/AbstractAgent">AbstractAgent</a></li>
<li><strong>Need advanced memory</strong>: Check out <a href="https://github.com/lpalbou/AbstractMemory">AbstractMemory</a></li>
<li><strong>Compare frameworks</strong>: Read <a href="https://github.com/lpalbou/AbstractCore/blob/main/docs/comparison.md">Framework Comparison</a></li>
<li><strong>See real examples</strong>: Browse <a href="examples.html">Examples</a></li>
</ul>
<hr/>
<p><strong>Remember</strong>: AbstractCore is infrastructure, not a full framework. It focuses on LLM provider abstraction and integrates with specialized tools for other needs.</p>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({ startOnLoad: true, theme: "dark" });</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
