<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Practical Examples - AbstractCore</title>
    <meta name="description" content="This guide shows real-world use cases for AbstractCore with complete, copy-paste examples. All examples work across any provider - just change the provider‚Ä¶">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Practical Examples</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">This guide shows real-world use cases for AbstractCore with complete, copy-paste examples. All examples work across any provider - just change the provider name.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#table-of-contents" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Table of Contents</a>
<a href="#basic-usage" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Basic Usage</a>
<a href="#glyph-visual-text-compression" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Glyph Visual-Text Compression</a>
<a href="#tool-calling-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Tool Calling Examples</a>
<a href="#tool-call-syntax-rewriting-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Tool Call Syntax Rewriting Examples</a>
<a href="#structured-output-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Structured Output Examples</a>
<a href="#streaming-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Streaming Examples</a>
<a href="#session-management" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Session Management</a>
<a href="#interaction-tracing-observability" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Interaction Tracing (Observability)</a>
<a href="#production-patterns" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Production Patterns</a>
<a href="#integration-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Integration Examples</a>
<a href="#next-steps" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Next Steps</a></div>

            <div class="doc-content">

<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#glyph-visual-text-compression">Glyph Visual-Text Compression</a></li>
<li><a href="#tool-calling-examples">Tool Calling Examples</a></li>
<li><a href="#tool-call-syntax-rewriting-examples">Tool Call Syntax Rewriting Examples</a></li>
<li><a href="#structured-output-examples">Structured Output Examples</a></li>
<li><a href="#streaming-examples">Streaming Examples</a></li>
<li><a href="#session-management">Session Management</a></li>
<li><a href="#interaction-tracing-observability">Interaction Tracing (Observability)</a></li>
<li><a href="#production-patterns">Production Patterns</a></li>
<li><a href="#integration-examples">Integration Examples</a></li>
</ul>
<h2 id="basic-usage">Basic Usage</h2>
<h3 id="simple-qa">Simple Q&amp;A</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Works with any provider
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)  # or &quot;anthropic&quot;, &quot;ollama&quot;...

response = llm.generate(&quot;What is the difference between Python and JavaScript?&quot;)
print(response.content)
</code></pre></div>
<h3 id="multiple-providers-comparison">Multiple Providers Comparison</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

providers = [
    (&quot;openai&quot;, &quot;gpt-4o-mini&quot;),
    (&quot;anthropic&quot;, &quot;claude-haiku-4-5&quot;),
    (&quot;ollama&quot;, &quot;qwen3:4b-instruct&quot;)
]

question = &quot;Explain Python list comprehensions with examples&quot;

for provider_name, model in providers:
    try:
        llm = create_llm(provider_name, model=model)
        response = llm.generate(question)
        print(f&quot;\n--- {provider_name.upper()} ---&quot;)
        print(response.content[:200] + &quot;...&quot;)
    except Exception as e:
        print(f&quot;{provider_name} failed: {e}&quot;)
</code></pre></div>
<h3 id="provider-fallback">Provider Fallback</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

def generate_with_fallback(prompt, **kwargs):
    &quot;&quot;&quot;Try multiple providers until one works.&quot;&quot;&quot;
    providers = [
        (&quot;openai&quot;, &quot;gpt-4o-mini&quot;),
        (&quot;anthropic&quot;, &quot;claude-haiku-4-5&quot;),
        (&quot;ollama&quot;, &quot;qwen3:4b-instruct&quot;)
    ]

    for provider_name, model in providers:
        try:
            llm = create_llm(provider_name, model=model)
            return llm.generate(prompt, **kwargs)
        except Exception as e:
            print(f&quot;{provider_name} failed: {e}&quot;)
            continue

    raise Exception(&quot;All providers failed&quot;)

# Usage
response = generate_with_fallback(&quot;What is machine learning?&quot;)
print(response.content)
</code></pre></div>
<h2 id="glyph-visual-text-compression">Glyph Visual-Text Compression</h2>
<p>Glyph compression renders long text into images for vision-capable models to reduce effective token usage (often 3‚Äì4x on long text; depends on content/model).</p>
<p>Requires <code>pip install "abstractcore[compression]"</code> (and <code>pip install "abstractcore[media]"</code> if you want PDF/Office text extraction).</p>
<h3 id="automatic-compression-with-ollama">Automatic Compression with Ollama</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Use a vision-capable model - Glyph works automatically
llm = create_llm(&quot;ollama&quot;, model=&quot;llama3.2-vision:11b&quot;)

# Large documents are automatically compressed when beneficial
response = llm.generate(
    &quot;What are the key findings and methodology in this research paper?&quot;,
    media=[&quot;research_paper.pdf&quot;]  # Automatically compressed if size &gt; threshold
)

print(f&quot;Analysis: {response.content}&quot;)
print(f&quot;Processing time: {response.gen_time}ms&quot;)

# Check if compression was used
if response.metadata and response.metadata.get('compression_used'):
    stats = response.metadata.get('compression_stats', {})
    print(f&quot;‚úÖ Glyph compression used!&quot;)
    print(f&quot;Compression ratio: {stats.get('compression_ratio', 'N/A')}x&quot;)
    print(f&quot;Original tokens: {stats.get('original_tokens', 'N/A')}&quot;)
    print(f&quot;Compressed tokens: {stats.get('compressed_tokens', 'N/A')}&quot;)
</code></pre></div>
<h3 id="explicit-compression-control">Explicit Compression Control</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Force compression for testing
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen2.5vl:7b&quot;)

# Always compress
response = llm.generate(
    &quot;Summarize the main conclusions of this document&quot;,
    media=[&quot;long_document.pdf&quot;],
    glyph_compression=&quot;always&quot;  # Force compression
)

# Never compress (for comparison)
response_no_compression = llm.generate(
    &quot;Summarize the main conclusions of this document&quot;, 
    media=[&quot;long_document.pdf&quot;],
    glyph_compression=&quot;never&quot;  # Disable compression
)

print(f&quot;With compression: {response.gen_time}ms&quot;)
print(f&quot;Without compression: {response_no_compression.gen_time}ms&quot;)
</code></pre></div>
<h3 id="custom-configuration">Custom Configuration</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.compression import GlyphConfig

# Configure compression behavior
glyph_config = GlyphConfig(
    enabled=True,
    global_default=&quot;auto&quot;,           # &quot;auto&quot;, &quot;always&quot;, &quot;never&quot;
    quality_threshold=0.95,          # Minimum quality score (0-1)
    target_compression_ratio=3.0,    # Target compression ratio
    provider_optimization=True,      # Enable provider-specific optimization
    cache_enabled=True,             # Enable compression caching
    provider_profiles={
        &quot;ollama&quot;: {
            &quot;dpi&quot;: 150,              # Higher DPI for better quality
            &quot;font_size&quot;: 9,          # Smaller font for more content
            &quot;quality_threshold&quot;: 0.95
        }
    }
)

llm = create_llm(&quot;ollama&quot;, model=&quot;granite3.2-vision:latest&quot;, glyph_config=glyph_config)

response = llm.generate(
    &quot;Analyze the figures and tables in this academic paper&quot;,
    media=[&quot;academic_paper.pdf&quot;]
)
</code></pre></div>
<h3 id="performance-benchmarking">Performance Benchmarking</h3>
<div class="code-block"><pre><code class="language-python">import time
from abstractcore import create_llm

def benchmark_glyph_compression(document_path, model_name=&quot;llama3.2-vision:11b&quot;):
    &quot;&quot;&quot;Compare processing with and without Glyph compression&quot;&quot;&quot;

    llm = create_llm(&quot;ollama&quot;, model=model_name)

    # Test without compression
    start = time.time()
    response_no_glyph = llm.generate(
        &quot;Provide a detailed analysis of this document&quot;,
        media=[document_path],
        glyph_compression=&quot;never&quot;
    )
    time_no_glyph = time.time() - start

    # Test with compression
    start = time.time()
    response_glyph = llm.generate(
        &quot;Provide a detailed analysis of this document&quot;,
        media=[document_path],
        glyph_compression=&quot;always&quot;
    )
    time_glyph = time.time() - start

    # Compare results
    print(f&quot;üìä Glyph Compression Benchmark&quot;)
    print(f&quot;Document: {document_path}&quot;)
    print(f&quot;Model: {model_name}&quot;)
    print(f&quot;&quot;)
    print(f&quot;Without Glyph: {time_no_glyph:.2f}s&quot;)
    print(f&quot;With Glyph:    {time_glyph:.2f}s&quot;)
    print(f&quot;Speedup:       {time_no_glyph/time_glyph:.2f}x&quot;)
    print(f&quot;&quot;)
    print(f&quot;Response quality comparison:&quot;)
    print(f&quot;No Glyph length:  {len(response_no_glyph.content)} chars&quot;)
    print(f&quot;Glyph length:     {len(response_glyph.content)} chars&quot;)

    return response_glyph, response_no_glyph

# Run benchmark
glyph_response, normal_response = benchmark_glyph_compression(&quot;large_document.pdf&quot;)
</code></pre></div>
<h3 id="multi-provider-testing">Multi-Provider Testing</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Test Glyph across different providers and models
models_to_test = [
    (&quot;ollama&quot;, &quot;llama3.2-vision:11b&quot;),
    (&quot;ollama&quot;, &quot;qwen2.5vl:7b&quot;),
    (&quot;ollama&quot;, &quot;granite3.2-vision:latest&quot;),
    # Add LMStudio if running
    # (&quot;lmstudio&quot;, &quot;your-vision-model&quot;),
]

document = &quot;research_paper.pdf&quot;
question = &quot;What are the key innovations presented in this paper?&quot;

for provider, model in models_to_test:
    try:
        print(f&quot;\nüß™ Testing {provider} - {model}&quot;)

        llm = create_llm(provider, model=model)

        response = llm.generate(
            question,
            media=[document],
            glyph_compression=&quot;auto&quot;
        )

        print(f&quot;‚úÖ Success - {response.gen_time}ms&quot;)
        print(f&quot;Response: {response.content[:100]}...&quot;)

        # Check compression usage
        if response.metadata and response.metadata.get('compression_used'):
            print(f&quot;üé® Glyph compression was used&quot;)
        else:
            print(f&quot;üìù Standard processing was used&quot;)

    except Exception as e:
        print(f&quot;‚ùå Failed: {e}&quot;)
</code></pre></div>
<p><strong>Key Benefits Demonstrated:</strong>
- <strong>Automatic optimization</strong>: Glyph decides when compression is beneficial
- <strong>Transparent integration</strong>: Works with existing media handling code
- <strong>Quality preservation</strong>: No loss of analytical accuracy
- <strong>Provider flexibility</strong>: Works across Ollama, LMStudio, and other vision providers</p>
<p><a href="glyphs.html">Learn more about Glyph configuration and advanced features</a></p>
<h2 id="tool-calling-examples">Tool Calling Examples</h2>
<h3 id="weather-tool">Weather Tool</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
import requests

def get_weather(city: str, units: str = &quot;metric&quot;) -&gt; str:
    &quot;&quot;&quot;Get current weather for a city.&quot;&quot;&quot;
    # In production, use a real weather API
    # This is a simulated implementation
    temperatures = {
        &quot;paris&quot;: &quot;22¬∞C, sunny&quot;,
        &quot;london&quot;: &quot;15¬∞C, cloudy&quot;,
        &quot;tokyo&quot;: &quot;28¬∞C, humid&quot;,
        &quot;new york&quot;: &quot;18¬∞C, windy&quot;
    }
    return temperatures.get(city.lower(), f&quot;Weather data not available for {city}&quot;)

# Tool definition
weather_tool = {
    &quot;name&quot;: &quot;get_weather&quot;,
    &quot;description&quot;: &quot;Get current weather information for a city&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;city&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;Name of the city&quot;
            },
            &quot;units&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [&quot;metric&quot;, &quot;imperial&quot;],
                &quot;description&quot;: &quot;Temperature units&quot;
            }
        },
        &quot;required&quot;: [&quot;city&quot;]
    }
}

# Works with any provider that supports tools
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

response = llm.generate(
    &quot;What's the weather like in Paris and London?&quot;,
    tools=[weather_tool]
)

print(response.content)
print(response.tool_calls)  # Structured tool call requests (host/runtime executes them)
</code></pre></div>
<h3 id="calculator-tool">Calculator Tool</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
import math

def calculate(expression: str) -&gt; str:
    &quot;&quot;&quot;Safely evaluate mathematical expressions.&quot;&quot;&quot;
    try:
        # In production, use a proper expression parser
        # This is simplified for demo purposes
        allowed_chars = set('0123456789+-*/.() ')
        if not all(c in allowed_chars for c in expression):
            return &quot;Error: Invalid characters in expression&quot;

        result = eval(expression)
        return f&quot;{expression} = {result}&quot;
    except Exception as e:
        return f&quot;Error calculating {expression}: {str(e)}&quot;

def sqrt(number: float) -&gt; str:
    &quot;&quot;&quot;Calculate square root.&quot;&quot;&quot;
    try:
        result = math.sqrt(number)
        return f&quot;‚àö{number} = {result}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;

# Tool definitions
tools = [
    {
        &quot;name&quot;: &quot;calculate&quot;,
        &quot;description&quot;: &quot;Perform basic mathematical calculations&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;expression&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Mathematical expression&quot;}
            },
            &quot;required&quot;: [&quot;expression&quot;]
        }
    },
    {
        &quot;name&quot;: &quot;sqrt&quot;,
        &quot;description&quot;: &quot;Calculate square root of a number&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;number&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;Number to calculate square root of&quot;}
            },
            &quot;required&quot;: [&quot;number&quot;]
        }
    }
]

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

response = llm.generate(
    &quot;What is 25 * 4 + 12, and what's the square root of 144?&quot;,
    tools=tools
)

print(response.content)
print(response.tool_calls)  # Structured tool call requests (host/runtime executes them)
</code></pre></div>
<h3 id="file-operations-tool">File Operations Tool</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from pathlib import Path
import os

def list_files(directory: str = &quot;.&quot;) -&gt; str:
    &quot;&quot;&quot;List files in a directory.&quot;&quot;&quot;
    try:
        path = Path(directory)
        if not path.exists():
            return f&quot;Directory {directory} does not exist&quot;

        files = []
        for item in path.iterdir():
            if item.is_file():
                files.append(f&quot;FILE: {item.name}&quot;)
            elif item.is_dir():
                files.append(f&quot;DIR: {item.name}/&quot;)

        return f&quot;Contents of {directory}:\n&quot; + &quot;\n&quot;.join(sorted(files))
    except Exception as e:
        return f&quot;Error listing files: {str(e)}&quot;

def read_file(filename: str) -&gt; str:
    &quot;&quot;&quot;Read contents of a text file.&quot;&quot;&quot;
    try:
        path = Path(filename)
        if not path.exists():
            return f&quot;File {filename} does not exist&quot;

        content = path.read_text(encoding='utf-8')
        return f&quot;Contents of {filename}:\n{content}&quot;
    except Exception as e:
        return f&quot;Error reading file: {str(e)}&quot;

# Tool definitions
file_tools = [
    {
        &quot;name&quot;: &quot;list_files&quot;,
        &quot;description&quot;: &quot;List files and directories in a given path&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;directory&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Directory path to list&quot;}
            }
        }
    },
    {
        &quot;name&quot;: &quot;read_file&quot;,
        &quot;description&quot;: &quot;Read the contents of a text file&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;filename&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Path to the file to read&quot;}
            },
            &quot;required&quot;: [&quot;filename&quot;]
        }
    }
]

llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

response = llm.generate(
    &quot;List the files in the current directory and read the README.md file if it exists&quot;,
    tools=file_tools
)

print(response.content)
print(response.tool_calls)  # Structured tool call requests (host/runtime executes them)
</code></pre></div>
<h2 id="tool-call-syntax-rewriting-examples">Tool Call Syntax Rewriting Examples</h2>
<blockquote>
<p><strong>Real-time tool call format conversion for agentic CLI compatibility</strong></p>
</blockquote>
<p>Tool call syntax rewriting enables AbstractCore to work seamlessly with any agentic CLI by converting tool calls to the expected format in real-time. This happens automatically during generation, including streaming.</p>
<blockquote>
<p><strong>Related</strong>: <a href="tool-syntax-rewriting.html">Tool Call Syntax Rewriting Guide</a></p>
</blockquote>
<h3 id="codex-cli-integration-qwen3-tags">Codex CLI Integration (Qwen3 Tags)</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Define tools (standard JSON format)
weather_tool = {
    &quot;name&quot;: &quot;get_weather&quot;,
    &quot;description&quot;: &quot;Get current weather for a city&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {&quot;city&quot;: {&quot;type&quot;: &quot;string&quot;}},
        &quot;required&quot;: [&quot;city&quot;]
    }
}

# Codex CLI expects qwen3-style tool-call tags in assistant content.
# By default, AbstractCore strips tool-call markup from `response.content`;
# pass `tool_call_tags` to preserve/emit the tags for downstream parsers.
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
response = llm.generate(&quot;What's the weather in Tokyo?&quot;, tools=[weather_tool], tool_call_tags=&quot;qwen3&quot;)

print(response.content)
print(response.tool_calls)
# Content includes: &lt;|tool_call|&gt;{&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: {&quot;city&quot;: &quot;Tokyo&quot;}}&lt;/|tool_call|&gt;
</code></pre></div>
<h3 id="crush-cli-integration">Crush CLI Integration</h3>
<div class="code-block"><pre><code class="language-python"># Crush CLI expects LLaMA3 format - just specify the format
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
response = llm.generate(&quot;Get weather for London&quot;, tools=[weather_tool], tool_call_tags=&quot;llama3&quot;)

print(response.content)
# Output includes: &lt;function_call&gt;{&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: {&quot;city&quot;: &quot;London&quot;}}&lt;/function_call&gt;
</code></pre></div>
<h3 id="custom-cli-format">Custom CLI Format</h3>
<div class="code-block"><pre><code class="language-python"># Your custom CLI expects: [TOOL]...JSON...[/TOOL]
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
response = llm.generate(&quot;Check weather in Paris&quot;, tools=[weather_tool], tool_call_tags=&quot;[TOOL],[/TOOL]&quot;)

print(response.content)
# Output includes: [TOOL]{&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: {&quot;city&quot;: &quot;Paris&quot;}}[/TOOL]
</code></pre></div>
<h3 id="real-time-streaming-with-tag-rewriting">Real-Time Streaming with Tag Rewriting</h3>
<div class="code-block"><pre><code class="language-python"># Streaming works seamlessly with any format
calculator_tool = {
    &quot;name&quot;: &quot;calculate&quot;,
    &quot;description&quot;: &quot;Perform mathematical calculations&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {&quot;expression&quot;: {&quot;type&quot;: &quot;string&quot;}},
        &quot;required&quot;: [&quot;expression&quot;]
    }
}

llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3-coder:30b&quot;)

print(&quot;AI: &quot;, end=&quot;&quot;, flush=True)
for chunk in llm.generate(
    &quot;Calculate 15 * 23 and explain the result&quot;,
    tools=[calculator_tool],
    stream=True,
    tool_call_tags=&quot;llama3&quot;,
):
    print(chunk.content, end=&quot;&quot;, flush=True)

    # Tool calls are surfaced in real-time (execution is host/runtime-owned)
    if chunk.tool_calls:
        for tool_call in chunk.tool_calls:
            print(f&quot;\n[TOOL CALL] {tool_call}&quot;)

print(&quot;\n&quot;)
# Shows: &lt;function_call&gt;{&quot;name&quot;: &quot;calculate&quot;, &quot;arguments&quot;: {&quot;expression&quot;: &quot;15 * 23&quot;}}&lt;/function_call&gt;
# Tool execution is owned by the host/runtime.
</code></pre></div>
<h3 id="multiple-tools-with-different-formats">Multiple Tools with Different Formats</h3>
<div class="code-block"><pre><code class="language-python"># Define multiple tools
tools = [
    {
        &quot;name&quot;: &quot;get_weather&quot;,
        &quot;description&quot;: &quot;Get weather information&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {&quot;city&quot;: {&quot;type&quot;: &quot;string&quot;}},
            &quot;required&quot;: [&quot;city&quot;]
        }
    },
    {
        &quot;name&quot;: &quot;calculate&quot;,
        &quot;description&quot;: &quot;Perform calculations&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {&quot;expression&quot;: {&quot;type&quot;: &quot;string&quot;}},
            &quot;required&quot;: [&quot;expression&quot;]
        }
    },
    {
        &quot;name&quot;: &quot;list_files&quot;,
        &quot;description&quot;: &quot;List files in a directory&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {&quot;directory&quot;: {&quot;type&quot;: &quot;string&quot;}},
            &quot;required&quot;: [&quot;directory&quot;]
        }
    }
]

# Test with XML format for Gemini CLI
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
response = llm.generate(
    &quot;What's 2+2, weather in NYC, and files in current directory?&quot;,
    tools=tools,
    tool_call_tags=&quot;xml&quot;,
)

print(response.content)
print(response.tool_calls)
# All tool calls converted to: &lt;tool_call&gt;{&quot;name&quot;: &quot;...&quot;, &quot;arguments&quot;: {...}}&lt;/tool_call&gt;
</code></pre></div>
<h3 id="session-based-format-configuration">Session-Based Format Configuration</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import BasicSession

# Apply a consistent tool-call tag format across a session by reusing a variable
tool_call_tags = &quot;llama3&quot;

llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
session = BasicSession(provider=llm)

session.generate(&quot;Calculate 10 * 5&quot;, tools=[calculator_tool], tool_call_tags=tool_call_tags)
session.generate(&quot;What's the weather like?&quot;, tools=[weather_tool], tool_call_tags=tool_call_tags)
session.generate(&quot;List files in documents&quot;, tools=[{
    &quot;name&quot;: &quot;list_files&quot;,
    &quot;description&quot;: &quot;List directory contents&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {&quot;path&quot;: {&quot;type&quot;: &quot;string&quot;}},
        &quot;required&quot;: [&quot;path&quot;]
    }
}], tool_call_tags=tool_call_tags)

# All responses contain: &lt;function_call&gt;...JSON...&lt;/function_call&gt;
</code></pre></div>
<h3 id="production-monitoring-with-events">Production Monitoring with Events</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global

# Monitor tool usage across different formats
def log_tool_calls(event):
    # Tool execution events are emitted when tools are executed (e.g., via ToolRegistry
    # or when using `execute_tools=True` (deprecated)).
    print(f&quot;[TOOL EVENT] {event.type}: {event.data}&quot;)

on_global(EventType.TOOL_COMPLETED, log_tool_calls)

# Test with different formats
for format_name in [&quot;qwen3&quot;, &quot;llama3&quot;, &quot;xml&quot;]:
    llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)
    response = llm.generate(&quot;Calculate 5 * 5&quot;, tools=[calculator_tool], tool_call_tags=format_name)
    print(f&quot;{format_name} format result: {response.content[:100]}...&quot;)
</code></pre></div>
<p><strong>Key Benefits</strong>:
- Per-call configuration: pass <code>tool_call_tags=...</code> when you need tool-call markup preserved/rewritten in <code>response.content</code>
- Real-time processing: No post-processing delays
- Streaming compatible: Works with streaming mode
- Format flexibility: Predefined formats plus custom tags</p>
<blockquote>
<p><strong>Related</strong>: <a href="tool-syntax-rewriting.html">Tool Call Syntax Rewriting Guide</a> | <a href="architecture.html#unified-streaming-architecture">Unified Streaming Architecture</a></p>
</blockquote>
<h2 id="structured-output-examples">Structured Output Examples</h2>
<blockquote>
<p><strong>Complete Guide</strong>: <a href="structured-output.html">Structured Output Documentation</a> - Native vs prompted strategies, provider support, schema design best practices</p>
</blockquote>
<h3 id="user-profile-extraction">User Profile Extraction</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from pydantic import BaseModel, field_validator
from typing import Optional

class UserProfile(BaseModel):
    name: str
    age: int
    email: str
    occupation: Optional[str] = None
    interests: list[str] = []

    @field_validator('age')
    @classmethod
    def validate_age(cls, v):
        if v &lt; 0 or v &gt; 150:
            raise ValueError('Age must be between 0 and 150')
        return v

    @field_validator('email')
    @classmethod
    def validate_email(cls, v):
        if '@' not in v:
            raise ValueError('Invalid email format')
        return v

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# Text with user information
user_text = &quot;&quot;&quot;
Hi, I'm Sarah Johnson, I'm 28 years old and work as a software engineer.
My email is sarah.johnson@techcorp.com. I love hiking, photography, and cooking.
&quot;&quot;&quot;

# Extract structured data with automatic validation
user = llm.generate(
    f&quot;Extract user profile from: {user_text}&quot;,
    response_model=UserProfile
)

print(f&quot;Name: {user.name}&quot;)
print(f&quot;Age: {user.age}&quot;)
print(f&quot;Email: {user.email}&quot;)
print(f&quot;Occupation: {user.occupation}&quot;)
print(f&quot;Interests: {', '.join(user.interests)}&quot;)
</code></pre></div>
<h3 id="product-catalog-extraction">Product Catalog Extraction</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from pydantic import BaseModel, field_validator
from typing import List
from enum import Enum

class ProductCategory(str, Enum):
    ELECTRONICS = &quot;electronics&quot;
    CLOTHING = &quot;clothing&quot;
    BOOKS = &quot;books&quot;
    HOME = &quot;home&quot;
    SPORTS = &quot;sports&quot;

class Product(BaseModel):
    name: str
    price: float
    category: ProductCategory
    description: str
    in_stock: bool = True

    @field_validator('price')
    @classmethod
    def validate_price(cls, v):
        if v &lt;= 0:
            raise ValueError('Price must be positive')
        return v

class ProductCatalog(BaseModel):
    products: List[Product]
    total_count: int

    @field_validator('total_count')
    @classmethod
    def validate_count(cls, v, info):
        products = info.data.get('products', [])
        if v != len(products):
            raise ValueError(f'Total count {v} does not match products length {len(products)}')
        return v

llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

catalog_text = &quot;&quot;&quot;
Our store has these items:
1. Gaming Laptop - $1299.99 - High-performance laptop for gaming and work
2. Wireless Headphones - $199.99 - Noise-cancelling bluetooth headphones
3. Python Programming Book - $49.99 - Complete guide to Python programming
4. Coffee Maker - $89.99 - Automatic drip coffee maker, currently out of stock
&quot;&quot;&quot;

catalog = llm.generate(
    f&quot;Extract product catalog from: {catalog_text}&quot;,
    response_model=ProductCatalog
)

print(f&quot;Total products: {catalog.total_count}&quot;)
for product in catalog.products:
    status = &quot;In Stock&quot; if product.in_stock else &quot;Out of Stock&quot;
    print(f&quot;- {product.name}: ${product.price} ({product.category}) - {status}&quot;)
</code></pre></div>
<h3 id="code-review-analysis">Code Review Analysis</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from pydantic import BaseModel
from typing import List
from enum import Enum

class Severity(str, Enum):
    LOW = &quot;low&quot;
    MEDIUM = &quot;medium&quot;
    HIGH = &quot;high&quot;
    CRITICAL = &quot;critical&quot;

class CodeIssue(BaseModel):
    line_number: int
    severity: Severity
    issue_type: str
    description: str
    suggestion: str

class CodeReview(BaseModel):
    language: str
    overall_quality: str
    issues: List[CodeIssue]
    recommendations: List[str]

llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)

code_to_review = '''
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)

def process_data(data):
    if data == None:
        return []
    result = []
    for item in data:
        result.append(item * 2)
    return result
'''

review = llm.generate(
    f&quot;Review this Python code for issues:\n{code_to_review}&quot;,
    response_model=CodeReview
)

print(f&quot;Language: {review.language}&quot;)
print(f&quot;Overall Quality: {review.overall_quality}&quot;)
print(f&quot;\nIssues Found ({len(review.issues)}):&quot;)
for issue in review.issues:
    print(f&quot;  Line {issue.line_number}: [{issue.severity.upper()}] {issue.issue_type}&quot;)
    print(f&quot;    Problem: {issue.description}&quot;)
    print(f&quot;    Fix: {issue.suggestion}\n&quot;)

print(&quot;Recommendations:&quot;)
for rec in review.recommendations:
    print(f&quot;  - {rec}&quot;)
</code></pre></div>
<h2 id="streaming-examples">Streaming Examples</h2>
<h3 id="basic-streaming-unified-2025">Basic Streaming (Unified 2025)</h3>
<div class="code-block"><pre><code class="language-python"># Streaming uses a unified processor across providers (exact chunking depends on the backend)
from abstractcore import create_llm

llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

print(&quot;AI Story Generator: &quot;, end=&quot;&quot;, flush=True)
for chunk in llm.generate(
    &quot;Write a short story about a programmer who discovers their code is alive&quot;,
    stream=True
):
    print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)
print(&quot;\n&quot;)
</code></pre></div>
<h3 id="advanced-streaming-with-progress-and-performance-tracking">Advanced Streaming with Progress and Performance Tracking</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
import time

def streaming_with_insights(prompt):
    # Supports any provider: OpenAI, Anthropic, Ollama, MLX
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

    print(&quot;Generating response...&quot;)

    start_time = time.time()
    chunks = []

    print(&quot;Response: &quot;, end=&quot;&quot;, flush=True)
    for chunk in llm.generate(prompt, stream=True):
        chunks.append(chunk)
        print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)

        # Optional real-time performance insights
        if len(chunks) % 10 == 0:
            current_time = time.time() - start_time
            chars_generated = sum(len(c.content or &quot;&quot;) for c in chunks)
            print(f&quot;\n[PROGRESS] {len(chunks)} chunks, {chars_generated} chars, {current_time:.1f}s&quot;)

    # Final performance summary
    total_time = time.time() - start_time
    total_chars = sum(len(chunk.content or &quot;&quot;) for chunk in chunks)

    print(f&quot;\n\n[STATS] Streaming Performance:&quot;)
    print(f&quot;- Total Chunks: {len(chunks)}&quot;)
    print(f&quot;- Total Characters: {total_chars}&quot;)
    print(f&quot;- Duration: {total_time:.2f}s&quot;)
    print(f&quot;- Speed: {total_chars/total_time:.0f} chars/sec&quot;)

# Usage with various prompts
streaming_with_insights(&quot;Explain quantum computing in simple terms&quot;)
</code></pre></div>
<h3 id="real-time-streaming-with-tools-unified-implementation">Real-Time Streaming with Tools (Unified Implementation)</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from datetime import datetime

def get_current_time() -&gt; str:
    &quot;&quot;&quot;Get the current time.&quot;&quot;&quot;
    return datetime.now().strftime(&quot;%H:%M:%S&quot;)

def get_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Get current weather for a city.&quot;&quot;&quot;
    weather_data = {
        &quot;New York&quot;: &quot;Sunny, 22¬∞C&quot;,
        &quot;London&quot;: &quot;Cloudy, 15¬∞C&quot;,
        &quot;Tokyo&quot;: &quot;Partly cloudy, 25¬∞C&quot;
    }
    return weather_data.get(city, f&quot;Weather data unavailable for {city}&quot;)

time_tool = {
    &quot;name&quot;: &quot;get_current_time&quot;,
    &quot;description&quot;: &quot;Get the current time&quot;,
    &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}}
}

weather_tool = {
    &quot;name&quot;: &quot;get_weather&quot;,
    &quot;description&quot;: &quot;Get current weather for a city&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;city&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Name of the city&quot;}
        }
    }
}

# Works similarly across providers (exact chunking depends on the backend)
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)

print(&quot;AI Assistant: &quot;, end=&quot;&quot;, flush=True)
for chunk in llm.generate(
    &quot;What time is it right now? And can you tell me the weather in New York?&quot;,
    tools=[time_tool, weather_tool],
    stream=True
):
    # Real-time chunk processing and tool call detection
    print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)

    # Tool calls are surfaced as structured dicts; execute them in your host/runtime.
    if chunk.tool_calls:
        print(f&quot;\n[TOOL] Tool calls: {chunk.tool_calls}&quot;)

print(&quot;\n&quot;)  # Newline after streaming

# Notes:
# - Real-time tool call detection
# - Streams chunks as they arrive (minimal buffering)
# - Works with OpenAI, Anthropic, Ollama, MLX (provider-dependent details)
</code></pre></div>
<h3 id="performance-optimized-streaming">Performance-Optimized Streaming</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
import time

def compare_providers(prompt):
    &quot;&quot;&quot;Compare streaming performance across providers.&quot;&quot;&quot;
    providers = [
        (&quot;openai&quot;, &quot;gpt-4o-mini&quot;),
        (&quot;anthropic&quot;, &quot;claude-haiku-4-5&quot;),
        (&quot;ollama&quot;, &quot;qwen3:4b-instruct&quot;)
    ]

    for provider, model in providers:
        try:
            llm = create_llm(provider, model=model)

            print(f&quot;\n[TEST] {provider.upper()} - {model}&quot;)
            start_time = time.time()

            chunks = []
            for chunk in llm.generate(prompt, stream=True):
                chunks.append(chunk)
                print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)

            total_time = time.time() - start_time
            total_chars = sum(len(chunk.content or &quot;&quot;) for chunk in chunks)

            print(f&quot;\n\n[PERF] {provider.upper()} Performance:&quot;)
            print(f&quot;- Chunks: {len(chunks)}&quot;)
            print(f&quot;- Characters: {total_chars}&quot;)
            print(f&quot;- Duration: {total_time:.2f}s&quot;)
            print(f&quot;- Speed: {total_chars/total_time:.0f} chars/sec&quot;)

        except Exception as e:
            print(f&quot;[ERROR] {provider} failed: {e}&quot;)

# Compare streaming performance
compare_providers(&quot;Write a creative short story about artificial intelligence&quot;)
</code></pre></div>
<p><strong>Streaming Features</strong>:
- Time-to-first-token depends on provider/model/network
- Unified strategy across all providers
- Real-time tool call detection
- Streams chunks as they arrive (minimal buffering)
- Supports: OpenAI, Anthropic, Ollama, MLX, LMStudio, HuggingFace
- Robust error handling for malformed responses</p>
<h2 id="session-management">Session Management</h2>
<h3 id="basic-conversation">Basic Conversation</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, BasicSession

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
session = BasicSession(
    provider=llm,
    system_prompt=&quot;You are a helpful coding tutor. Always provide examples.&quot;
)

# Multi-turn conversation
print(&quot;=== Conversation Start ===&quot;)

response1 = session.generate(&quot;Hi, I'm learning Python. What are decorators?&quot;)
print(&quot;User: Hi, I'm learning Python. What are decorators?&quot;)
print(f&quot;AI: {response1.content}\n&quot;)

response2 = session.generate(&quot;Can you show me a practical example?&quot;)
print(&quot;User: Can you show me a practical example?&quot;)
print(f&quot;AI: {response2.content}\n&quot;)

response3 = session.generate(&quot;What was my first question?&quot;)
print(&quot;User: What was my first question?&quot;)
print(f&quot;AI: {response3.content}\n&quot;)

print(f&quot;Total messages in conversation: {len(session.messages)}&quot;)
</code></pre></div>
<h3 id="session-persistence">Session Persistence</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, BasicSession
from pathlib import Path

# Create and use session
llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
session = BasicSession(
    provider=llm,
    system_prompt=&quot;You are a travel advisor. Help plan trips.&quot;
)

# Have a conversation
session.generate(&quot;I want to plan a trip to Japan&quot;)
session.generate(&quot;I'm interested in both modern cities and traditional culture&quot;)
session.generate(&quot;My budget is around $3000 for 10 days&quot;)

# Save session
session_file = Path(&quot;travel_planning_session.json&quot;)
session.save(session_file)
print(f&quot;Session saved to {session_file}&quot;)

# Later: Load session and continue
new_session = BasicSession.load(session_file, provider=llm)
response = new_session.generate(&quot;What were we discussing?&quot;)
print(f&quot;AI remembers: {response.content}&quot;)

# Clean up
session_file.unlink()  # Delete the file
</code></pre></div>
<h3 id="context-management">Context Management</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, BasicSession

def create_coding_assistant():
    &quot;&quot;&quot;Create a specialized coding assistant session.&quot;&quot;&quot;
    llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)

    system_prompt = &quot;&quot;&quot;
    You are an expert Python coding assistant. For each request:
    1. Provide working code examples
    2. Explain the code clearly
    3. Mention potential issues or improvements
    4. Keep responses concise but complete
    &quot;&quot;&quot;

    return BasicSession(provider=llm, system_prompt=system_prompt)

# Usage
assistant = create_coding_assistant()

# The assistant will remember the context throughout the conversation
assistant.generate(&quot;I need a function to validate email addresses&quot;)
assistant.generate(&quot;Now add logging to that function&quot;)
assistant.generate(&quot;How would I test this function?&quot;)

print(f&quot;Conversation history: {len(assistant.messages)} messages&quot;)

# Clear history but keep system prompt
assistant.clear_history()
print(f&quot;After clearing: {len(assistant.messages)} messages&quot;)  # Just system prompt remains
</code></pre></div>
<h2 id="interaction-tracing-observability">Interaction Tracing (Observability)</h2>
<h3 id="basic-tracing">Basic Tracing</h3>
<p>Enable tracing to capture complete LLM interaction history for debugging and transparency:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Enable tracing on provider
llm = create_llm(
    'openai',
    model='gpt-4o-mini',
    enable_tracing=True,
    max_traces=100  # Keep last 100 interactions (ring buffer)
)

# Generate with custom metadata
response = llm.generate(
    &quot;Explain quantum computing&quot;,
    temperature=0.7,
    trace_metadata={
        'user_id': 'user_123',
        'session_type': 'educational',
        'topic': 'quantum_physics'
    }
)

# Access trace by ID
trace_id = response.metadata['trace_id']
trace = llm.get_traces(trace_id=trace_id)

print(f&quot;Trace ID: {trace['trace_id']}&quot;)
print(f&quot;Timestamp: {trace['timestamp']}&quot;)
print(f&quot;Prompt: {trace['prompt']}&quot;)
print(f&quot;Response: {trace['response']['content'][:100]}...&quot;)
print(f&quot;Tokens: {trace['response']['usage']['total_tokens']}&quot;)
print(f&quot;Time: {trace['response']['generation_time_ms']:.2f}ms&quot;)
print(f&quot;Custom metadata: {trace['metadata']}&quot;)
</code></pre></div>
<h3 id="session-level-tracing">Session-Level Tracing</h3>
<p>Automatically track all interactions in a session with correlation:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.session import BasicSession

llm = create_llm('openai', model='gpt-4o-mini', enable_tracing=True)
session = BasicSession(provider=llm, enable_tracing=True)

# All interactions automatically traced
session.generate(&quot;What is Python?&quot;)
session.generate(&quot;Give me an example&quot;)
session.generate(&quot;Explain list comprehensions&quot;)

# Get all session traces
traces = session.get_interaction_history()

print(f&quot;\nSession ID: {session.id}&quot;)
print(f&quot;Total interactions: {len(traces)}&quot;)

for i, trace in enumerate(traces, 1):
    print(f&quot;\nInteraction {i}:&quot;)
    print(f&quot;  Prompt: {trace['prompt']}&quot;)
    print(f&quot;  Tokens: {trace['response']['usage']['total_tokens']}&quot;)
    print(f&quot;  Time: {trace['response']['generation_time_ms']:.0f}ms&quot;)
    print(f&quot;  Session ID: {trace['metadata']['session_id']}&quot;)
</code></pre></div>
<h3 id="multi-step-workflow-with-retries">Multi-Step Workflow with Retries</h3>
<p>Track code generation workflows with retry attempts:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.session import BasicSession

llm = create_llm('openai', model='gpt-4o-mini', enable_tracing=True)
session = BasicSession(provider=llm, enable_tracing=True)

# Step 1: Generate code
response = session.generate(
    &quot;Write a Python function to calculate fibonacci numbers&quot;,
    system_prompt=&quot;You are a Python code generator. Only output code.&quot;,
    step_type='code_generation',
    attempt_number=1,
    temperature=0
)

code = response.content
success = False

# Step 2-4: Execute with retry logic
for attempt in range(1, 4):
    try:
        exec(code)  # Simulate execution
        success = True
        break
    except Exception as e:
        # Retry with error context
        response = session.generate(
            f&quot;Previous code failed: {e}. Fix it.&quot;,
            step_type='code_generation',
            attempt_number=attempt + 1,
            temperature=0
        )
        code = response.content

# Get workflow summary
traces = session.get_interaction_history()

print(f&quot;\nWorkflow Summary:&quot;)
print(f&quot;Total attempts: {len(traces)}&quot;)
print(f&quot;Final status: {'Success' if success else 'Failed'}&quot;)

for trace in traces:
    step = trace['metadata']['step_type']
    attempt = trace['metadata']['attempt_number']
    tokens = trace['response']['usage']['total_tokens']
    print(f&quot;  {step} (Attempt {attempt}): {tokens} tokens&quot;)
</code></pre></div>
<h3 id="export-traces">Export Traces</h3>
<p>Export traces to different formats for analysis:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.utils import export_traces, summarize_traces

llm = create_llm('openai', model='gpt-4o-mini', enable_tracing=True)

# Generate some interactions
for i in range(5):
    llm.generate(f&quot;Question {i+1}&quot;, temperature=0)

traces = llm.get_traces()

# Export to JSONL (one JSON per line)
export_traces(traces, format='jsonl', file_path='traces.jsonl')

# Export to pretty JSON
export_traces(traces, format='json', file_path='traces.json')

# Export to Markdown report
export_traces(traces, format='markdown', file_path='trace_report.md')

# Get summary statistics
summary = summarize_traces(traces)
print(f&quot;\nSummary:&quot;)
print(f&quot;  Total interactions: {summary['total_interactions']}&quot;)
print(f&quot;  Total tokens: {summary['total_tokens']}&quot;)
print(f&quot;  Average tokens: {summary['avg_tokens_per_interaction']:.0f}&quot;)
print(f&quot;  Total time: {summary['total_time_ms']:.2f}ms&quot;)
print(f&quot;  Average time: {summary['avg_time_ms']:.2f}ms&quot;)
print(f&quot;  Providers: {summary['providers']}&quot;)
print(f&quot;  Models: {summary['models']}&quot;)
</code></pre></div>
<h3 id="retrieve-specific-traces">Retrieve Specific Traces</h3>
<p>Different ways to retrieve traces:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm('openai', model='gpt-4o-mini', enable_tracing=True)

# Generate some interactions
for i in range(10):
    llm.generate(f&quot;Test {i}&quot;, temperature=0)

# Get all traces
all_traces = llm.get_traces()
print(f&quot;Total traces: {len(all_traces)}&quot;)

# Get last 5 traces
recent = llm.get_traces(last_n=5)
print(f&quot;Last 5 prompts: {[t['prompt'] for t in recent]}&quot;)

# Get specific trace by ID
response = llm.generate(&quot;Specific query&quot;, temperature=0)
trace_id = response.metadata['trace_id']
trace = llm.get_traces(trace_id=trace_id)
print(f&quot;Specific trace: {trace['prompt']}&quot;)
</code></pre></div>
<p><a href="interaction-tracing.html">Learn more about Interaction Tracing</a></p>
<h2 id="production-patterns">Production Patterns</h2>
<h3 id="retry-and-error-handling">Retry and Error Handling</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig
from abstractcore.exceptions import ProviderAPIError, RateLimitError
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_production_llm():
    &quot;&quot;&quot;Create LLM with production-grade retry configuration.&quot;&quot;&quot;
    retry_config = RetryConfig(
        max_attempts=3,
        initial_delay=1.0,
        max_delay=30.0,
        use_jitter=True,
        failure_threshold=5
    )

    return create_llm(
        &quot;openai&quot;,
        model=&quot;gpt-4o-mini&quot;,
        retry_config=retry_config,
        timeout=30
    )

def safe_generate(prompt, **kwargs):
    &quot;&quot;&quot;Generate with comprehensive error handling.&quot;&quot;&quot;
    llm = create_production_llm()

    try:
        logger.info(f&quot;Generating response for prompt: {prompt[:50]}...&quot;)
        response = llm.generate(prompt, **kwargs)
        logger.info(f&quot;Response generated successfully: {len(response.content)} chars&quot;)
        return response

    except RateLimitError as e:
        logger.warning(f&quot;Rate limited: {e}&quot;)
        raise

    except ProviderAPIError as e:
        logger.error(f&quot;API error: {e}&quot;)
        raise

    except Exception as e:
        logger.error(f&quot;Unexpected error: {e}&quot;)
        raise

# Usage
try:
    response = safe_generate(&quot;What is machine learning?&quot;)
    print(response.content)
except Exception as e:
    print(f&quot;Generation failed: {e}&quot;)
</code></pre></div>
<h3 id="cost-monitoring">Cost Monitoring</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.events import EventType, on_global
from datetime import datetime
import json

class CostMonitor:
    def __init__(self, budget_limit=10.0):
        self.total_cost = 0.0
        self.budget_limit = budget_limit
        self.requests = []

        # Register event handlers
        on_global(EventType.GENERATION_COMPLETED, self.track_cost)

    def track_cost(self, event):
        &quot;&quot;&quot;Track costs from generation events.&quot;&quot;&quot;
        cost = event.data.get(&quot;cost_usd&quot;)
        if cost:
            # NOTE: `cost_usd` is a best-effort estimate based on token usage.
            cost_f = float(cost)
            self.total_cost += cost_f
            self.requests.append({
                'timestamp': event.timestamp.isoformat(),
                'provider': event.data.get('provider'),
                'model': event.data.get('model'),
                'cost_usd': cost_f,
                'tokens_input': event.data.get('tokens_input'),
                'tokens_output': event.data.get('tokens_output')
            })

            print(f&quot;[COST] ${cost_f:.4f} | Total: ${self.total_cost:.4f}&quot;)

            if self.total_cost &gt; self.budget_limit:
                print(f&quot;[WARN] BUDGET EXCEEDED: ${self.total_cost:.4f} &gt; ${self.budget_limit}&quot;)

    def get_report(self):
        &quot;&quot;&quot;Get cost report.&quot;&quot;&quot;
        return {
            'total_cost': self.total_cost,
            'budget_limit': self.budget_limit,
            'total_requests': len(self.requests),
            'average_cost': self.total_cost / len(self.requests) if self.requests else 0,
            'requests': self.requests
        }

# Usage
monitor = CostMonitor(budget_limit=1.0)  # $1 budget

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# Make some requests
for i in range(3):
    response = llm.generate(f&quot;Tell me a fact about number {i+1}&quot;)
    print(f&quot;Fact {i+1}: {response.content[:100]}...\n&quot;)

# Get report
report = monitor.get_report()
print(f&quot;\n[REPORT] Final Cost Summary:&quot;)
print(f&quot;Total cost: ${report['total_cost']:.4f}&quot;)
print(f&quot;Requests: {report['total_requests']}&quot;)
print(f&quot;Average per request: ${report['average_cost']:.4f}&quot;)
</code></pre></div>
<h3 id="load-balancing">Load Balancing</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
import random
import time
from typing import List, Tuple

class LoadBalancer:
    def __init__(self, providers: List[Tuple[str, str]]):
        &quot;&quot;&quot;Initialize with list of (provider, model) tuples.&quot;&quot;&quot;
        self.providers = []
        self.weights = []

        for provider_name, model in providers:
            try:
                llm = create_llm(provider_name, model=model)
                self.providers.append((llm, provider_name, model))
                self.weights.append(1.0)  # Equal weight initially
                print(f&quot;[OK] {provider_name} ({model}) ready&quot;)
            except Exception as e:
                print(f&quot;[FAIL] {provider_name} ({model}) failed: {e}&quot;)

    def generate(self, prompt, **kwargs):
        &quot;&quot;&quot;Generate using weighted random selection.&quot;&quot;&quot;
        if not self.providers:
            raise Exception(&quot;No providers available&quot;)

        # Weighted random selection
        provider_data = random.choices(
            self.providers,
            weights=self.weights,
            k=1
        )[0]

        llm, provider_name, model = provider_data

        try:
            start_time = time.time()
            response = llm.generate(prompt, **kwargs)
            duration = time.time() - start_time

            print(f&quot;[OK] {provider_name} responded in {duration:.2f}s&quot;)
            return response

        except Exception as e:
            print(f&quot;[FAIL] {provider_name} failed: {e}&quot;)
            # Remove failed provider temporarily
            idx = self.providers.index(provider_data)
            self.weights[idx] *= 0.1  # Reduce weight dramatically
            raise

# Usage
balancer = LoadBalancer([
    (&quot;openai&quot;, &quot;gpt-4o-mini&quot;),
    (&quot;anthropic&quot;, &quot;claude-haiku-4-5&quot;),
    (&quot;ollama&quot;, &quot;qwen3:4b-instruct&quot;)
])

# Make requests - they'll be distributed across available providers
for i in range(5):
    try:
        response = balancer.generate(f&quot;Tell me about topic number {i+1}&quot;)
        print(f&quot;Response {i+1}: {response.content[:50]}...\n&quot;)
    except Exception as e:
        print(f&quot;Request {i+1} failed: {e}\n&quot;)
</code></pre></div>
<h2 id="integration-examples">Integration Examples</h2>
<h3 id="fastapi-integration">FastAPI Integration</h3>
<div class="code-block"><pre><code class="language-python">from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from abstractcore import create_llm, BasicSession
from typing import Optional
import uuid

app = FastAPI(title=&quot;AbstractCore API&quot;)

# Global LLM instance
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# Store sessions in memory (use Redis in production)
sessions = {}

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    system_prompt: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str

@app.post(&quot;/chat&quot;, response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # Get or create session
        if request.session_id and request.session_id in sessions:
            session = sessions[request.session_id]
        else:
            session_id = request.session_id or str(uuid.uuid4())
            session = BasicSession(
                provider=llm,
                system_prompt=request.system_prompt or &quot;You are a helpful assistant.&quot;
            )
            sessions[session_id] = session

        # Generate response
        response = session.generate(request.message)

        return ChatResponse(
            response=response.content,
            session_id=session_id
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete(&quot;/sessions/{session_id}&quot;)
async def clear_session(session_id: str):
    if session_id in sessions:
        del sessions[session_id]
        return {&quot;message&quot;: &quot;Session cleared&quot;}
    raise HTTPException(status_code=404, detail=&quot;Session not found&quot;)

# Run with: uvicorn main:app --reload
if __name__ == &quot;__main__&quot;:
    import uvicorn
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)
</code></pre></div>
<h3 id="gradio-web-interface">Gradio Web Interface</h3>
<div class="code-block"><pre><code class="language-python">import gradio as gr
from abstractcore import create_llm, BasicSession
from typing import List, Tuple

class ChatInterface:
    def __init__(self):
        self.llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
        self.session = BasicSession(
            provider=self.llm,
            system_prompt=&quot;You are a helpful AI assistant.&quot;
        )

    def chat(self, message: str, history: List[Tuple[str, str]]) -&gt; Tuple[str, List[Tuple[str, str]]]:
        &quot;&quot;&quot;Handle chat interaction.&quot;&quot;&quot;
        try:
            response = self.session.generate(message)
            history.append((message, response.content))
            return &quot;&quot;, history
        except Exception as e:
            history.append((message, f&quot;Error: {str(e)}&quot;))
            return &quot;&quot;, history

    def clear(self) -&gt; Tuple[str, List]:
        &quot;&quot;&quot;Clear conversation history.&quot;&quot;&quot;
        self.session.clear_history()
        return &quot;&quot;, []

# Create interface
chat_interface = ChatInterface()

with gr.Blocks(title=&quot;AbstractCore Chat&quot;) as demo:
    gr.Markdown(&quot;# AbstractCore Chat Interface&quot;)

    chatbot = gr.Chatbot(label=&quot;Conversation&quot;, height=400)
    msg = gr.Textbox(
        label=&quot;Message&quot;,
        placeholder=&quot;Type your message here...&quot;,
        lines=2
    )

    with gr.Row():
        submit = gr.Button(&quot;Send&quot;, variant=&quot;primary&quot;)
        clear = gr.Button(&quot;Clear&quot;, variant=&quot;secondary&quot;)

    # Event handlers
    msg.submit(
        chat_interface.chat,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot]
    )

    submit.click(
        chat_interface.chat,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot]
    )

    clear.click(
        chat_interface.clear,
        outputs=[msg, chatbot]
    )

if __name__ == &quot;__main__&quot;:
    demo.launch(share=True)
</code></pre></div>
<h3 id="jupyter-notebook-integration">Jupyter Notebook Integration</h3>
<div class="code-block"><pre><code class="language-python"># Cell 1: Setup
from abstractcore import create_llm
from IPython.display import display, Markdown, HTML
import json

# Create LLM instance
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

def display_response(response, title=&quot;AI Response&quot;):
    &quot;&quot;&quot;Pretty display for Jupyter notebooks.&quot;&quot;&quot;
    html = f&quot;&quot;&quot;
    &lt;div style=&quot;border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px;&quot;&gt;
        &lt;h4 style=&quot;color: #333; margin-top: 0;&quot;&gt;{title}&lt;/h4&gt;
        &lt;p style=&quot;line-height: 1.6;&quot;&gt;{response.content}&lt;/p&gt;
    &lt;/div&gt;
    &quot;&quot;&quot;
    display(HTML(html))

print(&quot;AbstractCore setup complete!&quot;)

# Cell 2: Basic Usage
response = llm.generate(&quot;Explain quantum computing in simple terms&quot;)
display_response(response, &quot;Quantum Computing Explanation&quot;)

# Cell 3: Structured Output
from pydantic import BaseModel
from typing import List

class LearningPlan(BaseModel):
    topic: str
    difficulty: str
    estimated_hours: int
    prerequisites: List[str]
    learning_steps: List[str]

plan = llm.generate(
    &quot;Create a learning plan for someone who wants to learn machine learning&quot;,
    response_model=LearningPlan
)

# Display as nice table
display(HTML(f&quot;&quot;&quot;
&lt;table style=&quot;border-collapse: collapse; width: 100%;&quot;&gt;
    &lt;tr&gt;&lt;td&gt;&lt;strong&gt;Topic:&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;{plan.topic}&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;&lt;strong&gt;Difficulty:&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;{plan.difficulty}&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;&lt;strong&gt;Estimated Hours:&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;{plan.estimated_hours}&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;{', '.join(plan.prerequisites)}&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&quot;&quot;&quot;))

display(Markdown(&quot;### Learning Steps:&quot;))
for i, step in enumerate(plan.learning_steps, 1):
    display(Markdown(f&quot;{i}. {step}&quot;))
</code></pre></div>
<h3 id="discord-bot-integration">Discord Bot Integration</h3>
<div class="code-block"><pre><code class="language-python">import discord
from discord.ext import commands
from abstractcore import create_llm, BasicSession
import asyncio

# Bot setup
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix='!', intents=intents)

# LLM setup
llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
sessions = {}  # Store user sessions

@bot.event
async def on_ready():
    print(f'{bot.user} has connected to Discord!')

@bot.command(name='ask')
async def ask(ctx, *, question):
    &quot;&quot;&quot;Ask the AI a question.&quot;&quot;&quot;
    user_id = ctx.author.id

    # Get or create session for user
    if user_id not in sessions:
        sessions[user_id] = BasicSession(
            provider=llm,
            system_prompt=&quot;You are a helpful Discord bot assistant. Keep responses concise.&quot;
        )

    try:
        # Show typing indicator
        async with ctx.typing():
            response = sessions[user_id].generate(question)

        # Discord has a 2000 character limit
        content = response.content
        if len(content) &gt; 2000:
            content = content[:1997] + &quot;...&quot;

        await ctx.reply(content)

    except Exception as e:
        await ctx.reply(f&quot;Sorry, I encountered an error: {str(e)}&quot;)

@bot.command(name='clear')
async def clear_session(ctx):
    &quot;&quot;&quot;Clear your conversation history.&quot;&quot;&quot;
    user_id = ctx.author.id
    if user_id in sessions:
        sessions[user_id].clear_history()
        await ctx.reply(&quot;Your conversation history has been cleared!&quot;)
    else:
        await ctx.reply(&quot;You don't have an active session to clear.&quot;)

@bot.command(name='stats')
async def stats(ctx):
    &quot;&quot;&quot;Show session statistics.&quot;&quot;&quot;
    user_id = ctx.author.id
    if user_id in sessions:
        session = sessions[user_id]
        message_count = len(session.messages)
        await ctx.reply(f&quot;Your session has {message_count} messages.&quot;)
    else:
        await ctx.reply(&quot;You don't have an active session.&quot;)

# Run bot (add your Discord bot token)
# bot.run('YOUR_DISCORD_BOT_TOKEN')
</code></pre></div>
<h2 id="next-steps">Next Steps</h2>
<p>These examples show AbstractCore's versatility across different use cases. To continue learning:</p>
<ol>
<li><strong>Start with basics</strong> - Try the simple Q&amp;A examples</li>
<li><strong>Add tools</strong> - Experiment with the tool calling examples</li>
<li><strong>Structure output</strong> - Use Pydantic models for type-safe responses</li>
<li><strong>Go production</strong> - Implement error handling and monitoring</li>
<li><strong>Build apps</strong> - Use the integration examples as starting points</li>
</ol>
<p>For more information:
- <a href="getting-started.html">Getting Started</a> - Basic setup and usage
- <a href="capabilities.html">Capabilities</a> - What AbstractCore can do
- <a href="prerequisites.html">Prerequisites</a> - Provider setup and configuration
- <a href="api-reference.html">API Reference</a> - Complete API documentation</p>
<hr />
<p><strong>Remember</strong>: All these examples work with any provider - just change the <code>create_llm()</code> call to switch between OpenAI, Anthropic, Ollama, MLX, and others!</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
