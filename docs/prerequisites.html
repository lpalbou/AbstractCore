<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prerequisites & Setup - AbstractCore</title>
    <meta name="description" content="Complete setup guide for AbstractCore with all LLM providers - OpenAI, Anthropic, Ollama, MLX, LMStudio, and HuggingFace.">
    <link rel="icon" type="image/x-icon" href="../assets/favicon.ico">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    
    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>
    
    <script>
        // Initialize navbar with docs-specific configuration
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '../',
                menuItems: [
                    { 
                        text: 'GitHub', 
                        href: 'https://github.com/lpalbou/AbstractCore',
                        target: '_blank',
                        icon: 'github'
                    },
                    { 
                        text: 'PyPI', 
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Prerequisites & Setup Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Complete setup guide for AbstractCore with all LLM providers. Choose the provider(s) that best fit your needs - you can use multiple providers in the same application.
                </p>

            <!-- Quick Decision Guide -->
            <div style="background: linear-gradient(135deg, var(--primary-color), var(--secondary-color)); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem; color: white;">
                <h2 style="margin: 0 0 1.5rem 0;">Quick Decision Guide</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem;">
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want to get started immediately?</strong><br>
                        <a href="#openai-setup" style="color: #fbbf24;">OpenAI Setup</a> (requires API key, costs ~$0.001-0.01 per request)
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want free local models?</strong><br>
                        <a href="#ollama-setup" style="color: #fbbf24;">Ollama Setup</a> (free, runs on your machine)
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Have Apple Silicon Mac?</strong><br>
                        <a href="#mlx-setup" style="color: #fbbf24;">MLX Setup</a> (optimized for M1/M2/M3 chips)
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want a GUI for local models?</strong><br>
                        <a href="#lmstudio-setup" style="color: #fbbf24;">LMStudio Setup</a> (easiest local setup)

            <div class="doc-content">
                <section id="core-installation">
                    <h2>Core Installation</h2>
                    <p>First, install AbstractCore with your preferred providers:</p>
                    
                    <div class="code-block">
                        <pre><code class="language-bash"># Option 1: Start with cloud providers (fastest setup)
pip install abstractcore[openai,anthropic]

# Option 2: Local models only (free, no API keys needed)
pip install abstractcore[ollama,mlx]

# Option 3: Everything (recommended for development)
pip install abstractcore[all]

# Option 4: Minimal install (add providers as needed)
pip install abstractcore

# For media handling (images, PDFs, Office docs)
pip install abstractcore[media]</code></pre>
                </section>

                <section id="centralized-config">
                    <h2>Recommended: Centralized Configuration</h2>
                    <p>After installation, configure AbstractCore once to avoid repetitive provider/model specifications:</p>

                    <h3>1. Check Current Status</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">abstractcore --status</code></pre>
                    </div>

                    <h3>2. Set Global Defaults (One-Time Setup)</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Set global fallback model
abstractcore --set-global-default ollama/llama3:8b

# Or use cloud provider as default
abstractcore --set-global-default openai/gpt-4o-mini

# Set app-specific defaults
abstractcore --set-app-default cli ollama qwen3:4b
abstractcore --set-app-default summarizer openai gpt-4o-mini</code></pre>
                    </div>

                    <h3>3. Configure API Keys (Optional but Recommended)</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Store API keys in config file instead of environment variables
abstractcore --set-api-key openai sk-your-key-here
abstractcore --set-api-key anthropic your-anthropic-key</code></pre>
                    </div>

                    <h3>4. Configure Vision Fallback (Optional)</h3>
                    <p>Enable text-only models to process images:</p>
                    <div class="code-block">
                        <pre><code class="language-bash"># Download local vision model (recommended)
abstractcore --download-vision-model

# Or use existing Ollama model
abstractcore --set-vision-caption qwen2.5vl:7b</code></pre>
                    </div>

                    <p><strong>Benefits:</strong> Never specify provider/model again, consistent behavior across apps, environment-specific configurations.</p>
                    <p>Learn more: <a href="centralized-config.html">Centralized Configuration Guide</a></p>
                </section>

                <section id="openai-setup">
                    <h2>OpenAI Setup</h2>
                    <p>OpenAI provides the most reliable and fastest models, including GPT-4 and GPT-3.5.</p>
                    
                    <h3>1. Get API Key</h3>
                    <ol>
                        <li>Visit <a href="https://platform.openai.com/api-keys" target="_blank">OpenAI API Keys</a></li>
                        <li>Create a new API key</li>
                        <li>Copy the key (starts with <code>sk-</code>)</li>
                    </ol>

                    <h3>2. Set Environment Variable</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Linux/macOS
export OPENAI_API_KEY="sk-your-key-here"

# Windows
set OPENAI_API_KEY=sk-your-key-here</code></pre>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <section id="anthropic-setup">
                    <h2>Anthropic Setup</h2>
                    <p>Anthropic's Claude models are excellent for reasoning and analysis tasks.</p>
                    
                    <h3>1. Get API Key</h3>
                    <ol>
                        <li>Visit <a href="https://console.anthropic.com/" target="_blank">Anthropic Console</a></li>
                        <li>Create an API key</li>
                        <li>Copy the key</li>
                    </ol>

                    <h3>2. Set Environment Variable</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Linux/macOS
export ANTHROPIC_API_KEY="your-key-here"

# Windows
set ANTHROPIC_API_KEY=your-key-here</code></pre>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("anthropic", model="claude-3-5-haiku-latest")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <section id="ollama-setup">
                    <h2>Ollama Setup</h2>
                    <p>Ollama provides free local models that run entirely on your machine.</p>
                    
                    <h3>1. Install Ollama</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download</code></pre>

                    <h3>2. Start Ollama Service</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">ollama serve</code></pre>

                    <h3>3. Download a Model</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Recommended models
ollama pull qwen3:4b-instruct-2507-q4_K_M  # Fast, good quality
ollama pull qwen3-coder:30b                 # Best for coding
ollama pull granite3.3:2b                   # Lightweight</code></pre>

                    <h3>4. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("ollama", model="qwen3:4b-instruct-2507-q4_K_M")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <section id="mlx-setup">
                    <h2>MLX Setup (Apple Silicon)</h2>
                    <p>MLX provides optimized models for Apple Silicon Macs (M1/M2/M3/M4).</p>
                    
                    <h3>1. Install MLX</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">pip install abstractcore[mlx]</code></pre>

                    <h3>2. Download a Model</h3>
                    <div class="code-block">
                        <pre><code class="language-python"># Models are downloaded automatically on first use
from abstractcore import create_llm

llm = create_llm("mlx", model="mlx-community/Qwen2.5-Coder-7B-Instruct-4bit")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <section id="lmstudio-setup">
                    <h2>LMStudio Setup</h2>
                    <p>LMStudio provides a user-friendly GUI for running local models.</p>
                    
                    <h3>1. Install LMStudio</h3>
                    <ol>
                        <li>Download from <a href="https://lmstudio.ai/" target="_blank">lmstudio.ai</a></li>
                        <li>Install and launch the application</li>
                        <li>Download a model through the GUI</li>
                    </ol>

                    <h3>2. Start Local Server</h3>
                    <ol>
                        <li>In LMStudio, go to "Local Server" tab</li>
                        <li>Select your model</li>
                        <li>Click "Start Server"</li>
                        <li>Note the server URL (usually http://localhost:1234)</li>
                    </ol>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("lmstudio", model="your-model-name")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <section id="huggingface-setup">
                    <h2>HuggingFace Setup</h2>
                    <p>Access thousands of open-source models through HuggingFace.</p>
                    
                    <h3>1. Install Dependencies</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">pip install abstractcore[huggingface]</code></pre>

                    <h3>2. Optional: Set HF Token</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># For private/gated models
export HUGGINGFACE_API_TOKEN="your-token-here"</code></pre>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("huggingface", model="microsoft/DialoGPT-medium")
response = llm.generate("Hello, world!")
print(response.content)</code></pre>
                </section>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Next Steps</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="centralized-config.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">⭐ Centralized Configuration</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Set up once, use everywhere - global defaults and preferences</p>
                        </a>

                        <a href="getting-started.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Getting Started</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Learn the basics and create your first LLM application</p>
                        </a>

                        <a href="media-handling-system.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">⭐ Media Handling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Attach images, PDFs, Office docs with simple syntax</p>
                        </a>

                        <a href="vision-capabilities.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">⭐ Vision Capabilities</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Image analysis with vision fallback for text-only models</p>
                        </a>

                        <a href="tool-calling.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Tool Calling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Universal tool support across all providers</p>
                        </a>

                        <a href="examples.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Examples</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Real-world usage examples and patterns</p>
                        </a>

                        <a href="api-reference.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">API Reference</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Complete API documentation</p>
                        </a>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
