<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prerequisites &amp; Setup Guide - AbstractCore</title>
    <meta name="description" content="This guide walks you through setting up AbstractCore with different LLM providers. Choose the provider(s) that are suitable for your needs ‚Äî you can use multiple providers in‚Ä¶">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Prerequisites &amp; Setup Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">This guide walks you through setting up AbstractCore with different LLM providers. Choose the provider(s) that are suitable for your needs ‚Äî you can use multiple providers in the same application.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#quick-decision-guide" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Decision Guide</a>
<a href="#core-installation" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Installation</a>
<a href="#cloud-provider-setup" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Cloud Provider Setup</a>
<a href="#local-provider-setup" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Local Provider Setup</a>
<a href="#troubleshooting" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Troubleshooting</a>
<a href="#testing-your-setup" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Testing Your Setup</a>
<a href="#security-notes" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Security Notes</a></div>

            <div class="doc-content">


<h2 id="quick-decision-guide">Quick Decision Guide</h2>
<p><strong>Want to get started immediately?</strong> ‚Üí <a href="#openai-setup">OpenAI Setup</a> (requires API key)</p>
<p><strong>Want free local models?</strong> ‚Üí <a href="#ollama-setup">Ollama Setup</a> (free, runs on your machine)</p>
<p><strong>Have Apple Silicon Mac?</strong> ‚Üí <a href="#mlx-setup">MLX Setup</a> (optimized for M1/M2/M3/M4 chips)</p>
<p><strong>Have NVIDIA GPU?</strong> ‚Üí <a href="#vllm-setup">vLLM Setup</a> (production GPU inference; NVIDIA CUDA only)</p>
<p><strong>Want a GUI for local models?</strong> ‚Üí <a href="#lmstudio-setup">LMStudio Setup</a> (easiest local setup)</p>
<p><strong>Want a gateway/proxy?</strong> ‚Üí <a href="#gateway-provider-setup-openrouter-portkey">Gateway Provider Setup</a> (OpenRouter/Portkey routing + governance)</p>
<p><strong>Using a custom OpenAI-compatible <code>/v1</code> endpoint?</strong> ‚Üí <a href="#openai-compatible-setup">OpenAI-Compatible Setup</a></p>
<h2 id="core-installation">Core Installation</h2>
<p>Install AbstractCore, then add the extras you need:</p>
<div class="code-block"><pre><code class="language-bash"># Core (small default)
pip install abstractcore

# Providers (only if you use them)
pip install "abstractcore[openai]"       # OpenAI SDK
pip install "abstractcore[anthropic]"    # Anthropic SDK
pip install "abstractcore[huggingface]"  # Transformers / torch (heavy)
pip install "abstractcore[mlx]"          # Apple Silicon only (heavy)
pip install "abstractcore[vllm]"         # NVIDIA CUDA/ROCm only (heavy)

# Optional features
pip install "abstractcore[tools]"       # built-in web tools (web_search, skim_websearch, skim_url, fetch_url)
pip install "abstractcore[media]"       # images, PDFs, Office docs
pip install "abstractcore[embeddings]"  # EmbeddingManager + local embedding models
pip install "abstractcore[tokens]"      # precise token counting (tiktoken)
pip install "abstractcore[server]"      # OpenAI-compatible HTTP gateway
pip install "abstractcore[compression]" # Glyph visual-text compression (Pillow renderer)

# Turnkey "everything" installs (pick one)
pip install "abstractcore[all-apple]"    # macOS/Apple Silicon (includes MLX, excludes vLLM)
pip install "abstractcore[all-non-mlx]"  # Linux/Windows/Intel Mac (excludes MLX and vLLM)
pip install "abstractcore[all-gpu]"      # Linux NVIDIA GPU (includes vLLM, excludes MLX)
</code></pre></div>
<p><strong>Hardware Notes:</strong>
- <code>[mlx]</code> - Only works on Apple Silicon (M1/M2/M3/M4)
- <code>[vllm]</code> - Only works with NVIDIA CUDA GPUs
- <code>[all-apple]</code> - Best for Apple Silicon (includes MLX, excludes vLLM)
- <code>[all-non-mlx]</code> - Best for Linux/Windows/Intel Mac (excludes MLX and vLLM)
- <code>[all-gpu]</code> - Best for Linux NVIDIA GPU (includes vLLM, excludes MLX)</p>
<h2 id="cloud-provider-setup">Cloud Provider Setup</h2>
<h3 id="openai-setup">OpenAI Setup</h3>
<p><strong>Best for</strong>: Production applications and OpenAI‚Äôs hosted models</p>
<h4 id="1-get-api-key">1. Get API Key</h4>
<ol>
<li>Go to <a href="https://platform.openai.com/api-keys">OpenAI API Dashboard</a></li>
<li>Create account or sign in</li>
<li>Click "Create new secret key"</li>
<li>Copy the key (starts with <code>sk-</code>)</li>
</ol>
<h4 id="2-set-environment-variable">2. Set Environment Variable</h4>
<div class="code-block"><pre><code class="language-bash"># Option 1: Export in terminal (temporary)
export OPENAI_API_KEY="sk-your-actual-api-key-here"

# Option 2: Add to ~/.bashrc or ~/.zshrc (permanent)
echo 'export OPENAI_API_KEY="sk-your-actual-api-key-here"' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Option 3: Create .env file in your project
echo 'OPENAI_API_KEY=sk-your-actual-api-key-here' &gt; .env
</code></pre></div>
<h4 id="3-test-setup">3. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Test with an example model (use any model available on your account)
llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate("Say hello in French")
print(response.content)  # Should output: "Bonjour!"
</code></pre></div>
<p><strong>Model names</strong>: Use any model supported by your account (examples: <code>gpt-4o-mini</code>, <code>gpt-4o</code>).</p>
<h3 id="anthropic-setup">Anthropic Setup</h3>
<p><strong>Best for</strong>: Claude models via Anthropic‚Äôs API</p>
<h4 id="1-get-api-key_1">1. Get API Key</h4>
<ol>
<li>Go to <a href="https://console.anthropic.com/">Anthropic Console</a></li>
<li>Create account or sign in</li>
<li>Go to "API Keys" section</li>
<li>Click "Create Key"</li>
<li>Copy the key (starts with <code>sk-ant-</code>)</li>
</ol>
<h4 id="2-set-environment-variable_1">2. Set Environment Variable</h4>
<div class="code-block"><pre><code class="language-bash"># Option 1: Export in terminal (temporary)
export ANTHROPIC_API_KEY="sk-ant-your-actual-api-key-here"

# Option 2: Add to shell profile (permanent)
echo 'export ANTHROPIC_API_KEY="sk-ant-your-actual-api-key-here"' &gt;&gt; ~/.bashrc
source ~/.bashrc

# Option 3: Create .env file
echo 'ANTHROPIC_API_KEY=sk-ant-your-actual-api-key-here' &gt; .env
</code></pre></div>
<h4 id="3-test-setup_1">3. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Test with an example model (use any model available on your account)
llm = create_llm("anthropic", model="claude-haiku-4-5")
response = llm.generate("Explain Python in one sentence")
print(response.content)
</code></pre></div>
<p><strong>Model names</strong>: Use any model supported by your account (examples: <code>claude-haiku-4-5</code>, <code>claude-sonnet-4-5</code>).</p>
<h3 id="gateway-provider-setup-openrouter-portkey">Gateway Provider Setup (OpenRouter, Portkey)</h3>
<p><strong>Best for</strong>: routing, observability/governance, and unified billing across multiple backends.</p>
<p>Gateways expose an OpenAI-compatible <code>/v1</code> endpoint and forward your payload to the routed backend model. Because some backends are strict (for example OpenAI reasoning families like gpt-5/o1 reject unsupported parameters), AbstractCore‚Äôs gateway providers forward optional generation parameters (like <code>temperature</code>, <code>top_p</code>, <code>max_output_tokens</code>) <strong>only when explicitly set</strong>.</p>
<h4 id="openrouter-setup">OpenRouter Setup</h4>
<ol>
<li>Create an API key: https://openrouter.ai/keys</li>
<li>Set the environment variable:</li>
</ol>
<div class="code-block"><pre><code class="language-bash">export OPENROUTER_API_KEY="sk-or-..."
# Optional override (default: https://openrouter.ai/api/v1)
export OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"
</code></pre></div>
<ol start="3">
<li>Test:</li>
</ol>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openrouter", model="openai/gpt-4o-mini")
resp = llm.generate("Say hello in French")
print(resp.content)
</code></pre></div>
<h4 id="portkey-setup">Portkey Setup</h4>
<p>Portkey routes requests using a <strong>config id</strong> (commonly <code>pcfg_...</code>).</p>
<ol>
<li>
<p>Create an API key and config in Portkey, then copy:
   - <code>PORTKEY_API_KEY</code>
   - <code>PORTKEY_CONFIG</code> (config id)</p>
</li>
<li>
<p>Set environment variables:</p>
</li>
</ol>
<div class="code-block"><pre><code class="language-bash">export PORTKEY_API_KEY="pk_..."
export PORTKEY_CONFIG="pcfg_..."
# Optional override (default: https://api.portkey.ai/v1)
export PORTKEY_BASE_URL="https://api.portkey.ai/v1"
</code></pre></div>
<ol start="3">
<li>Test:</li>
</ol>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("portkey", model="gpt-4o-mini", config_id="pcfg_...")
resp = llm.generate("Say hello in French")
print(resp.content)
</code></pre></div>
<h2 id="local-provider-setup">Local Provider Setup</h2>
<h3 id="ollama-setup">Ollama Setup</h3>
<p><strong>Best for</strong>: Privacy, no API keys, offline usage, customization</p>
<p><strong>Requirements</strong>: 8GB+ RAM, works on Mac/Linux/Windows</p>
<h4 id="1-install-ollama">1. Install Ollama</h4>
<p><strong>macOS:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
# OR download from https://ollama.com/download
</code></pre></div>
<p><strong>Linux:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre></div>
<p><strong>Windows:</strong>
1. Download installer from <a href="https://ollama.com/download">ollama.com/download</a>
2. Run the installer
3. Restart terminal</p>
<h4 id="2-start-ollama-service">2. Start Ollama Service</h4>
<div class="code-block"><pre><code class="language-bash"># Start Ollama server (runs in background)
ollama serve
</code></pre></div>
<h4 id="3-download-models">3. Download Models</h4>
<div class="code-block"><pre><code class="language-bash"># Pull any model you want to use, then verify it's installed.
ollama pull qwen3:4b-instruct-2507-q4_K_M
ollama list
</code></pre></div>
<h4 id="4-test-setup">4. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Test with any model you installed via `ollama pull ...`
llm = create_llm("ollama", model="qwen3:4b-instruct-2507-q4_K_M")
response = llm.generate("What is Python?")
print(response.content)
</code></pre></div>
<h3 id="mlx-setup">MLX Setup</h3>
<p><strong>Best for</strong>: M1/M2/M3/M4 Macs, optimized inference, good speed</p>
<p><strong>Requirements</strong>: Apple Silicon Mac (M1/M2/M3/M4)</p>
<h4 id="1-install-mlx-dependencies">1. Install MLX Dependencies</h4>
<div class="code-block"><pre><code class="language-bash"># MLX is automatically installed with AbstractCore
pip install "abstractcore[mlx]"
</code></pre></div>
<h4 id="2-download-models">2. Download Models</h4>
<p>MLX models are automatically downloaded when first used. Popular options:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Models are auto-downloaded on first use
llm = create_llm("mlx", model="mlx-community/Qwen2.5-Coder-7B-Instruct-4bit")  # 4.2GB
# OR
llm = create_llm("mlx", model="mlx-community/Llama-3.2-3B-Instruct-4bit")      # 1.8GB
</code></pre></div>
<h4 id="3-test-setup_2">3. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Test with a good balance model
llm = create_llm("mlx", model="mlx-community/Llama-3.2-3B-Instruct-4bit")
response = llm.generate("Explain machine learning briefly")
print(response.content)
</code></pre></div>
<p><strong>Popular MLX Models</strong>:
- <code>mlx-community/Llama-3.2-3B-Instruct-4bit</code> - 1.8GB, fast
- <code>mlx-community/Qwen2.5-Coder-7B-Instruct-4bit</code> - 4.2GB, suitable for code
- <code>mlx-community/Llama-3.1-8B-Instruct-4bit</code> - 4.7GB, high quality</p>
<h3 id="lmstudio-setup">LMStudio Setup</h3>
<p><strong>Best for</strong>: Easy GUI management, Windows users, non-technical users</p>
<p><strong>Requirements</strong>: 8GB+ RAM, works on Mac/Linux/Windows</p>
<h4 id="1-install-lmstudio">1. Install LMStudio</h4>
<ol>
<li>Download from <a href="https://lmstudio.ai/">lmstudio.ai</a></li>
<li>Install the application</li>
<li>Launch LMStudio</li>
</ol>
<h4 id="2-download-models_1">2. Download Models</h4>
<ol>
<li>Open LMStudio</li>
<li>Go to "Discover" tab</li>
<li>Search for recommended models:
   - <code>microsoft/Phi-3-mini-4k-instruct-gguf</code> (small, fast)
   - <code>microsoft/Phi-3-medium-4k-instruct-gguf</code> (medium quality)
   - <code>meta-llama/Llama-2-7b-chat-gguf</code> (good general purpose)</li>
<li>Click download for your preferred model</li>
</ol>
<h4 id="3-start-local-server">3. Start Local Server</h4>
<ol>
<li>Go to "Local Server" tab in LMStudio</li>
<li>Select your downloaded model</li>
<li>Click "Start Server"</li>
<li>Note the port (usually 1234)</li>
</ol>
<h4 id="4-test-setup_1">4. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# LM Studio exposes an OpenAI-compatible server (default: http://localhost:1234/v1).
# Use the model ID shown in LM Studio (or try "local-model" if unsure).
llm = create_llm("lmstudio", model="local-model", base_url="http://localhost:1234/v1")
resp = llm.generate("Hello, how are you?")
print(resp.content)
</code></pre></div>
<h3 id="huggingface-setup">HuggingFace Setup</h3>
<p><strong>Best for</strong>: Latest research models, custom models, GGUF files</p>
<p><strong>Requirements</strong>: 8GB+ RAM, Python environment</p>
<h4 id="1-install-dependencies">1. Install Dependencies</h4>
<div class="code-block"><pre><code class="language-bash">pip install "abstractcore[huggingface]"
</code></pre></div>
<h4 id="2-optional-get-huggingface-token">2. Optional: Get HuggingFace Token</h4>
<p>For private models or higher rate limits:</p>
<ol>
<li>Go to <a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a></li>
<li>Create a "Read" token</li>
<li>Set environment variable:</li>
</ol>
<div class="code-block"><pre><code class="language-bash">export HUGGINGFACE_TOKEN="hf_your-token-here"
</code></pre></div>
<h4 id="3-test-setup_3">3. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Use a small model for testing (auto-downloads)
llm = create_llm("huggingface", model="microsoft/DialoGPT-medium")
response = llm.generate("Hello there!")
print(response.content)
</code></pre></div>
<p><strong>Popular HuggingFace Models</strong>:
- <code>microsoft/DialoGPT-medium</code> - Good for conversation
- <code>facebook/blenderbot-400M-distill</code> - Conversational AI
- <code>microsoft/CodeBERT-base</code> - Code understanding</p>
<h3 id="vllm-setup">vLLM Setup</h3>
<p><strong>Best for</strong>: Production GPU deployments, high-throughput inference, tensor parallelism</p>
<p><strong>Requirements</strong>:
- <strong>NVIDIA GPU with CUDA support</strong> (A100, H100, RTX 4090, etc.)
- Linux operating system
- CUDA 12.1+ installed
- 16GB+ VRAM recommended
- <strong>NOT compatible with</strong>: Apple Silicon, AMD GPUs, CPU-only systems</p>
<p><strong>NVIDIA CUDA only.</strong> If you‚Äôre on Apple Silicon, use MLX. If you‚Äôre on CPU-only, use Ollama/HuggingFace.</p>
<h4 id="hardware-compatibility-warning">‚ö†Ô∏è Hardware Compatibility Warning</h4>
<p><strong>vLLM ONLY works with NVIDIA CUDA GPUs.</strong> It will NOT work on:
- ‚ùå Apple Silicon (M1/M2/M3/M4) - Use MLX provider instead
- ‚ùå AMD GPUs - Use HuggingFace or Ollama instead
- ‚ùå Intel integrated graphics
- ‚ùå CPU-only systems</p>
<h4 id="1-install-vllm">1. Install vLLM</h4>
<div class="code-block"><pre><code class="language-bash"># Install AbstractCore with vLLM support
pip install "abstractcore[vllm]"

# This installs vLLM which requires NVIDIA CUDA
# If you get CUDA errors, ensure CUDA 12.1+ is installed:
# https://developer.nvidia.com/cuda-downloads
</code></pre></div>
<h4 id="2-start-vllm-server">2. Start vLLM Server</h4>
<p><strong>IMPORTANT</strong>: Check your GPU setup first to avoid Out Of Memory (OOM) errors:</p>
<div class="code-block"><pre><code class="language-bash"># Check available GPUs
nvidia-smi

# Shows: GPU name, VRAM capacity, and current usage
# Example: 4x NVIDIA L4 (23GB each) = 92GB total
</code></pre></div>
<p><strong>Choose the right startup command based on your hardware:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Single GPU (24GB+) - Works for 7B-14B models
vllm serve Qwen/Qwen2.5-Coder-7B-Instruct --port 8000

# Single GPU (24GB+) - For 30B models, reduce memory
vllm serve Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --port 8000 \
    --gpu-memory-utilization 0.85 \
    --max-model-len 4096

# Multiple GPUs (RECOMMENDED for 30B models) - Use tensor parallelism
# Example: 4x NVIDIA L4 (23GB each)
vllm serve Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --host 0.0.0.0 --port 8000 \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --max-num-seqs 128

# Multiple GPUs + LoRA support (Production setup)
vllm serve Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --host 0.0.0.0 --port 8000 \
    --tensor-parallel-size 4 \
    --enable-lora --max-loras 4 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --max-num-seqs 128
</code></pre></div>
<p><strong>Key Parameters:</strong>
- <code>--tensor-parallel-size N</code> - Split model across N GPUs (REQUIRED for 30B+ models on &lt;40GB GPUs)
- <code>--gpu-memory-utilization 0.9</code> - Use 90% of GPU memory (leave 10% for CUDA overhead)
- <code>--max-model-len</code> - Maximum context length (reduce if OOM)
- <code>--max-num-seqs</code> - Maximum concurrent sequences (128 recommended for 30B models, default 256 may cause OOM)
- <code>--enable-lora</code> - Enable dynamic LoRA adapter loading
- <code>--max-loras</code> - Maximum number of LoRA adapters to keep in memory</p>
<p><strong>Troubleshooting OOM Errors:</strong></p>
<p>If you see <code>CUDA out of memory</code> errors:</p>
<ol>
<li><strong>Reduce concurrent sequences</strong>: <code>--max-num-seqs 128</code> (or 64, 32 for tighter memory)</li>
<li><strong>Enable tensor parallelism</strong>: <code>--tensor-parallel-size 2</code> (or 4, 8 depending on GPU count)</li>
<li><strong>Reduce memory usage</strong>: <code>--gpu-memory-utilization 0.85 --max-model-len 4096</code></li>
<li><strong>Use smaller model</strong>: <code>Qwen/Qwen2.5-Coder-7B-Instruct</code> instead of 30B</li>
<li><strong>Use quantized model</strong>: <code>Qwen/Qwen2.5-Coder-30B-Instruct-AWQ</code> (4-bit quantization)</li>
</ol>
<p><strong>Test server is running:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Check server health
curl http://localhost:8000/health

# List available models
curl http://localhost:8000/v1/models

# Test generation
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
    "messages": [{"role": "user", "content": "Say hello"}],
    "max_tokens": 50
  }'
</code></pre></div>
<h4 id="3-test-setup_4">3. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Basic generation
llm = create_llm("vllm", model="Qwen/Qwen3-Coder-30B-A3B-Instruct")
response = llm.generate("Write a Python function to sort a list")
print(response.content)

# With guided JSON (vLLM-specific feature)
response = llm.generate(
    "List 3 programming languages",
    guided_json={
        "type": "object",
        "properties": {
            "languages": {"type": "array", "items": {"type": "string"}}
        }
    }
)
print(response.content)
</code></pre></div>
<h4 id="4-vllm-specific-features">4. vLLM-Specific Features</h4>
<p><strong>Guided Decoding</strong> (syntax-constrained generation):</p>
<div class="code-block"><pre><code class="language-python"># Regex-constrained generation
response = llm.generate(
    "Write a Python function",
    guided_regex=r"def \w+\([^)]*\):\n(?:\s{4}.*\n)+"
)

# JSON schema enforcement
response = llm.generate(
    "Extract person info",
    guided_json={"type": "object", "properties": {...}}
)
</code></pre></div>
<p><strong>Multi-LoRA</strong> (1 base model ‚Üí many specialized agents):</p>
<div class="code-block"><pre><code class="language-python"># Load specialized adapters
llm.load_adapter("sql-expert", "/models/adapters/sql-lora")
llm.load_adapter("react-dev", "/models/adapters/react-lora")

# Route to specialized adapter
response = llm.generate("Write SQL query", model="sql-expert")
</code></pre></div>
<p><strong>Beam Search</strong> (higher accuracy for complex tasks):</p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(
    "Solve this complex algorithm problem...",
    use_beam_search=True,
    best_of=5  # Generate 5 candidates, return best
)
</code></pre></div>
<h4 id="environment-variables">Environment Variables</h4>
<div class="code-block"><pre><code class="language-bash"># vLLM server URL (default: http://localhost:8000/v1)
export VLLM_BASE_URL="http://192.168.1.100:8000/v1"

# Optional API key (if server started with --api-key)
export VLLM_API_KEY="your-api-key"

# HuggingFace cache (shared with HF/MLX providers)
export HF_HOME="~/.cache/huggingface"
</code></pre></div>
<p><strong>Available Models</strong>:
- <code>Qwen/Qwen3-Coder-30B-A3B-Instruct</code> (default) - Excellent for code
- <code>meta-llama/Llama-3.1-8B-Instruct</code> - Good general purpose
- <code>mistralai/Mistral-7B-Instruct-v0.3</code> - Fast and efficient
- Any HuggingFace model compatible with vLLM</p>
<p><strong>Performance notes</strong>: Throughput depends on model size, context length, concurrency, quantization, and GPU. See vLLM docs for tuning knobs (<code>--tensor-parallel-size</code>, <code>--max-model-len</code>, <code>--max-num-seqs</code>, ‚Ä¶).</p>
<h3 id="openai-compatible-setup">OpenAI-Compatible Setup</h3>
<p><strong>Best for</strong>: any OpenAI-compatible <code>/v1</code> endpoint (llama.cpp servers, LocalAI, text-generation-webui, custom proxies, etc.)</p>
<p>AbstractCore supports a generic OpenAI-compatible provider plus specific convenience providers (LM Studio, vLLM, OpenRouter, Portkey).</p>
<h4 id="1-get-the-endpoint-base-url">1. Get the endpoint base URL</h4>
<p>You must include <code>/v1</code> for OpenAI-compatible servers:</p>
<div class="code-block"><pre><code class="language-bash">export OPENAI_COMPATIBLE_BASE_URL="http://localhost:1234/v1"
# Optional (if your endpoint requires auth)
export OPENAI_COMPATIBLE_API_KEY="your-api-key"
</code></pre></div>
<h4 id="2-test-setup">2. Test Setup</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai-compatible", model="default", base_url="http://localhost:1234/v1")
resp = llm.generate("Say hello in French")
print(resp.content)
</code></pre></div>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="common-issues">Common Issues</h3>
<h4 id="no-module-named-abstractcore">"No module named .abstractcore."</h4>
<div class="code-block"><pre><code class="language-bash"># Make sure you installed AbstractCore
pip install abstractcore
</code></pre></div>
<h4 id="openai-api-key-not-found">"OpenAI API key not found"</h4>
<div class="code-block"><pre><code class="language-bash"># Check if environment variable is set
echo $OPENAI_API_KEY

# If empty, set it:
export OPENAI_API_KEY="sk-your-key-here"
</code></pre></div>
<h4 id="connection-error-to-ollama">"Connection error to Ollama"</h4>
<div class="code-block"><pre><code class="language-bash"># Make sure Ollama is running
ollama serve

# Check if models are available
ollama list

# Pull a model if none available
ollama pull gemma3:1b
</code></pre></div>
<h4 id="model-not-found-in-mlx">"Model not found in MLX"</h4>
<div class="code-block"><pre><code class="language-python"># Use exact model names from HuggingFace MLX community
llm = create_llm("mlx", model="mlx-community/Llama-3.2-3B-Instruct-4bit")
</code></pre></div>
<h4 id="lmstudio-connection-refused">"LMStudio connection refused"</h4>
<div class="code-block"><pre><code class="language-bash"># Make sure LMStudio server is running on correct port
# Check LMStudio logs for the exact port and URL
</code></pre></div>
<h3 id="memory-issues">Memory Issues</h3>
<h4 id="out-of-memory-with-local-models">"Out of memory" with local models</h4>
<div class="code-block"><pre><code class="language-bash"># Try smaller models
ollama pull gemma3:1b        # Only 1.3GB
ollama pull tinyllama        # Only 637MB

# Or increase swap space on Linux
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
</code></pre></div>
<h4 id="mlx-models-too-slow">MLX models too slow</h4>
<div class="code-block"><pre><code class="language-python"># Use 4-bit quantized models for faster inference
llm = create_llm("mlx", model="mlx-community/Llama-3.2-3B-Instruct-4bit")
</code></pre></div>
<h3 id="api-key-issues">API Key Issues</h3>
<h4 id="openai-billing-issues">OpenAI billing issues</h4>
<ol>
<li>Check your <a href="https://platform.openai.com/account/billing">billing dashboard</a></li>
<li>Add payment method if needed</li>
<li>Check usage limits</li>
</ol>
<h4 id="anthropic-rate-limits">Anthropic rate limits</h4>
<ol>
<li>Check your <a href="https://console.anthropic.com/">console</a></li>
<li>Upgrade to higher tier if needed</li>
<li>Implement retry logic in your code</li>
</ol>
<h2 id="testing-your-setup">Testing Your Setup</h2>
<h3 id="universal-test-script">Universal Test Script</h3>
<p>Save this as <code>test_setup.py</code> and run it to test all your providers:</p>
<div class="code-block"><pre><code class="language-python">#!/usr/bin/env python3
"""Test script for AbstractCore providers"""

import os
from abstractcore import create_llm

def test_provider(provider_name, model, **kwargs):
    """Test a specific provider"""
    try:
        print(f"\nüß™ Testing {provider_name} with {model}...")
        llm = create_llm(provider_name, model=model, **kwargs)
        response = llm.generate("Say 'Hello from AbstractCore!'")
        print(f"[OK] {provider_name}: {response.content}")
        return True
    except Exception as e:
        print(f"[FAIL] {provider_name}: {e}")
        return False

def main():
    print("AbstractCore Provider Test Suite")
    print("=" * 40)

    results = {}

    # Test cloud providers (if API keys available)
    if os.getenv("OPENAI_API_KEY"):
        results["OpenAI"] = test_provider("openai", "gpt-4o-mini")
    else:
        print("\n‚ö†Ô∏è  Skipping OpenAI (no OPENAI_API_KEY)")

    if os.getenv("ANTHROPIC_API_KEY"):
        results["Anthropic"] = test_provider("anthropic", "claude-haiku-4-5")
    else:
        print("\n‚ö†Ô∏è  Skipping Anthropic (no ANTHROPIC_API_KEY)")

    if os.getenv("OPENROUTER_API_KEY"):
        results["OpenRouter"] = test_provider("openrouter", "openai/gpt-4o-mini")
    else:
        print("\n‚ö†Ô∏è  Skipping OpenRouter (no OPENROUTER_API_KEY)")

    # Test local providers
    results["Ollama"] = test_provider("ollama", "gemma3:1b")

    try:
        results["MLX"] = test_provider("mlx", "mlx-community/Llama-3.2-3B-Instruct-4bit")
    except:
        print("\n‚ö†Ô∏è  Skipping MLX (not on Apple Silicon or model not available)")

    try:
        # Note: OpenAI-compatible servers expect `/v1` in the base URL (LM Studio default is http://localhost:1234/v1)
        results["LMStudio"] = test_provider("lmstudio", "qwen/qwen3-4b-2507", base_url="http://localhost:1234/v1")
    except:
        print("\n‚ö†Ô∏è  Skipping LMStudio (server not running on localhost:1234)")

    # Summary
    print("\n" + "=" * 40)
    print("Test Results:")
    working = [name for name, success in results.items() if success]
    if working:
        print(f"[OK] Working providers: {', '.join(working)}")
    else:
        print("[FAIL] No providers working")

    print("\n[INFO] Next steps:")
    print("- Add API keys for cloud providers")
    print("- Install Ollama and download models")
    print("- Start LMStudio local server")
    print("- See docs/prerequisites.md for detailed setup")

if __name__ == "__main__":
    main()
</code></pre></div>
<p>Run the test:</p>
<div class="code-block"><pre><code class="language-bash">python test_setup.py
</code></pre></div>
<h3 id="live-api-smoke-tests-opt-in">Live API smoke tests (opt-in)</h3>
<p>Some tests are intentionally <strong>real network calls</strong> and are disabled by default. To enable them, set:
- <code>ABSTRACTCORE_RUN_LIVE_API_TESTS=1</code></p>
<p>Example (OpenRouter):</p>
<div class="code-block"><pre><code class="language-bash">ABSTRACTCORE_RUN_LIVE_API_TESTS=1 OPENROUTER_API_KEY="$OPENROUTER_API_KEY" \
  python -m pytest -q tests/test_graceful_fallback.py::test_openrouter_generation_smoke
</code></pre></div>
<p>Local provider smoke tests use <code>ABSTRACTCORE_RUN_LOCAL_PROVIDER_TESTS=1</code> (and <code>ABSTRACTCORE_RUN_MLX_TESTS=1</code> for MLX).</p>
<h2 id="security-notes">Security Notes</h2>
<h3 id="api-keys">API Keys</h3>
<ul>
<li>Never commit API keys to version control</li>
<li>Use environment variables or <code>.env</code> files</li>
<li>Rotate keys periodically</li>
<li>Monitor usage for unexpected spikes</li>
</ul>
<h3 id="local-models">Local Models</h3>
<ul>
<li>Local models keep data on your machine</li>
<li>No internet required after initial download</li>
<li>Models can be large (1GB-20GB+)</li>
<li>Some models may have usage restrictions</li>
</ul>
<h3 id="network-security">Network Security</h3>
<ul>
<li>LMStudio and Ollama servers run locally by default</li>
<li>Be careful exposing servers to network (use authentication)</li>
<li>Consider firewall rules for production deployments</li>
</ul>
<p>This setup guide should get you running with any AbstractCore provider. Choose what works well for your use case - you can always add more providers later!</p>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
