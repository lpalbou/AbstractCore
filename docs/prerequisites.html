<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prerequisites & Setup - AbstractCore</title>
    <meta name="description" content="Complete setup guide for AbstractCore providers: OpenAI, Anthropic, OpenRouter, Ollama, LMStudio, MLX, HuggingFace, vLLM, and generic OpenAI-compatible endpoints.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [
                    { text: 'Features', href: '/docs/capabilities.html' },
                    { text: 'Quick Start', href: '/docs/getting-started.html' },
                    { text: 'Documentation', href: '/#docs' },
                    { text: 'Examples', href: '/docs/examples.html' },
                    {
                        text: 'GitHub',
                        href: 'https://github.com/lpalbou/abstractcore',
                        target: '_blank',
                        icon: 'github'
                    },
                    {
                        text: 'PyPI',
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Prerequisites & Setup Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Install AbstractCore and configure one or more providers. You can mix cloud + local providers in the same application.
                </p>
            </div>

            <!-- Quick Decision Guide -->
            <div style="background: linear-gradient(135deg, var(--primary-color), var(--secondary-color)); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem; color: white;">
                <h2 style="margin: 0 0 1.5rem 0;">Quick Decision Guide</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 1rem;">
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Need the simplest cloud setup?</strong><br>
                        <a href="#openai-setup" style="color: #fbbf24;">OpenAI</a> (OPENAI_API_KEY)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want Claude (vision) models?</strong><br>
                        <a href="#anthropic-setup" style="color: #fbbf24;">Anthropic</a> (ANTHROPIC_API_KEY)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want many models behind one key?</strong><br>
                        <a href="#openrouter-setup" style="color: #fbbf24;">OpenRouter</a> (OPENROUTER_API_KEY)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want free local models?</strong><br>
                        <a href="#ollama-setup" style="color: #fbbf24;">Ollama</a> (runs locally)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Want a GUI local server?</strong><br>
                        <a href="#lmstudio-setup" style="color: #fbbf24;">LMStudio</a> (OpenAI-compatible)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Running your own GPU server?</strong><br>
                        <a href="#vllm-setup" style="color: #fbbf24;">vLLM</a> (OpenAI-compatible)
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <strong>Have an OpenAI-compatible endpoint?</strong><br>
                        <a href="#openai-compatible-setup" style="color: #fbbf24;">OpenAI-Compatible</a> (bring your base_url)
                    </div>
                </div>
            </div>

            <div class="doc-content">
                <section id="core-installation">
                    <h2>Core Installation</h2>
                    <p>Install AbstractCore and the provider(s) you plan to use:</p>

                    <div class="code-block">
                        <pre><code class="language-bash"># Core (small default)
pip install abstractcore

# Providers (install only what you use; zsh: keep quotes)
pip install "abstractcore[openai]"       # OpenAI SDK
pip install "abstractcore[anthropic]"    # Anthropic SDK
pip install "abstractcore[huggingface]"  # Transformers / torch (heavy)
pip install "abstractcore[mlx]"          # Apple Silicon local inference (heavy)
pip install "abstractcore[vllm]"         # GPU server integration (heavy)

# Optional features
pip install "abstractcore[tools]"       # built-in web tools (fetch_url, web_search)
pip install "abstractcore[media]"       # images, PDFs, Office docs
pip install "abstractcore[embeddings]"  # vector embeddings + RAG helpers
pip install "abstractcore[tokens]"      # precise token counting (tiktoken)
pip install "abstractcore[server]"      # OpenAI-compatible HTTP gateway
pip install "abstractcore[compression]" # glyph visual-text compression

# Turnkey installs (pick one)
pip install "abstractcore[all-apple]"    # macOS/Apple Silicon (includes MLX, excludes vLLM)
pip install "abstractcore[all-non-mlx]"  # Linux/Windows/Intel Mac (excludes MLX and vLLM)
pip install "abstractcore[all-gpu]"      # Linux NVIDIA GPU (includes vLLM, excludes MLX)</code></pre>
                    </div>
                </section>

                <section id="centralized-config">
                    <h2>Recommended: Centralized Configuration</h2>
                    <p>AbstractCore can store defaults and API keys in <code>~/.abstractcore/config/abstractcore.json</code> so your apps can run without repeating flags.</p>

                    <div class="code-block">
                        <pre><code class="language-bash"># Show current status and defaults
abstractcore --status

# Set a global default (provider/model)
abstractcore --set-global-default ollama/qwen3:4b-instruct

# App-specific defaults
abstractcore --set-app-default cli ollama qwen3:4b-instruct
abstractcore --set-app-default summarizer openai gpt-4o-mini

# Store API keys in config (recommended)
abstractcore --set-api-key openai sk-your-key
abstractcore --set-api-key anthropic sk-ant-your-key
abstractcore --set-api-key openrouter sk-or-your-key</code></pre>
                    </div>

                    <p><strong>Learn more:</strong> <a href="centralized-config.html">Centralized Configuration Guide</a></p>
                </section>

                <section id="openai-setup">
                    <h2>OpenAI Setup</h2>
                    <p>Use OpenAI directly (fast setup, reliable models). You’ll need an API key.</p>

                    <h3>1. Set Environment Variable</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export OPENAI_API_KEY="sk-your-key-here"</code></pre>
                    </div>

                    <h3>2. (Optional) Override Base URL</h3>
                    <p>Useful for proxies or gateways:</p>
                    <div class="code-block">
                        <pre><code class="language-bash">export OPENAI_BASE_URL="https://api.openai.com/v1"</code></pre>
                    </div>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")
print(llm.generate("Hello, world!").content)</code></pre>
                    </div>

                    <p><strong>Vision note:</strong> For image inputs, use a vision-capable model (e.g. <code>gpt-4o</code>) when needed.</p>
                </section>

                <section id="anthropic-setup">
                    <h2>Anthropic Setup</h2>
                    <p>Claude models (including Haiku 4.5) support vision input. You’ll need an API key.</p>

                    <h3>1. Set Environment Variable</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export ANTHROPIC_API_KEY="your-key-here"</code></pre>
                    </div>

                    <h3>2. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("anthropic", model="claude-haiku-4-5", temperature=0.0)
print(llm.generate("Hello, world!").content)</code></pre>
                    </div>

                    <p><strong>Determinism note:</strong> Anthropic does not support <code>seed</code>; use <code>temperature=0.0</code> for more consistent outputs.</p>
                </section>

                <section id="openrouter-setup">
                    <h2>OpenRouter Setup (OpenAI-Compatible API)</h2>
                    <p>OpenRouter is an OpenAI-compatible aggregator API with multi-provider routing. You’ll need an API key.</p>

                    <h3>1. Set Environment Variable</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export OPENROUTER_API_KEY="sk-or-your-key-here"</code></pre>
                    </div>

                    <h3>2. (Optional) Identify Your App</h3>
                    <p>OpenRouter recommends these headers for analytics/abuse prevention:</p>
                    <div class="code-block">
                        <pre><code class="language-bash">export OPENROUTER_SITE_URL="https://www.abstractcore.ai"
export OPENROUTER_APP_NAME="AbstractCore"</code></pre>
                    </div>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openrouter", model="openai/gpt-4o-mini")
print(llm.generate("Hello from OpenRouter!").content)</code></pre>
                    </div>
                </section>

                <section id="ollama-setup">
                    <h2>Ollama Setup (Local)</h2>
                    <p>Run open-source models locally (free, privacy-first).</p>

                    <h3>1. Install + Start</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># macOS
brew install ollama

# Start server (default: http://localhost:11434)
ollama serve</code></pre>
                    </div>

                    <h3>2. Pull a Model</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">ollama pull qwen3:4b-instruct</code></pre>
                    </div>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("ollama", model="qwen3:4b-instruct")
print(llm.generate("Hello from Ollama!").content)</code></pre>
                    </div>

                    <h3>4. (Optional) Point to a Remote Ollama</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Either of these are supported by AbstractCore's Ollama provider
export OLLAMA_BASE_URL="http://your-host:11434"
export OLLAMA_HOST="http://your-host:11434"</code></pre>
                    </div>
                </section>

                <section id="lmstudio-setup">
                    <h2>LMStudio Setup (Local OpenAI-Compatible)</h2>
                    <p>LMStudio runs local models behind an OpenAI-compatible API (default <code>http://localhost:1234/v1</code>).</p>

                    <h3>1. Start the LMStudio Local Server</h3>
                    <ol>
                        <li>Download LMStudio from <a href="https://lmstudio.ai/" target="_blank">lmstudio.ai</a></li>
                        <li>Download a model in the UI (recommended: <code>qwen/qwen3-4b-2507</code>)</li>
                        <li>Start the “Local Server” (note the base URL)</li>
                    </ol>

                    <h3>2. (Optional) Set Base URL</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export LMSTUDIO_BASE_URL="http://localhost:1234/v1"</code></pre>
                    </div>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("lmstudio", model="qwen/qwen3-4b-2507")
print(llm.generate("Hello from LMStudio!").content)</code></pre>
                    </div>
                </section>

                <section id="openai-compatible-setup">
                    <h2>OpenAI-Compatible Setup (Generic)</h2>
                    <p>Use any OpenAI-compatible endpoint (llama.cpp, LocalAI, custom proxies, etc.) by providing a <code>base_url</code>.</p>

                    <h3>1. Configure Base URL</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export OPENAI_COMPATIBLE_BASE_URL="http://127.0.0.1:1234/v1"
# Optional (if your endpoint requires auth)
export OPENAI_COMPATIBLE_API_KEY="your-key"</code></pre>
                    </div>

                    <h3>2. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai-compatible", model="your-model-name")
print(llm.generate("Hello from an OpenAI-compatible server!").content)</code></pre>
                    </div>
                </section>

                <section id="vllm-setup">
                    <h2>vLLM Setup (Local/Hosted OpenAI-Compatible)</h2>
                    <p>vLLM exposes an OpenAI-compatible API (default <code>http://localhost:8000/v1</code>) and supports advanced features (guided decoding, Multi-LoRA, beam search).</p>

                    <h3>1. Run a vLLM Server</h3>
                    <div class="code-block">
                        <pre><code class="language-bash"># Example (adjust to your GPU + model)
vllm serve Qwen/Qwen3-Coder-30B-A3B-Instruct --host 0.0.0.0 --port 8000</code></pre>
                    </div>

                    <h3>2. Configure Base URL</h3>
                    <div class="code-block">
                        <pre><code class="language-bash">export VLLM_BASE_URL="http://localhost:8000/v1"
# Optional if your deployment uses auth
export VLLM_API_KEY="your-key"</code></pre>
                    </div>

                    <h3>3. Test Connection</h3>
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("vllm", model="Qwen/Qwen3-Coder-30B-A3B-Instruct")
print(llm.generate("Hello from vLLM!").content)</code></pre>
                    </div>
                </section>

                <section id="mlx-setup">
                    <h2>MLX Setup (Apple Silicon)</h2>
                    <p>MLX provides optimized local inference on Apple Silicon (M1/M2/M3/M4).</p>

                    <div class="code-block">
                        <pre><code class="language-bash">pip install "abstractcore[mlx]"</code></pre>
                    </div>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("mlx", model="mlx-community/Qwen3-4B-4bit")
print(llm.generate("Hello from MLX!").content)</code></pre>
                    </div>
                </section>

                <section id="huggingface-setup">
                    <h2>HuggingFace Setup</h2>
                    <p>Run open-source models via HuggingFace tooling.</p>

                    <div class="code-block">
                        <pre><code class="language-bash">pip install "abstractcore[huggingface]"</code></pre>
                    </div>

                    <div class="code-block">
                        <pre><code class="language-bash"># Optional: for private/gated models
export HUGGINGFACE_API_TOKEN="your-token-here"</code></pre>
                    </div>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("huggingface", model="unsloth/Qwen3-4B-Instruct-2507-GGUF")
print(llm.generate("Hello from HuggingFace!").content)</code></pre>
                    </div>
                </section>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Next Steps</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="getting-started.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Getting Started</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">5-minute setup + first LLM call</p>
                        </a>
                        <a href="centralized-config.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Centralized Configuration</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Defaults + API keys in one place</p>
                        </a>
                        <a href="media-handling-system.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Media Handling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Attach images, PDFs, Office docs</p>
                        </a>
                        <a href="tool-calling.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Tool Calling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Universal tools + syntax rewriting</p>
                        </a>
                        <a href="server.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">HTTP Server</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">OpenAI-compatible REST API</p>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
