<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbstractCore Server - AbstractCore</title>
    <meta name="description" content="Transform AbstractCore into an OpenAI-compatible API server. One server, all models, any client.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">AbstractCore Server</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">Transform AbstractCore into an OpenAI-compatible API server. One server, all models, any client.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#interactive-api-docs-start-here" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Interactive API docs (start here)</a>
<a href="#quick-start" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Start</a>
<a href="#configuration" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Configuration</a>
<a href="#api-endpoints" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">API Endpoints</a>
<a href="#agentic-cli-integration" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Agentic CLI integration</a>
<a href="#deployment" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Deployment</a>
<a href="#debug-and-monitoring" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Debug and Monitoring</a>
<a href="#common-patterns" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Common Patterns</a>
<a href="#troubleshooting" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Troubleshooting</a>
<a href="#why-abstractcore-server" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Why AbstractCore Server?</a>
<a href="#related-documentation" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Related Documentation</a></div>

            <div class="doc-content">

<p>If you want a dedicated <strong>single-model</strong> <code>/v1</code> server (one provider/model per worker), see <a href="endpoint.html">Endpoint</a>.</p>
<h2 id="interactive-api-docs-start-here">Interactive API docs (start here)</h2>
<p>Visit while the server is running:
- <strong>Swagger UI</strong>: <code>http://localhost:8000/docs</code>
- <strong>ReDoc</strong>: <code>http://localhost:8000/redoc</code></p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="install-and-run-2-minutes">Install and Run (2 minutes)</h3>
<div class="code-block"><pre><code class="language-bash"># Install
pip install &quot;abstractcore[server]&quot;

# Start server
python -m abstractcore.server.app

# Or with uvicorn directly
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# Test
curl http://localhost:8000/health
# Response: {&quot;status&quot;:&quot;healthy&quot;}
</code></pre></div>
<h3 id="first-request">First Request</h3>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openai/gpt-4o-mini&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
  }'
</code></pre></div>
<p>Or with Python:</p>
<div class="code-block"><pre><code class="language-python">from openai import OpenAI

client = OpenAI(base_url=&quot;http://localhost:8000/v1&quot;, api_key=&quot;unused&quot;)

response = client.chat.completions.create(
    model=&quot;anthropic/claude-haiku-4-5&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain quantum computing&quot;}]
)
print(response.choices[0].message.content)
</code></pre></div>
<hr />
<h2 id="configuration">Configuration</h2>
<h3 id="environment-variables">Environment Variables</h3>
<div class="code-block"><pre><code class="language-bash"># Provider API keys
export OPENAI_API_KEY=&quot;sk-...&quot;
export ANTHROPIC_API_KEY=&quot;sk-ant-...&quot;
export OPENROUTER_API_KEY=&quot;sk-or-...&quot;
export PORTKEY_API_KEY=&quot;pk_...&quot;         # optional (Portkey)
export PORTKEY_CONFIG=&quot;pcfg_...&quot;        # required for Portkey routing

# Local providers
export OLLAMA_BASE_URL=&quot;http://localhost:11434&quot;          # (or legacy: OLLAMA_HOST)
export LMSTUDIO_BASE_URL=&quot;http://localhost:1234/v1&quot;
export VLLM_BASE_URL=&quot;http://localhost:8000/v1&quot;

# Server bind (only used by `python -m abstractcore.server.app`)
export HOST=&quot;0.0.0.0&quot;
export PORT=&quot;8000&quot;

# Debug mode
export ABSTRACTCORE_DEBUG=true

# Dangerous (multi-tenant hazard): allow unload_after for providers that can unload shared server state (e.g. Ollama)
export ABSTRACTCORE_ALLOW_UNSAFE_UNLOAD_AFTER=1
</code></pre></div>
<h3 id="startup-options">Startup Options</h3>
<div class="code-block"><pre><code class="language-bash"># Using AbstractCore's built-in CLI
python -m abstractcore.server.app --help                    # View all options
python -m abstractcore.server.app --debug                   # Debug mode
python -m abstractcore.server.app --host 127.0.0.1 --port 8080  # Custom host/port
python -m abstractcore.server.app --debug --port 8001       # Debug on custom port

# Using uvicorn directly
uvicorn abstractcore.server.app:app --reload                # Development with auto-reload
uvicorn abstractcore.server.app:app --workers 4             # Production with multiple workers
uvicorn abstractcore.server.app:app --port 3000             # Custom port
</code></pre></div>
<hr />
<h2 id="api-endpoints">API Endpoints</h2>
<h3 id="chat-completions">Chat Completions</h3>
<p><strong>Endpoint:</strong> <code>POST /v1/chat/completions</code></p>
<p>Standard OpenAI-compatible endpoint. Works with all providers.</p>
<p><strong>Request:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;provider/model-name&quot;,
  &quot;messages&quot;: [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}
  ],
  &quot;temperature&quot;: 0.7,
  &quot;max_tokens&quot;: 1000,
  &quot;stream&quot;: false
}
</code></pre></div>
<p><strong>Key Parameters:</strong>
- <code>model</code> (required): Prefer <code>"provider/model-name"</code> (e.g., <code>"openai/gpt-4o-mini"</code>). If you pass a bare model name (no <code>/</code>), the server will best-effort auto-detect a provider.
- <code>messages</code> (required): Array of message objects
- <code>stream</code> (optional): Enable streaming responses
- <code>tools</code> (optional): Tools for function calling
- <code>agent_format</code> (optional, AbstractCore extension): Tool-call syntax output format for agentic clients (<code>"auto"|"openai"|"codex"|"qwen3"|"llama3"|"gemma"|"xml"|"passthrough"</code>). When omitted, the server auto-detects from user-agent + model heuristics.
- <code>api_key</code> (optional, AbstractCore extension): Provider API key for per-request authentication. Falls back to environment variables (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>OPENROUTER_API_KEY</code>, <code>PORTKEY_API_KEY</code>)
- <code>base_url</code> (optional, AbstractCore extension): Override the provider endpoint (include <code>/v1</code> for OpenAI-compatible servers like LM Studio / vLLM / OpenRouter)
- <code>unload_after</code> (optional, AbstractCore extension): If <code>true</code>, calls <code>llm.unload_model(model)</code> after the request completes. Disabled for <code>ollama/*</code> unless <code>ABSTRACTCORE_ALLOW_UNSAFE_UNLOAD_AFTER=1</code>.
- <code>thinking</code> (optional, AbstractCore extension): Unified thinking/reasoning control (<code>null|"auto"|"on"|"off"</code> or <code>"low"|"medium"|"high"</code> when supported)
- <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>: Standard LLM parameters</p>
<p><strong>Example with streaming:</strong></p>
<div class="code-block"><pre><code class="language-python">from openai import OpenAI

client = OpenAI(base_url=&quot;http://localhost:8000/v1&quot;, api_key=&quot;unused&quot;)

stream = client.chat.completions.create(
    model=&quot;ollama/qwen3-coder:30b&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a story&quot;}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end=&quot;&quot;, flush=True)
</code></pre></div>
<h4 id="provider-base_url-override-abstractcore-extension">Provider <code>base_url</code> override (AbstractCore extension)</h4>
<p>Route a provider to a specific endpoint (useful for remote OpenAI-compatible servers):</p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;lmstudio/qwen/qwen3-4b-2507&quot;,
    &quot;base_url&quot;: &quot;http://localhost:1234/v1&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello from a remote LM Studio endpoint&quot;}]
  }'
</code></pre></div>
<h4 id="per-request-api_key-abstractcore-extension">Per-request <code>api_key</code> (AbstractCore extension)</h4>
<p>Pass API keys directly in requests (useful for multi-tenant scenarios or OpenRouter):</p>
<div class="code-block"><pre><code class="language-bash"># OpenRouter with per-request API key
curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openrouter/anthropic/claude-3.5-sonnet&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}],
    &quot;api_key&quot;: &quot;sk-or-v1-your-openrouter-key&quot;
  }'

# OpenAI-compatible endpoint with custom auth
curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openai-compatible/my-model&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}],
    &quot;api_key&quot;: &quot;your-api-key&quot;,
    &quot;base_url&quot;: &quot;https://my-custom-endpoint.com/v1&quot;
  }'
</code></pre></div>
<p>If <code>api_key</code> is not provided, AbstractCore falls back to environment variables.</p>
<h3 id="media-generation-endpoints-optional">Media generation endpoints (optional)</h3>
<p>AbstractCore Server can optionally expose OpenAI-compatible <strong>image generation</strong> and <strong>audio</strong> endpoints.</p>
<p>Important notes:
- These are <strong>interoperability-first</strong> endpoints (return <code>b64_json</code> or raw bytes), not an artifact-first durability contract.
- If the required plugin/backend is not available, the server returns <code>501</code> with actionable messaging.</p>
<h4 id="images-generateedit-requires-abstractvision">Images (generate/edit) ‚Äî requires <code>abstractvision</code></h4>
<p>Endpoints:
- <code>POST /v1/images/generations</code>
- <code>POST /v1/images/edits</code></p>
<p>Install:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[server]&quot;
pip install abstractvision
</code></pre></div>
<h4 id="audio-stttts-requires-an-audiovoice-capability-plugin-typically-abstractvoice">Audio (STT/TTS) ‚Äî requires an audio/voice capability plugin (typically <code>abstractvoice</code>)</h4>
<p>Endpoints:
- <code>POST /v1/audio/transcriptions</code> (multipart; <code>file=...</code>)
- <code>POST /v1/audio/speech</code> (json; <code>input=...</code>, optional <code>voice</code>, optional <code>format</code>)</p>
<p>Install:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[server]&quot;
pip install abstractvoice
</code></pre></div>
<p>Notes:
- <code>/v1/audio/transcriptions</code> requires <code>python-multipart</code> for form parsing (included in the server extra).</p>
<p>Examples:</p>
<div class="code-block"><pre><code class="language-bash"># Speech-to-text (STT)
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F &quot;file=@speech.wav&quot; \
  -F &quot;language=en&quot;

# Text-to-speech (TTS)
curl -X POST http://localhost:8000/v1/audio/speech \
  -H &quot;Content-Type: application/json&quot; \
  -d '{&quot;input&quot;:&quot;Hello!&quot;,&quot;format&quot;:&quot;wav&quot;}' \
  --output hello.wav
</code></pre></div>
<p>If you want to ‚Äúask a model about an audio file‚Äù, prefer one of:
- Run STT first (<code>/v1/audio/transcriptions</code>) then send the transcript to <code>POST /v1/chat/completions</code>, or
- Configure the server‚Äôs default audio strategy (<code>config.audio.strategy</code>) to enable STT fallback for audio attachments, then attach audio in chat requests.</p>
<h3 id="multimodal-requests-images-documents-files">Multimodal Requests (Images, Documents, Files)</h3>
<p>AbstractCore server supports comprehensive file attachments using OpenAI-compatible multimodal message format, plus AbstractCore's convenient <code>@filename</code> syntax.</p>
<h4 id="supported-file-types">Supported File Types</h4>
<ul>
<li><strong>Images</strong>: PNG, JPEG, GIF, WEBP, BMP, TIFF</li>
<li><strong>Documents</strong>: PDF, DOCX, XLSX, PPTX</li>
<li><strong>Data/Text</strong>: CSV, TSV, TXT, MD, JSON, XML</li>
<li><strong>Size Limits</strong>: 10MB per file, 32MB total per request</li>
</ul>
<h4 id="method-1-filename-syntax-abstractcore-extension">Method 1: @filename Syntax (AbstractCore Extension)</h4>
<p>Simple syntax that works with all providers:</p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openai/gpt-4o&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is in this document? @/path/to/report.pdf&quot;}
    ]
  }'
</code></pre></div>
<h4 id="method-2-openai-vision-api-format-image-urls">Method 2: OpenAI Vision API Format (Image URLs)</h4>
<p>Standard OpenAI format for images:</p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;anthropic/claude-haiku-4-5&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is in this image?&quot;},
        {
          &quot;type&quot;: &quot;image_url&quot;,
          &quot;image_url&quot;: {
            &quot;url&quot;: &quot;https://example.com/image.jpg&quot;
          }
        }
      ]
    }
  ]
}
</code></pre></div>
<p><strong>Base64 Images:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;type&quot;: &quot;image_url&quot;,
  &quot;image_url&quot;: {
    &quot;url&quot;: &quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...&quot;
  }
}
</code></pre></div>
<h4 id="method-3-openai-file-format-forward-compatible">Method 3: OpenAI File Format (Forward-Compatible)</h4>
<p>AbstractCore supports OpenAI's planned file format with simplified structure (consistent with image_url):</p>
<p><strong>File URL Format (Recommended - Same Pattern as image_url):</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;ollama/qwen3:4b&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this document&quot;},
        {
          &quot;type&quot;: &quot;file&quot;,
          &quot;file_url&quot;: {
            &quot;url&quot;: &quot;https://example.com/documents/report.pdf&quot;
          }
        }
      ]
    }
  ]
}
</code></pre></div>
<p><strong>Local File Path:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;type&quot;: &quot;file&quot;,
  &quot;file_url&quot;: {
    &quot;url&quot;: &quot;/Users/username/documents/data.csv&quot;
  }
}
</code></pre></div>
<p><strong>Base64 Data URL:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;type&quot;: &quot;file&quot;,
  &quot;file_url&quot;: {
    &quot;url&quot;: &quot;data:application/pdf;base64,JVBERi0xLjQKMSAwIG9iago&lt;PAovVHlwZS...&quot;
  }
}
</code></pre></div>
<p><strong>Filename Extraction:</strong>
- <strong>URLs/Paths</strong>: Extracted automatically (<code>/path/file.pdf</code> ‚Üí <code>file.pdf</code>)
- <strong>Base64</strong>: Generated from MIME type (<code>data:application/pdf;base64,...</code> ‚Üí <code>document.pdf</code>)</p>
<h4 id="mixed-content-example">Mixed Content Example</h4>
<p>Combine text, images, and documents in a single request:</p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;openai/gpt-4o&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Compare this chart with the data in the spreadsheet&quot;},
        {
          &quot;type&quot;: &quot;image_url&quot;,
          &quot;image_url&quot;: {&quot;url&quot;: &quot;data:image/png;base64,iVBORw0KGgoAAAANS...&quot;}
        },
        {
          &quot;type&quot;: &quot;file&quot;,
          &quot;file_url&quot;: {
            &quot;url&quot;: &quot;https://example.com/data/sales_data.xlsx&quot;
          }
        }
      ]
    }
  ]
}
</code></pre></div>
<h4 id="python-client-examples">Python Client Examples</h4>
<p><strong>Using OpenAI Client:</strong></p>
<div class="code-block"><pre><code class="language-python">from openai import OpenAI
import base64

client = OpenAI(base_url=&quot;http://localhost:8000/v1&quot;, api_key=&quot;unused&quot;)

# Method 1: @filename syntax
response = client.chat.completions.create(
    model=&quot;anthropic/claude-haiku-4-5&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Summarize @document.pdf&quot;}]
)

# Method 2: File URL (HTTP/HTTPS)
response = client.chat.completions.create(
    model=&quot;openai/gpt-4o&quot;,
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the key findings?&quot;},
            {
                &quot;type&quot;: &quot;file&quot;,
                &quot;file_url&quot;: {
                    &quot;url&quot;: &quot;https://example.com/documents/report.pdf&quot;
                }
            }
        ]
    }]
)

# Method 3: Local file path
response = client.chat.completions.create(
    model=&quot;anthropic/claude-haiku-4-5&quot;,
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this local document&quot;},
            {
                &quot;type&quot;: &quot;file&quot;,
                &quot;file_url&quot;: {
                    &quot;url&quot;: &quot;/Users/username/documents/report.pdf&quot;
                }
            }
        ]
    }]
)

# Method 4: Base64 data URL
with open(&quot;report.pdf&quot;, &quot;rb&quot;) as f:
    file_data = base64.b64encode(f.read()).decode()

response = client.chat.completions.create(
    model=&quot;lmstudio/qwen/qwen3-next-80b&quot;,
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the key findings?&quot;},
            {
                &quot;type&quot;: &quot;file&quot;,
                &quot;file_url&quot;: {
                    &quot;url&quot;: f&quot;data:application/pdf;base64,{file_data}&quot;
                }
            }
        ]
    }]
)
</code></pre></div>
<p><strong>Universal Provider Support:</strong></p>
<div class="code-block"><pre><code class="language-python"># Same syntax works across all providers
providers_models = [
    &quot;openai/gpt-4o&quot;,
    &quot;anthropic/claude-haiku-4-5&quot;,
    &quot;ollama/qwen2.5vl:7b&quot;,
    &quot;lmstudio/qwen/qwen2.5-vl-7b&quot;
]

for model in providers_models:
    response = client.chat.completions.create(
        model=model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Analyze @data.csv and @chart.png&quot;}]
    )
    print(f&quot;{model}: {response.choices[0].message.content[:100]}...&quot;)
</code></pre></div>
<hr />
<h3 id="openai-responses-api">OpenAI Responses API</h3>
<p><strong>Endpoint:</strong> <code>POST /v1/responses</code></p>
<p>AbstractCore implements an OpenAI-compatible Responses-style API, including <code>input_file</code> support.</p>
<h4 id="why-use-v1responses">Why Use /v1/responses?</h4>
<ul>
<li><strong>OpenAI Compatible</strong>: Drop-in replacement for OpenAI's Responses API</li>
<li><strong>Native File Support</strong>: <code>input_file</code> type designed specifically for document attachments</li>
<li><strong>Cleaner API</strong>: Explicit separation between text (<code>input_text</code>) and files (<code>input_file</code>)</li>
<li><strong>Backward Compatible</strong>: Existing <code>messages</code> format still works alongside new <code>input</code> format</li>
<li><strong>Optional Streaming</strong>: Streaming opt-in with <code>"stream": true</code> (defaults to <code>false</code>)</li>
</ul>
<h4 id="request-format">Request Format</h4>
<p><strong>OpenAI Responses API Format (Recommended):</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;gpt-4o&quot;,
  &quot;input&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
        {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Analyze this document&quot;},
        {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/report.pdf&quot;}
      ]
    }
  ],
  &quot;stream&quot;: false,
  &quot;max_tokens&quot;: 2000,
  &quot;temperature&quot;: 0.7
}
</code></pre></div>
<p><strong>Legacy Format (Still Supported):</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;model&quot;: &quot;openai/gpt-4&quot;,
  &quot;messages&quot;: [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a story&quot;}
  ],
  &quot;stream&quot;: false
}
</code></pre></div>
<h4 id="automatic-format-detection">Automatic Format Detection</h4>
<p>The server automatically detects which format you're using:
- <strong>OpenAI Format</strong>: Presence of <code>input</code> field ‚Üí converts to internal format
- <strong>Legacy Format</strong>: Presence of <code>messages</code> field ‚Üí processes directly
- <strong>Error</strong>: Missing both fields ‚Üí returns 400 error with clear message</p>
<h4 id="examples">Examples</h4>
<p><strong>Simple Text Request:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/responses \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;lmstudio/qwen/qwen3-next-80b&quot;,
    &quot;input&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
          {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;What is Python?&quot;}
        ]
      }
    ]
  }'
</code></pre></div>
<p><strong>File Analysis:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/responses \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openai/gpt-4o&quot;,
    &quot;input&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
          {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Analyze the letter and summarize key points&quot;},
          {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://www.berkshirehathaway.com/letters/2024ltr.pdf&quot;}
        ]
      }
    ]
  }'
</code></pre></div>
<p><strong>Multiple Files:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/responses \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;anthropic/claude-haiku-4-5&quot;,
    &quot;input&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
          {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Compare these documents&quot;},
          {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/report1.pdf&quot;},
          {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/report2.pdf&quot;},
          {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/chart.png&quot;}
        ]
      }
    ],
    &quot;max_tokens&quot;: 2000
  }'
</code></pre></div>
<p><strong>Streaming Response:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/responses \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;openai/gpt-4o&quot;,
    &quot;input&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
          {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Summarize this document&quot;},
          {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/document.pdf&quot;}
        ]
      }
    ],
    &quot;stream&quot;: true
  }' --no-buffer
</code></pre></div>
<h4 id="supported-media-types">Supported Media Types</h4>
<p>All file types supported via URL, local path, or base64:</p>
<ul>
<li><strong>Documents</strong>: PDF, DOCX, XLSX, PPTX</li>
<li><strong>Data Files</strong>: CSV, TSV, JSON, XML</li>
<li><strong>Text Files</strong>: TXT, MD</li>
<li><strong>Images</strong>: PNG, JPEG, GIF, WEBP, BMP, TIFF</li>
<li><strong>Size Limits</strong>: 10MB per file, 32MB total per request</li>
</ul>
<p><strong>Source Options:</strong></p>
<div class="code-block"><pre><code class="language-json">// HTTP/HTTPS URL
{&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/report.pdf&quot;}

// Local file path
{&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;/path/to/document.xlsx&quot;}

// Base64 data URL
{&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;data:application/pdf;base64,JVBERi0x...&quot;}
</code></pre></div>
<h4 id="python-client-example">Python Client Example</h4>
<div class="code-block"><pre><code class="language-python">from openai import OpenAI

client = OpenAI(base_url=&quot;http://localhost:8000/v1&quot;, api_key=&quot;unused&quot;)

# Direct request to /v1/responses endpoint
import requests

response = requests.post(
    &quot;http://localhost:8000/v1/responses&quot;,
    json={
        &quot;model&quot;: &quot;gpt-4o&quot;,
        &quot;input&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [
                    {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Analyze this document&quot;},
                    {&quot;type&quot;: &quot;input_file&quot;, &quot;file_url&quot;: &quot;https://example.com/report.pdf&quot;}
                ]
            }
        ]
    }
)

result = response.json()
print(result[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])
</code></pre></div>
<hr />
<h3 id="embeddings">Embeddings</h3>
<p><strong>Endpoint:</strong> <code>POST /v1/embeddings</code></p>
<p>Generate embedding vectors for semantic search, RAG, and similarity analysis.</p>
<p><strong>Request:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;input&quot;: &quot;Text to embed&quot;,
  &quot;model&quot;: &quot;huggingface/sentence-transformers/all-MiniLM-L6-v2&quot;
}
</code></pre></div>
<p><strong>Supported Providers:</strong>
- <strong>HuggingFace</strong>: Local models with ONNX acceleration
- <strong>Ollama</strong>: <code>ollama/granite-embedding:278m</code>, etc.
- <strong>LMStudio</strong>: Any loaded embedding model</p>
<p><strong>Batch Embedding:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/embeddings \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;input&quot;: [&quot;text 1&quot;, &quot;text 2&quot;, &quot;text 3&quot;],
    &quot;model&quot;: &quot;ollama/granite-embedding:278m&quot;
  }'
</code></pre></div>
<hr />
<h3 id="model-discovery">Model Discovery</h3>
<p><strong>Endpoint:</strong> <code>GET /v1/models</code></p>
<p>List all available models from configured providers.</p>
<p><strong>Query Parameters:</strong>
- <code>provider</code>: Filter by provider (e.g., <code>ollama</code>, <code>openai</code>)
- <code>type</code>: Filter by type (<code>text-generation</code> or <code>text-embedding</code>)</p>
<p><strong>Examples:</strong></p>
<div class="code-block"><pre><code class="language-bash"># All models
curl http://localhost:8000/v1/models

# Ollama models only
curl http://localhost:8000/v1/models?provider=ollama

# Embedding models only
curl http://localhost:8000/v1/models?type=text-embedding

# Ollama embeddings
curl http://localhost:8000/v1/models?provider=ollama&amp;type=text-embedding
</code></pre></div>
<hr />
<h3 id="provider-status">Provider Status</h3>
<p><strong>Endpoint:</strong> <code>GET /providers</code></p>
<p>List all available providers and their status.</p>
<p><strong>Response:</strong></p>
<div class="code-block"><pre><code class="language-json">{
  &quot;providers&quot;: [
    {
      &quot;name&quot;: &quot;ollama&quot;,
      &quot;type&quot;: &quot;llm&quot;,
      &quot;model_count&quot;: 15,
      &quot;status&quot;: &quot;available&quot;
    }
  ]
}
</code></pre></div>
<hr />
<h3 id="health-check">Health Check</h3>
<p><strong>Endpoint:</strong> <code>GET /health</code></p>
<p>Server health check for monitoring.</p>
<p><strong>Response:</strong> <code>{"status": "healthy"}</code></p>
<hr />
<h2 id="agentic-cli-integration">Agentic CLI integration</h2>
<p>AbstractCore Server is <strong>OpenAI-compatible</strong>. Most OpenAI-compatible CLIs/SDKs can be pointed at it by setting:</p>
<ul>
<li><code>OPENAI_BASE_URL="http://localhost:8000/v1"</code> (or an equivalent flag)</li>
<li><code>OPENAI_API_KEY="unused"</code> (many clients require a non-empty key even for local servers)</li>
</ul>
<h3 id="tool-calling-interoperability">Tool calling interoperability</h3>
<ul>
<li>The server <strong>does not execute tools</strong> (it always returns tool calls; your host/runtime executes them).</li>
<li>It can emit tool calls either as structured <code>tool_calls</code> (OpenAI/Codex style) <strong>or</strong> as tagged content for clients that parse tool calls from assistant text.</li>
<li>Control the output format with <code>agent_format</code> (request body, AbstractCore extension), or rely on auto-detection (user-agent + model heuristics).</li>
</ul>
<p>Supported <code>agent_format</code> values: <code>auto</code>, <code>openai</code>, <code>codex</code>, <code>qwen3</code>, <code>llama3</code>, <code>gemma</code>, <code>xml</code>, <code>passthrough</code>.</p>
<h3 id="codex-cli-example">Codex CLI (example)</h3>
<div class="code-block"><pre><code class="language-bash">export OPENAI_BASE_URL=&quot;http://localhost:8000/v1&quot;
export OPENAI_API_KEY=&quot;unused&quot;

codex --model &quot;ollama/qwen3-coder:30b&quot; &quot;Write a factorial function&quot;
</code></pre></div>
<h3 id="forcing-a-format-curl">Forcing a format (curl)</h3>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;ollama/qwen3:4b-instruct-2507-q4_K_M&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Use the tool.&quot;}],
    &quot;tools&quot;: [
      {
        &quot;type&quot;: &quot;function&quot;,
        &quot;function&quot;: {
          &quot;name&quot;: &quot;get_weather&quot;,
          &quot;description&quot;: &quot;Get weather by city&quot;,
          &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {&quot;city&quot;: {&quot;type&quot;: &quot;string&quot;}},
            &quot;required&quot;: [&quot;city&quot;]
          }
        }
      }
    ],
    &quot;agent_format&quot;: &quot;llama3&quot;
  }'
</code></pre></div>
<hr />
<h2 id="deployment">Deployment</h2>
<h3 id="docker">Docker</h3>
<div class="code-block"><pre><code class="language-dockerfile">FROM python:3.9-slim

RUN pip install &quot;abstractcore[server]&quot;

EXPOSE 8000

CMD [&quot;uvicorn&quot;, &quot;abstractcore.server.app:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;, &quot;--workers&quot;, &quot;4&quot;]
</code></pre></div>
<p><strong>Run:</strong></p>
<div class="code-block"><pre><code class="language-bash">docker build -t abstractcore-server .
docker run -p 8000:8000 -e OPENAI_API_KEY=$OPENAI_API_KEY abstractcore-server
</code></pre></div>
<h3 id="docker-compose">Docker Compose</h3>
<div class="code-block"><pre><code class="language-yaml">version: '3.8'

services:
  abstractcore:
    image: abstractcore-server:latest
    ports:
      - &quot;8000:8000&quot;
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    restart: unless-stopped
</code></pre></div>
<h3 id="production-with-gunicorn">Production with Gunicorn</h3>
<div class="code-block"><pre><code class="language-bash">pip install gunicorn

gunicorn abstractcore.server.app:app \
  --worker-class uvicorn.workers.UvicornWorker \
  --workers 4 \
  --bind 0.0.0.0:8000
</code></pre></div>
<hr />
<h2 id="debug-and-monitoring">Debug and Monitoring</h2>
<h3 id="enable-debug-mode">Enable Debug Mode</h3>
<p>Debug mode provides comprehensive logging and detailed error reporting for troubleshooting API issues.</p>
<div class="code-block"><pre><code class="language-bash"># Method 1: Using command line flag (recommended)
python -m abstractcore.server.app --debug

# Method 2: Using environment variable
export ABSTRACTCORE_DEBUG=true
python -m abstractcore.server.app

# Method 3: With uvicorn directly
export ABSTRACTCORE_DEBUG=true
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000
</code></pre></div>
<h3 id="debug-features">Debug Features</h3>
<p><strong>Enhanced Error Reporting:</strong>
- <strong>Before</strong>: Uninformative "422 Unprocessable Entity" messages
- <strong>After</strong>: Detailed field validation errors with request body capture</p>
<p><strong>Example Debug Output:</strong></p>
<div class="code-block"><pre><code class="language-json">üî¥ Request Validation Error (422) | method=POST | error_count=2 | errors=[
  {&quot;field&quot;: &quot;body -&gt; model&quot;, &quot;message&quot;: &quot;Field required&quot;, &quot;type&quot;: &quot;missing&quot;},
  {&quot;field&quot;: &quot;body -&gt; messages&quot;, &quot;message&quot;: &quot;Field required&quot;, &quot;type&quot;: &quot;missing&quot;}
] | client=127.0.0.1

üìã Request Body (Validation Error) | body={&quot;invalid&quot;: &quot;data&quot;}
</code></pre></div>
<p><strong>Request/Response Tracking:</strong>
- Full HTTP request details (method, URL, headers, client IP)
- Response status codes and processing times
- Structured JSON logging for machine processing</p>
<p><strong>Log Files:</strong>
- <code>logs/abstractcore_TIMESTAMP.log</code> - Structured events
- <code>logs/YYYYMMDD-payloads.jsonl</code> - Full request bodies
- <code>logs/verbatim_TIMESTAMP.jsonl</code> - Complete I/O</p>
<p><strong>Useful Commands:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Find errors
grep '&quot;level&quot;: &quot;error&quot;' logs/abstractcore_*.log

# Track token usage
cat logs/verbatim_*.jsonl | jq '.metadata.tokens | .input + .output' | \
  awk '{sum+=$1} END {print &quot;Total:&quot;, sum}'

# Monitor specific model
grep '&quot;model&quot;: &quot;qwen3-coder:30b&quot;' logs/verbatim_*.jsonl
</code></pre></div>
<h2 id="common-patterns">Common Patterns</h2>
<h3 id="multi-provider-fallback">Multi-Provider Fallback</h3>
<div class="code-block"><pre><code class="language-python">import requests

providers = [
    &quot;ollama/qwen3-coder:30b&quot;,
    &quot;openai/gpt-4o-mini&quot;,
    &quot;anthropic/claude-haiku-4-5&quot;
]

def generate_with_fallback(prompt):
    for model in providers:
        try:
            response = requests.post(
                &quot;http://localhost:8000/v1/chat/completions&quot;,
                json={&quot;model&quot;: model, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]},
                timeout=30
            )
            if response.status_code == 200:
                return response.json()
        except Exception:
            continue
    raise Exception(&quot;All providers failed&quot;)
</code></pre></div>
<h3 id="local-model-gateway">Local Model Gateway</h3>
<div class="code-block"><pre><code class="language-bash"># Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen3-coder:30b

# Use via AbstractCore server
curl -X POST http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;ollama/qwen3-coder:30b&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a Python function&quot;}]
  }'
</code></pre></div>
<hr />
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="server-wont-start">Server Won't Start</h3>
<div class="code-block"><pre><code class="language-bash"># Check port availability
lsof -i :8000

# Use different port
uvicorn abstractcore.server.app:app --port 3000
</code></pre></div>
<h3 id="no-models-available">No Models Available</h3>
<div class="code-block"><pre><code class="language-bash"># Check providers
curl http://localhost:8000/providers

# Check API keys
echo $OPENAI_API_KEY

# Start Ollama
ollama serve
ollama list
</code></pre></div>
<h3 id="authentication-errors">Authentication Errors</h3>
<div class="code-block"><pre><code class="language-bash"># Set API keys
export OPENAI_API_KEY=&quot;sk-...&quot;
export ANTHROPIC_API_KEY=&quot;sk-ant-...&quot;

# Restart server after setting keys
</code></pre></div>
<hr />
<h2 id="why-abstractcore-server">Why AbstractCore Server?</h2>
<ul>
<li><strong>Universal</strong>: One API for all providers  </li>
<li><strong>OpenAI Compatible</strong>: Drop-in replacement  </li>
<li><strong>Simple</strong>: Clean, focused endpoints  </li>
<li><strong>Fast</strong>: Lightweight, high-performance  </li>
<li><strong>Debuggable</strong>: Comprehensive logging  </li>
<li><strong>CLI Ready</strong>: Codex, Gemini CLI, Crush support  </li>
<li><strong>Production Ready</strong>: Docker, multi-worker, health checks  </li>
</ul>
<hr />
<h2 id="related-documentation">Related Documentation</h2>
<ul>
<li><strong><a href="getting-started.html">Getting Started</a></strong> - Core library quick start</li>
<li><strong><a href="architecture.html">Architecture</a></strong> - System architecture including server</li>
<li><strong><a href="api-reference.html">Python API Reference</a></strong> - Core library API</li>
<li><strong><a href="embeddings.html">Embeddings Guide</a></strong> - Embeddings deep dive</li>
<li><strong><a href="troubleshooting.html">Troubleshooting</a></strong> - Common issues and solutions</li>
</ul>
<hr />
<p><strong>AbstractCore Server</strong> - One server, all models, any client.</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
