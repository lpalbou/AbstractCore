<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio & Voice (STT/TTS) - AbstractCore</title>
    <meta name="description" content="Speech-to-text for audio inputs and optional text-to-speech outputs via AbstractCore capability plugins (AbstractVoice).">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [
                    { text: 'Features', href: '/docs/capabilities.html' },
                    { text: 'Quick Start', href: '/docs/getting-started.html' },
                    { text: 'Documentation', href: '/#docs' },
                    { text: 'Examples', href: '/docs/examples.html' },
                    {
                        text: 'GitHub',
                        href: 'https://github.com/lpalbou/abstractcore',
                        target: '_blank',
                        icon: 'github'
                    },
                    {
                        text: 'PyPI',
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Audio &amp; Voice (STT/TTS)</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Transcribe audio into text for LLM requests (policy-driven), and optionally synthesize audio output via capability plugins.
                </p>
            </div>

            <div class="doc-content">
                <h2>Overview</h2>
                <p>
                    AbstractCore treats audio as a first-class media type, but it is intentionally <strong>policy-driven</strong>:
                    by default, attaching audio to an LLM request will <strong>fail loudly</strong> unless you opt into an explicit fallback.
                </p>
                <ul>
                    <li><strong>Audio input (speech-to-text):</strong> set <code>audio_policy="speech_to_text"</code> to inject a transcript into the prompt</li>
                    <li><strong>Auto policy:</strong> <code>audio_policy="auto"</code> uses native audio when supported, otherwise STT when configured</li>
                    <li><strong>Audio output (text-to-speech):</strong> use <code>llm.voice.tts(...)</code> or <code>generate_with_outputs(...)</code> (optional)</li>
                </ul>

                <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; margin: 2rem 0; border-left: 4px solid var(--primary-color);">
                    <p style="margin: 0; color: var(--text-secondary);">
                        Capability backends are provided by optional plugins (for example <code>abstractvoice</code>) and are discovered lazily at runtime.
                        This keeps <code>pip install abstractcore</code> lightweight.
                    </p>
                </div>

                <h2>Install</h2>
                <div class="code-block">
                    <pre><code class="language-bash"># Core + capability plugin (recommended)
pip install abstractcore abstractvoice

# Plus any provider you use
pip install "abstractcore[openai]"       # or "abstractcore[anthropic]" / "abstractcore[huggingface]" / "abstractcore[mlx]" / "abstractcore[vllm]"

# If you also want images/PDF/Office docs
pip install "abstractcore[media]"</code></pre>
                </div>

                <h2>Speech-to-text in <code>generate()</code></h2>
                <p>
                    Attach an audio file via <code>media=[...]</code> and opt into the STT fallback policy.
                    AbstractCore transcribes the audio (via the audio capability plugin), removes the audio from the provider-native media path,
                    and injects a short transcript block into your prompt. This works with any provider/model because the transcript becomes plain text.
                </p>
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")

resp = llm.generate(
    "Summarize the key decisions from this call.",
    media=["./call.wav"],
    audio_policy="speech_to_text",
    audio_language="en",  # optional (also supports stt_language=...)
)

print(resp.content)
print(resp.metadata.get("media_enrichment"))</code></pre>
                </div>

                <h3>Transparency metadata (<code>media_enrichment</code>)</h3>
                <p>
                    When AbstractCore converts a non-text input (image/audio/video) into injected text context, it records what happened in
                    <code>response.metadata.media_enrichment</code> (for example: policy used, modality, and injected transcript size).
                </p>

                <h2>Default policy via centralized config</h2>
                <p>
                    If you attach audio frequently, you can set a default policy in <code>~/.abstractcore/config/abstractcore.json</code>
                    (see <a href="centralized-config.html">Centralized Configuration</a>).
                </p>
                <div class="code-block">
                    <pre><code class="language-json">{
  "audio": {
    "strategy": "speech_to_text",
    "stt_language": "en"
  }
}</code></pre>
                </div>

                <h2>Direct transcription (no LLM call)</h2>
                <p>
                    You can also use the capability surface directly:
                </p>
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

core = create_llm("openai", model="gpt-4o-mini")
text = core.audio.transcribe("./call.wav", language="en")
print(text)</code></pre>
                </div>

                <h2>Text-to-speech output (optional)</h2>
                <p>
                    If a voice backend is available (for example via <code>abstractvoice</code>), you can synthesize audio from text:
                </p>
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

core = create_llm("openai", model="gpt-4o-mini")
wav_bytes = core.voice.tts("Hello from AbstractCore.", format="wav")
print(len(wav_bytes))</code></pre>
                </div>
                <p>
                    Or, as a convenience wrapper, generate text then run TTS as an explicit second step:
                </p>
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

core = create_llm("openai", model="gpt-4o-mini")
result = core.generate_with_outputs(
    "Write a 1-sentence welcome message.",
    outputs={"tts": {"format": "wav"}},
)

print(result.response.content)
wav_bytes = result.outputs["tts"]</code></pre>
                </div>

                <h2>HTTP server: OpenAI-compatible audio endpoints</h2>
                <p>
                    The AbstractCore server exposes OpenAI-compatible audio endpoints backed by capability plugins:
                </p>
                <ul>
                    <li><code>POST /v1/audio/transcriptions</code> (multipart form; speech-to-text)</li>
                    <li><code>POST /v1/audio/speech</code> (JSON; text-to-speech)</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">pip install "abstractcore[server]" abstractvoice
python -m abstractcore.server.app</code></pre>
                </div>
                <div class="code-block">
                    <pre><code class="language-bash"># STT (requires: pip install abstractvoice; python-multipart is included in abstractcore[server])
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F file=@./call.wav \
  -F language=en

# TTS (requires: pip install abstractvoice on the server)
curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"input":"Hello from AbstractCore","format":"wav"}' \
  --output out.wav</code></pre>
                </div>
                <p>See <a href="server.html">HTTP Server Guide</a> for full setup and deployment notes.</p>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Related Documentation</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="media-handling-system.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Media Handling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Files, media types, and fallbacks</p>
                        </a>

                        <a href="centralized-config.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Centralized Configuration</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Set default audio policy and language</p>
                        </a>

                        <a href="server.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">HTTP Server Guide</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">OpenAI-compatible endpoints</p>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
