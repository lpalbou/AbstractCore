<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbstractCore Architecture - AbstractCore</title>
    <meta name="description" content="AbstractCore provides a unified interface to major LLM providers with production-oriented reliability features. This document explains how it works…">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">AbstractCore Architecture</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">AbstractCore provides a unified interface to major LLM providers with production-oriented reliability features. This document explains how it works internally and why it&#x27;s designed this way.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#system-overview" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">System Overview</a>
<a href="#design-principles" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Design Principles</a>
<a href="#core-components" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Components</a>
<a href="#architecture-benefits" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Architecture Benefits</a>
<a href="#extension-points" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Extension Points</a>
<a href="#performance-characteristics" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Performance Characteristics</a>
<a href="#security-considerations" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Security Considerations</a>
<a href="#testing-strategy" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Testing Strategy</a>
<a href="#integration-with-abstractframework" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Integration with AbstractFramework</a>
<a href="#summary" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Summary</a></div>

            <div class="doc-content">

<p>If you're new to AbstractCore and want to start building quickly, read:
- <code>docs/getting-started.md</code>
- <code>docs/api.md</code></p>
<p>Related docs (user-facing):
- Media inputs (images/audio/video + documents): <code>docs/media-handling-system.md</code>
- Vision input + fallback: <code>docs/vision-capabilities.md</code>
- Capability plugins (voice/audio/vision): <code>docs/capabilities.md</code>
- OpenAI-compatible gateway server: <code>docs/server.md</code>
- Single-model OpenAI-compatible endpoint: <code>docs/endpoint.md</code>
- Tool calling semantics (passthrough vs execution): <code>docs/tool-calling.md</code></p>
<h2 id="system-overview">System Overview</h2>
<p>AbstractCore operates as a Python library and can also be exposed via <strong>optional OpenAI-compatible HTTP servers</strong>:</p>
<ul>
<li><strong>Gateway server (multi-provider)</strong>: <code>abstractcore.server.app</code> (docs: <code>docs/server.md</code>)</li>
<li><strong>Endpoint server (single-model)</strong>: <code>abstractcore.endpoint.app</code> (docs: <code>docs/endpoint.md</code>)</li>
</ul>
<div class="mermaid">graph TD
    A[Your Application] --&gt; B[AbstractCore API]
    AA[HTTP Clients] --&gt; BB[AbstractCore Server]
    BB --&gt; B

    B --&gt; C[Provider Interface]
    C --&gt; D[Event System]
    C --&gt; E[Tool System]
    C --&gt; F[Retry System]
    C --&gt; G[Provider Implementations]

    G --&gt; H[OpenAI Provider]
    G --&gt; HH[OpenAI-Compatible Provider]
    G --&gt; I[Anthropic Provider]
    G --&gt; J[Ollama Provider]
    G --&gt; K[MLX Provider]
    G --&gt; L[LMStudio Provider]
    G --&gt; M[HuggingFace Provider]
    G --&gt; MM[vLLM Provider]
    G --&gt; MN[OpenRouter Provider]
    G --&gt; MP[Portkey Provider]

    H --&gt; N[OpenAI API]
    HH --&gt; NN[OpenAI-Compatible /v1 Endpoint]
    I --&gt; O[Anthropic API]
    J --&gt; P[Ollama Server]
    K --&gt; Q[MLX Models]
    L --&gt; R[LMStudio Server]
    M --&gt; S[HuggingFace Models]
    MM --&gt; RR[vLLM Server]
    MN --&gt; RO[OpenRouter API]
    MP --&gt; RP[Portkey API Gateway]

    style B fill:#e1f5fe
    style BB fill:#4caf50
    style C fill:#f3e5f5
    style G fill:#fff3e0</div>

<h2 id="design-principles">Design Principles</h2>
<h3 id="1-provider-abstraction">1. Provider Abstraction</h3>
<p><strong>Goal</strong>: Same interface for all providers
<strong>Implementation</strong>: Common interface with provider-specific implementations</p>
<h3 id="2-production-reliability">2. Production Reliability</h3>
<p><strong>Goal</strong>: Handle real-world failures gracefully
<strong>Implementation</strong>: Built-in retry logic, circuit breakers, comprehensive error handling</p>
<h3 id="3-universal-tool-support">3. Universal Tool Support</h3>
<p><strong>Goal</strong>: Tools work everywhere, even with providers that don't support them natively
<strong>Implementation</strong>: Native support where available, intelligent prompting as fallback</p>
<h3 id="4-simplicity-over-features">4. Simplicity Over Features</h3>
<p><strong>Goal</strong>: Clean, focused API that's easy to understand
<strong>Implementation</strong>: Minimal core with clear extension points</p>
<h3 id="5-optional-http-access">5. Optional HTTP Access</h3>
<p><strong>Goal</strong>: Flexible deployment as library or server
<strong>Implementation</strong>: OpenAI-compatible REST API built on core library</p>
<h2 id="core-components">Core Components</h2>
<h3 id="1-factory-pattern-create_llm">1. Factory Pattern (<code>create_llm</code>)</h3>
<p>The main entry point uses the factory pattern for clean provider instantiation:</p>
<div class="mermaid">graph LR
    A[create_llm] --&gt; B{Provider Type}
    B --&gt; C[OpenAI Provider]
    B --&gt; D[Anthropic Provider]
    B --&gt; E[Ollama Provider]
    B --&gt; F[Other Providers...]

    C --&gt; G[Configured Instance]
    D --&gt; G
    E --&gt; G
    F --&gt; G

    style A fill:#4caf50
    style G fill:#2196f3</div>

<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Factory creates the right provider with proper configuration
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;, temperature=0.7)

# OpenAI-compatible /v1 endpoints (LMStudio, vLLM, custom proxies)
llm_local = create_llm(&quot;lmstudio&quot;, model=&quot;qwen/qwen3-4b-2507&quot;, base_url=&quot;http://localhost:1234/v1&quot;)
llm_openrouter = create_llm(&quot;openrouter&quot;, model=&quot;openai/gpt-4o-mini&quot;)  # requires OPENROUTER_API_KEY
llm_portkey = create_llm(&quot;portkey&quot;, model=&quot;gpt-4o-mini&quot;, config_id=&quot;pcfg_...&quot;)  # requires PORTKEY_API_KEY + PORTKEY_CONFIG
</code></pre></div>
<p>Gateway providers (OpenRouter/Portkey) route to external backends; AbstractCore forwards only <strong>explicit</strong> generation parameters to avoid sending defaults that strict backends reject.</p>
<h3 id="2-provider-interface">2. Provider Interface</h3>
<p>All providers implement <code>AbstractCoreInterface</code> (see <code>abstractcore/core/interface.py</code>):</p>
<div class="code-block"><pre><code class="language-python">class AbstractCoreInterface(ABC):
    @abstractmethod
    def generate(
        self,
        prompt: str,
        messages: Optional[List[Dict[str, str]]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        media: Optional[List[Union[str, Dict[str, Any], &quot;MediaContent&quot;]]] = None,
        stream: bool = False,
        thinking: Optional[Union[bool, str]] = None,
        **kwargs,
    ) -&gt; Union[GenerateResponse, Iterator[GenerateResponse]]:
        &quot;&quot;&quot;Generate a response (or a stream of chunks).&quot;&quot;&quot;

    @abstractmethod
    def get_capabilities(self) -&gt; List[str]:
        &quot;&quot;&quot;Get provider capabilities&quot;&quot;&quot;

    @abstractmethod
    def unload_model(self, model_name: str) -&gt; None:
        &quot;&quot;&quot;Unload/cleanup resources for a specific model (best-effort).&quot;&quot;&quot;
</code></pre></div>
<p>This ensures:
- <strong>Consistency</strong>: Same methods across all providers
- <strong>Reliability</strong>: Standardized error handling
- <strong>Extensibility</strong>: Easy to add new providers
- <strong>Memory Management</strong>: Explicit control over model lifecycle</p>
<h4 id="response-normalization-model-output-cleanup">Response Normalization (Model Output Cleanup)</h4>
<p><code>BaseProvider</code> also applies <strong>asset-driven response normalization</strong> so downstream code sees clean, consistent output across providers:</p>
<ul>
<li><strong>Output wrappers</strong>: Strip configured leading/trailing wrapper tokens (e.g., GLM <code>&lt;|begin_of_box|&gt;…&lt;|end_of_box|&gt;</code>)</li>
<li><strong>Harmony transcripts (GPT-OSS)</strong>: Extract <code>&lt;|channel|&gt;final</code> into <code>GenerateResponse.content</code> and capture <code>&lt;|channel|&gt;analysis</code> as <code>GenerateResponse.metadata["reasoning"]</code> (non-streaming)</li>
<li><strong>Thinking tags</strong>: Extract inline <code>&lt;think&gt;...&lt;/think&gt;</code> blocks into <code>GenerateResponse.metadata["reasoning"]</code> (when configured)</li>
</ul>
<p><strong>Why this belongs in <code>BaseProvider</code> (even for streaming):</strong>
- These artifacts are <strong>model/template-specific</strong>, not provider-specific (the same model can be served via Ollama, vLLM, LMStudio, HF, or MLX)
- In streaming mode, wrappers often appear in the first/last chunks; stripping them incrementally avoids leaking markup into UIs and tool parsers without buffering the full response</p>
<p>Configuration comes from <code>abstractcore/assets/architecture_formats.json</code> and <code>abstractcore/assets/model_capabilities.json</code>; implementation lives in <code>abstractcore/architectures/response_postprocessing.py</code>.</p>
<h4 id="memory-management">Memory Management</h4>
<p>The <code>unload_model(model_name)</code> method is a <strong>best-effort resource cleanup hook</strong>.</p>
<ul>
<li><strong>API providers</strong> (OpenAI, Anthropic): typically a no-op (safe to call).</li>
<li><strong>Local / self-hosted providers</strong>: behavior is provider-specific:</li>
<li>some can actively release memory (or request server-side eviction),</li>
<li>others can only close client connections and rely on server-side TTL/auto-eviction.</li>
<li>Example: <strong>LMStudio</strong> does not expose an explicit “unload model” API; <code>unload_model()</code> closes HTTP clients and relies on LMStudio TTL/auto-evict.</li>
</ul>
<p>In the OpenAI-compatible AbstractCore server (<code>abstractcore.server.app</code>), requests can set <code>unload_after</code> (default <code>false</code>)
to call <code>llm.unload_model(model)</code> after the request completes. For providers that can unload shared server state (e.g. Ollama),
this is disabled by default and must be explicitly enabled by the server operator.</p>
<div class="code-block"><pre><code class="language-python"># Load model, use it, then free memory
llm = create_llm(&quot;ollama&quot;, model=&quot;large-model&quot;)
response = llm.generate(&quot;Hello&quot;)
llm.unload_model(llm.model)  # Explicitly free memory
del llm
</code></pre></div>
<p>This is critical for:
- Test suites that load multiple models sequentially
- Memory-constrained environments (&lt;32GB RAM)
- Production systems serving different models sequentially</p>
<h3 id="3-media-handling-system">3. Media Handling System</h3>
<p>AbstractCore includes a policy-driven media handling system that enables file attachments across all providers:</p>
<div class="mermaid">graph TD
    A[User Input: @file.pdf] --&gt; B[MessagePreprocessor]
    B --&gt; C[Extract Files + Clean Text]
    C --&gt; D[AutoMediaHandler]
    D --&gt; E{File Type Detection}
    E --&gt;|Images| F[ImageProcessor]
    E --&gt;|PDFs| G[PDFProcessor]
    E --&gt;|Office| H[OfficeProcessor]
    E --&gt;|Text/CSV| I[TextProcessor]

    F --&gt; J[MediaContent Objects]
    G --&gt; J
    H --&gt; J
    I --&gt; J

    J --&gt; K{Provider Type}
    K --&gt;|OpenAI| L[OpenAI Format]
    K --&gt;|Anthropic| M[Anthropic Format]
    K --&gt;|Local| N[Text Embedding]

    L --&gt; O[Provider API Call]
    M --&gt; O
    N --&gt; O

    style D fill:#4caf50
    style J fill:#2196f3
    style O fill:#ff9800</div>

<h4 id="media-system-architecture">Media System Architecture</h4>
<p><strong>Core Components:</strong>
- <strong>MessagePreprocessor</strong>: Parses <code>@filename</code> syntax in CLI and extracts file references
- <strong>AutoMediaHandler</strong>: Intelligent coordinator that selects appropriate processors
- <strong>Specialized Processors</strong>:
  - <code>ImageProcessor</code> (PIL-based for images)
  - <code>PDFProcessor</code> (PyMuPDF4LLM for documents)
  - <code>OfficeProcessor</code> (Unstructured for DOCX/XLSX/PPTX)
  - <code>TextProcessor</code> (pandas for CSV/TSV data analysis)
- <strong>Provider Handlers</strong>: Format media content for each provider's API requirements</p>
<p><strong>Provider-Specific Formatting:</strong></p>
<div class="code-block"><pre><code class="language-python"># Same MediaContent gets formatted differently:

# OpenAI (JSON with image_url):
{
  &quot;role&quot;: &quot;user&quot;,
  &quot;content&quot;: [
    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this&quot;},
    {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;data:image/png;base64,...&quot;}}
  ]
}

# Anthropic (Messages API with source):
{
  &quot;role&quot;: &quot;user&quot;,
  &quot;content&quot;: [
    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this&quot;},
    {&quot;type&quot;: &quot;image&quot;, &quot;source&quot;: {&quot;type&quot;: &quot;base64&quot;, &quot;media_type&quot;: &quot;image/png&quot;, &quot;data&quot;: &quot;...&quot;}}
  ]
}

# Local (Text embedding):
&quot;Analyze this\n\nImage description: A chart showing quarterly trends...&quot;
</code></pre></div>
<p><strong>Graceful Fallback Strategy:</strong>
1. <strong>Advanced Processing</strong>: PyMuPDF4LLM, Unstructured libraries
2. <strong>Basic Processing</strong>: Simple text extraction
3. <strong>Metadata Fallback</strong>: File information and properties
4. <strong>Degrades gracefully for documents</strong>: PDFs/Office/text aim to return best-effort extracted text/metadata rather than crashing.
5. <strong>Policy-driven for true multimodal inputs</strong>: for image/audio/video message parts, behavior is policy-driven; unsupported requests fail loudly unless an explicit enrichment fallback is configured (see <code>docs/media-handling-system.md</code> and <code>docs/vision-capabilities.md</code>).</p>
<h4 id="unified-media-api">Unified Media API</h4>
<p>The same <code>media=[]</code> parameter works across all providers:</p>
<div class="code-block"><pre><code class="language-python"># Universal API - works with any provider
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o&quot;)  # or &quot;anthropic&quot;, &quot;ollama&quot;, etc.
response = llm.generate(
    &quot;Analyze these files&quot;,
    media=[&quot;report.pdf&quot;, &quot;chart.png&quot;, &quot;data.xlsx&quot;]
)
</code></pre></div>
<p><strong>CLI Integration:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Simple @filename syntax works everywhere
python -m abstractcore.utils.cli --prompt &quot;What's in @document.pdf and @image.jpg&quot;
</code></pre></div>
<h4 id="capability-plugins-voiceaudiovision">Capability plugins (voice/audio/vision)</h4>
<p>To keep the default <code>abstractcore</code> install dependency-light while still enabling deterministic modality APIs, AbstractCore supports optional <strong>capability plugins</strong>:
- <code>abstractvoice</code> provides <code>core.voice</code> + <code>core.audio</code> (TTS/STT).
- <code>abstractvision</code> provides <code>core.vision</code> (T2I/I2I/T2V/I2V; backend-pluggable).</p>
<p>Discovery:
- <code>llm.capabilities.status()</code> returns a JSON-safe snapshot (which backends are available/selected, plus install hints).
- Convenience facades exist as properties: <code>llm.voice</code>, <code>llm.audio</code>, <code>llm.vision</code> (lazy; missing plugins raise actionable errors).</p>
<h3 id="4-request-lifecycle">4. Request Lifecycle</h3>
<div class="mermaid">sequenceDiagram
    participant App as Your App
    participant Core as AbstractCore
    participant Events as Event System
    participant Retry as Retry Logic
    participant Provider as LLM Provider
    participant Tools as Tool System

    App-&gt;&gt;Core: generate(&quot;prompt&quot;, tools=tools)
    Core-&gt;&gt;Events: emit(GENERATION_STARTED)
    Core-&gt;&gt;Retry: wrap_with_retry()

    alt Provider Call Success
        Retry-&gt;&gt;Provider: API call
        Provider-&gt;&gt;Retry: response
        Retry-&gt;&gt;Core: successful response
    else Provider Call Fails
        Retry-&gt;&gt;Provider: API call (attempt 1)
        Provider-&gt;&gt;Retry: rate limit error
        Retry-&gt;&gt;Retry: wait with backoff
        Retry-&gt;&gt;Provider: API call (attempt 2)
        Provider-&gt;&gt;Retry: success
        Retry-&gt;&gt;Core: successful response
    end

    alt Has Tool Calls
        Core-&gt;&gt;Events: emit(TOOL_STARTED)
        Core-&gt;&gt;Tools: execute_tools()
        Tools-&gt;&gt;Core: tool results
        Core-&gt;&gt;Events: emit(TOOL_COMPLETED)
    end

    Core-&gt;&gt;Events: emit(GENERATION_COMPLETED)
    Core-&gt;&gt;App: GenerateResponse</div>

<p>Note: in the Python API, <code>execute_tools</code> defaults to <code>False</code> (<strong>pass-through</strong>). Tool calls are returned in <code>GenerateResponse.tool_calls</code> for your host/runtime to execute. <code>execute_tools=True</code> exists for simple demos but is deprecated for most production use cases. The optional HTTP gateway server runs in pass-through mode.</p>
<h3 id="5-tool-system-architecture">5. Tool System Architecture</h3>
<p>The tool system provides universal tool-call detection (and optional local execution) across all providers:</p>
<div class="mermaid">graph TD
    A[LLM Response] --&gt; B{Has Tool Calls?}
    B --&gt;|No| C[Return Response]
    B --&gt;|Yes| D[Parse Tool Calls]
    D --&gt; E[Event: TOOL_STARTED]
    E --&gt; F{Event Prevented?}
    F --&gt;|Yes| G[Skip Tool Execution]
    F --&gt;|No| H[Execute Tools]
    H --&gt; I[Collect Results]
    I --&gt; J[Event: TOOL_COMPLETED]
    J --&gt; K[Append Results to Response]
    K --&gt; C

    style D fill:#ffeb3b
    style H fill:#4caf50
    style E fill:#ff9800</div>

<h4 id="tool-execution-flow">Tool Execution Flow</h4>
<ol>
<li><strong>Tool Detection</strong>: Parse tool calls from LLM response</li>
<li><strong>Event Emission</strong>: Emit <code>TOOL_STARTED</code> (preventable)</li>
<li><strong>Optional local execution (deprecated)</strong>: execute tools inside AbstractCore when <code>execute_tools=True</code> (providers never execute arbitrary local tools)</li>
<li><strong>Result Collection</strong>: Gather results and error information</li>
<li><strong>Event Emission</strong>: Emit <code>TOOL_COMPLETED</code> with results</li>
<li><strong>Response Integration</strong>: Append tool results to original response</li>
</ol>
<h4 id="provider-specific-tool-handling-with-tag-rewriting">Provider-Specific Tool Handling with Tag Rewriting</h4>
<div class="mermaid">graph LR
    A[Tool Definition] --&gt; B{Provider Type}
    B --&gt; C[OpenAI: Native JSON]
    B --&gt; D[Anthropic: Native XML]
    B --&gt; E[Ollama: Architecture-specific]
    B --&gt; F[Others: Prompted Format]

    C --&gt; G[LLM Generation]
    D --&gt; G
    E --&gt; G
    F --&gt; G

    G --&gt; H[Tool Call Tag Rewriter]
    H --&gt; I[Target Format Conversion]
    I --&gt; J[Universal Tool Parser]
    J --&gt; K[Local Tool Execution]

    style A fill:#e1f5fe
    style H fill:#ff9800
    style I fill:#9c27b0
    style K fill:#4caf50</div>

<h4 id="tool-call-tag-rewriting-system">Tool Call Tag Rewriting System</h4>
<p>AbstractCore includes a sophisticated tag rewriting system that enables compatibility with any agentic CLI:</p>
<p><strong>Rewriting Pipeline</strong>:</p>
<div class="mermaid">graph TD
    A[Raw LLM Response] --&gt; B[Pattern Detection]
    B --&gt; C{Tag Format Needed?}
    C --&gt;|No| D[Default Qwen3 Format]
    C --&gt;|Yes| E[Target Format Conversion]

    E --&gt; F{Format Type}
    F --&gt;|Predefined| G[llama3, xml, gemma, etc.]
    F --&gt;|Custom| H[User-defined Tags]

    G --&gt; I[Rewritten Tool Call]
    H --&gt; I
    D --&gt; I

    I --&gt; J[Tool Execution]

    style B fill:#2196f3
    style E fill:#ff9800
    style I fill:#4caf50</div>

<p><strong>Supported Formats</strong>:
- <strong>Default (Qwen3)</strong>: <code>&lt;|tool_call|&gt;...JSON...&lt;/|tool_call|&gt;</code> - Compatible with Codex CLI
- <strong>LLaMA3</strong>: <code>&lt;function_call&gt;...JSON...&lt;/function_call&gt;</code> - Compatible with Crush CLI
- <strong>XML</strong>: <code>&lt;tool_call&gt;...JSON...&lt;/tool_call&gt;</code> - Compatible with Gemini CLI
- <strong>Gemma</strong>: <code>tool_code...JSON...</code> - Compatible with Gemma models
- <strong>Custom</strong>: Any user-defined format (e.g., <code>[TOOL]...JSON...[/TOOL]</code>)</p>
<p><strong>Real-Time Integration</strong>:
- <strong>Streaming Compatible</strong>: Works seamlessly with unified streaming architecture
- <strong>Zero Latency</strong>: No additional processing delays
- <strong>Universal Detection</strong>: Automatically detects source format from any model
- <strong>Graceful Fallback</strong>: Returns original content if rewriting fails</p>
<h3 id="6-retry-and-reliability-system">6. Retry and Reliability System</h3>
<p>Production-grade error handling with multiple layers:</p>
<div class="mermaid">graph TD
    A[LLM Request] --&gt; B[Retry Manager]
    B --&gt; C{Error Type}
    C --&gt;|Rate Limit| D[Exponential Backoff]
    C --&gt;|Network Error| D
    C --&gt;|Timeout| D
    C --&gt;|Auth Error| E[Fail Fast]
    C --&gt;|Invalid Request| E

    D --&gt; F{Max Attempts?}
    F --&gt;|No| G[Wait + Jitter]
    G --&gt; H[Retry Request]
    H --&gt; B
    F --&gt;|Yes| I[Circuit Breaker]

    I --&gt; J{Failure Threshold?}
    J --&gt;|No| K[Return Error]
    J --&gt;|Yes| L[Open Circuit]
    L --&gt; M[Fail Fast for Duration]

    style D fill:#ff9800
    style I fill:#f44336
    style L fill:#d32f2f</div>

<h4 id="retry-configuration">Retry Configuration</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

config = RetryConfig(
    max_attempts=3,           # Try up to 3 times
    initial_delay=1.0,        # Start with 1 second delay
    max_delay=60.0,           # Cap at 1 minute
    use_jitter=True,          # Add randomness
    failure_threshold=5,      # Circuit breaker after 5 failures
    recovery_timeout=60.0     # Test recovery after 1 minute
)

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;, retry_config=config)
</code></pre></div>
<h3 id="7-event-system">7. Event System</h3>
<p>Observability hooks through events:</p>
<div class="mermaid">graph TD
    A[LLM Operation] --&gt; B[Event Emission]
    B --&gt; C[Global Event Bus]
    C --&gt; D[Event Listeners]

    D --&gt; E[Monitoring]
    D --&gt; F[Logging]
    D --&gt; G[Cost Tracking]
    D --&gt; H[Tool Control]
    D --&gt; I[Custom Logic]

    E --&gt; J[Metrics Dashboard]
    F --&gt; K[Log Files]
    G --&gt; L[Cost Alerts]
    H --&gt; M[Security Gates]
    I --&gt; N[Business Logic]

    style B fill:#9c27b0
    style C fill:#673ab7
    style H fill:#f44336</div>

<h4 id="event-types-and-use-cases">Event Types and Use Cases</h4>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global

# Cost monitoring (best-effort estimate; based on token usage)
def monitor_costs(event):
    if event.type != EventType.GENERATION_COMPLETED:
        return
    cost = event.data.get(&quot;cost_usd&quot;)
    if isinstance(cost, (int, float)) and cost &gt; 0.10:
        alert(f&quot;High estimated cost: ${cost:.2f}&quot;)

# Tool monitoring
def log_tools(event):
    if event.type == EventType.TOOL_COMPLETED:
        log(f&quot;Tool completed: {event.data.get('tool_name')}&quot;)

# Performance tracking
def track_performance(event):
    if event.type != EventType.GENERATION_COMPLETED:
        return
    duration_ms = event.data.get(&quot;duration_ms&quot;)
    if isinstance(duration_ms, (int, float)) and duration_ms &gt; 10_000:
        log(f&quot;Slow request: {float(duration_ms):.0f}ms&quot;)

on_global(EventType.GENERATION_COMPLETED, monitor_costs)
on_global(EventType.TOOL_COMPLETED, log_tools)
on_global(EventType.GENERATION_COMPLETED, track_performance)
</code></pre></div>
<h3 id="8-structured-output-system-with-streaming-integration">8. Structured Output System with Streaming Integration</h3>
<p>Type-safe responses with automatic validation, retry, and unified streaming:</p>
<div class="mermaid">graph TD
    A[LLM Generate] --&gt; B{Streaming Mode?}
    B --&gt;|Yes| C[Unified Streaming Processor]
    B --&gt;|No| D[Standard JSON Parsing]

    C --&gt; E[Incremental Tool Detector]
    E --&gt; F[Real-time Chunk Processing]
    F --&gt; G[Tool Call Detection]
    G --&gt; H[Mid-Stream Tool Execution]

    D --&gt; I[Parse JSON]
    I --&gt; J{Valid JSON?}
    J --&gt;|No| K[Retry with Error Feedback]
    J --&gt;|Yes| L[Pydantic Validation]

    L --&gt; M{Valid Model?}
    M --&gt;|No| K
    M --&gt;|Yes| N[Return Typed Object]

    K --&gt; O{Max Retries?}
    O --&gt;|No| A
    O --&gt;|Yes| P[Raise ValidationError]

    style C fill:#4caf50
    style E fill:#2196f3
    style F fill:#ff9800
    style G fill:#9c27b0
    style K fill:#f44336</div>

<h4 id="unified-streaming-architecture">Unified Streaming Architecture</h4>
<p>AbstractCore’s streaming system provides character-by-character streaming with incremental tool detection and optional tool-call syntax rewriting.</p>
<p><strong>Architecture Components</strong>:</p>
<div class="mermaid">graph TD
    A[Stream Input] --&gt; B[UnifiedStreamProcessor]
    B --&gt; C[IncrementalToolDetector]
    C --&gt; D[Tag Rewriter]
    D --&gt; E[Tool Execution (optional)]
    E --&gt; F[Stream Output]

    B --&gt; G[Character-by-Character Handling]
    G --&gt; H[Intelligent Buffering]
    H --&gt; C

    style B fill:#4caf50
    style C fill:#2196f3
    style D fill:#ff9800
    style E fill:#9c27b0</div>

<p><strong>Key Features</strong>:</p>
<ol>
<li><strong>Unified Streaming Strategy</strong></li>
<li>Single consistent approach across all providers</li>
<li>Best-effort time-to-first-token (TTFT) telemetry for debugging</li>
<li>
<p>Minimal buffering (incremental parsing)</p>
</li>
<li>
<p><strong>Incremental Tool Detection</strong></p>
</li>
<li>Real-time tool call detection during streaming</li>
<li>Emits <code>chunk.tool_calls</code> as soon as a full tool call is detected</li>
<li>
<p>Handles partial tool calls across chunk boundaries</p>
</li>
<li>
<p><strong>Character-by-Character Streaming</strong></p>
</li>
<li>Handles micro-chunking from providers (very small deltas)</li>
<li>Intelligent buffering for partial tool calls</li>
<li>
<p>Robust parsing with auto-repair for malformed JSON</p>
</li>
<li>
<p><strong>Tool Call Tag Rewriting Integration</strong></p>
</li>
<li>Real-time format conversion during streaming</li>
<li>Support for multiple formats (Qwen3, LLaMA3, Gemma, XML, custom)</li>
<li>Designed to avoid large buffering while keeping tool calls structured</li>
</ol>
<p><strong>Streaming with Tag Rewriting Example</strong>:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

@tool
def analyze_code(code: str) -&gt; str:
    &quot;&quot;&quot;Return a small, deterministic analysis.&quot;&quot;&quot;
    return f&quot;chars={len(code)}&quot;

llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct&quot;)  # requires Ollama running (default: http://localhost:11434)
for chunk in llm.generate(
    &quot;Write a Python function, then call analyze_code on it.&quot;,
    stream=True,
    tools=[analyze_code],
    tool_call_tags=&quot;llama3&quot;,  # Emit &lt;function_call&gt;...&lt;/function_call&gt; style tags
):
    print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)
    if chunk.tool_calls:
        print(f&quot;\nTool calls: {chunk.tool_calls}&quot;)

# Output format: &lt;function_call&gt;{&quot;name&quot;: &quot;analyze_code&quot;}...&lt;/function_call&gt;
</code></pre></div>
<p>Implementation pointers (source of truth):
- Unified streaming + tool detection: <code>abstractcore/providers/streaming.py</code>
- Streaming wrapper + TTFT metadata: <code>abstractcore/providers/base.py</code></p>
<h4 id="automatic-error-feedback">Automatic Error Feedback</h4>
<p>When validation fails, AbstractCore provides detailed feedback to the LLM:</p>
<div class="code-block"><pre><code class="language-python"># If LLM returns invalid data, AbstractCore automatically retries with:
&quot;&quot;&quot;
IMPORTANT: Your previous response had validation errors:
• Field 'age': Age must be positive (got -25)
• Field 'email': Invalid email format

Please correct these errors and provide valid JSON.
&quot;&quot;&quot;
</code></pre></div>
<h3 id="9-session-management">9. Session Management</h3>
<p>Simple conversation memory without complexity:</p>
<div class="mermaid">graph LR
    A[BasicSession] --&gt; B[Message History]
    A --&gt; C[System Prompt]
    A --&gt; D[Provider Reference]

    B --&gt; E[generate()]
    C --&gt; E
    D --&gt; E

    E --&gt; F[Add to History]
    F --&gt; G[Return Response]

    A --&gt; H[save()/load()]
    H --&gt; I[JSON Persistence]

    style A fill:#2196f3
    style B fill:#4caf50</div>

<h3 id="10-server-architecture-optional-component">10. Server Architecture (Optional Component)</h3>
<p>The AbstractCore server provides OpenAI-compatible HTTP endpoints built on top of the core library:</p>
<div class="mermaid">   graph TD
        A[HTTP Client] --&gt; B[FastAPI Server]
        B --&gt; C{Endpoint Router}

        C --&gt; D[/v1/chat/completions]
        C --&gt; E[/v1/embeddings]
        C --&gt; F[/v1/models]
        C --&gt; G[/providers]
        C --&gt; Img[/v1/images/* (optional)]
        C --&gt; Aud[/v1/audio/* (optional)]
        C --&gt; Cache[/acore/prompt_cache/*]

    D --&gt; H[Request Validation]
    E --&gt; H
    F --&gt; I[Provider Discovery]
    G --&gt; I

    H --&gt; J[AbstractCore Library]
    I --&gt; J

    J --&gt; K[Provider Interface]
    K --&gt; L[LLM Providers]

    style B fill:#4caf50
    style J fill:#e1f5fe
    style K fill:#f3e5f5</div>

<p><strong>Architecture Layers</strong>:</p>
<ol>
<li><strong>HTTP Layer</strong>: FastAPI-based REST API with request validation</li>
<li><strong>Translation Layer</strong>: Converts HTTP requests to AbstractCore library calls</li>
<li><strong>Core Layer</strong>: Uses the full AbstractCore provider system</li>
<li><strong>Response Layer</strong>: Transforms responses to OpenAI-compatible format</li>
</ol>
<p><strong>Key Capabilities</strong>:</p>
<ul>
<li><strong>OpenAI Compatibility</strong>: Drop-in replacement for OpenAI API clients</li>
<li><strong>Universal Provider Access</strong>: Single API for all providers (OpenAI, Anthropic, Ollama, etc.)</li>
<li><strong>Format Conversion</strong>: Automatic tool call format conversion for agentic CLIs</li>
<li><strong>Streaming Support</strong>: Server-sent events for real-time responses</li>
<li><strong>Model Discovery</strong>: Dynamic model listing across all providers</li>
<li><strong>Embedding Support</strong>: Multi-provider embedding generation (HuggingFace, Ollama, LMStudio)</li>
<li><strong>Optional Vision Endpoints</strong>: OpenAI-compatible <code>/v1/images/generations</code> and <code>/v1/images/edits</code> (plus <code>/v1/vision/*</code> control plane) delegated to <code>abstractvision</code> (safe-by-default; requires explicit config).</li>
<li><strong>Optional Audio Endpoints</strong>: OpenAI-compatible <code>/v1/audio/transcriptions</code> and <code>/v1/audio/speech</code> delegated to capability plugins (typically <code>abstractvoice</code>).</li>
<li><strong>Prompt Cache Control Plane</strong>: <code>/acore/prompt_cache/*</code> proxy endpoints for cache stats/set/update/fork/clear (best-effort; typically targets an <code>abstractcore.endpoint</code> upstream).</li>
</ul>
<p><strong>Request Flow Example</strong>:</p>
<div class="mermaid">sequenceDiagram
    participant Client
    participant Server as FastAPI Server
    participant Core as AbstractCore
    participant Provider as LLM Provider

    Client-&gt;&gt;Server: POST /v1/chat/completions
    Server-&gt;&gt;Server: Validate Request
    Server-&gt;&gt;Core: create_llm(provider, model)
    Server-&gt;&gt;Core: llm.generate(messages, tools)
    Core-&gt;&gt;Provider: API call with retry logic
    Provider-&gt;&gt;Core: Response
    Core-&gt;&gt;Core: Execute tools if needed
    Core-&gt;&gt;Server: GenerateResponse
    Server-&gt;&gt;Server: Convert to OpenAI format
    Server-&gt;&gt;Client: HTTP Response (streaming or complete)</div>

<p><strong>Server Features</strong>:</p>
<ul>
<li><strong>Automatic Retry</strong>: Built-in retry logic from core library</li>
<li><strong>Event System</strong>: Full observability through events</li>
<li><strong>Debug Logging</strong>: Comprehensive request/response logging</li>
<li><strong>Health Checks</strong>: <code>/health</code> endpoint for monitoring</li>
<li><strong>Interactive Docs</strong>: Auto-generated Swagger UI at <code>/docs</code></li>
<li><strong>Multi-Worker Support</strong>: Production deployment with multiple workers</li>
</ul>
<h2 id="architecture-benefits">Architecture Benefits</h2>
<h3 id="1-provider-agnostic">1. Provider Agnostic</h3>
<ul>
<li><strong>Same code works everywhere</strong>: Switch providers by changing one line</li>
<li><strong>No vendor lock-in</strong>: Easy migration between cloud and local providers</li>
<li><strong>Consistent semantics</strong>: tools, streaming, and structured output follow the same API surface (provider/model differences still apply)</li>
</ul>
<h3 id="2-production-ready">2. Production Ready</h3>
<ul>
<li><strong>Automatic reliability</strong>: Built-in retry logic and circuit breakers</li>
<li><strong>Comprehensive observability</strong>: Events for every operation</li>
<li><strong>Error handling</strong>: Proper error classification and handling</li>
</ul>
<h3 id="3-extensible">3. Extensible</h3>
<ul>
<li><strong>Event system</strong>: Hook into any operation</li>
<li><strong>Tool system</strong>: Add new tools easily</li>
<li><strong>Provider system</strong>: Add new providers with minimal code</li>
</ul>
<h3 id="4-performance-optimized">4. Performance Optimized</h3>
<ul>
<li><strong>Lazy loading</strong>: Providers loaded only when needed</li>
<li><strong>Connection pooling</strong>: Reuse HTTP connections</li>
<li><strong>Efficient parsing</strong>: Optimized JSON and tool parsing</li>
</ul>
<h2 id="extension-points">Extension Points</h2>
<p>AbstractCore is designed to be extended:</p>
<h3 id="adding-a-new-provider">Adding a New Provider</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.providers.base import BaseProvider

class MyProvider(BaseProvider):
    def generate(self, prompt: str, **kwargs) -&gt; GenerateResponse:
        # Implement provider-specific logic
        return GenerateResponse(content=&quot;...&quot;)

    def get_capabilities(self) -&gt; List[str]:
        return [&quot;text_generation&quot;, &quot;streaming&quot;]
</code></pre></div>
<h3 id="adding-tools">Adding Tools</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import tool

@tool
def my_custom_tool(param: str) -&gt; str:
    &quot;&quot;&quot;Custom tool that does something useful.&quot;&quot;&quot;
    return f&quot;Processed: {param}&quot;
</code></pre></div>
<h2 id="performance-characteristics">Performance Characteristics</h2>
<p>AbstractCore’s overhead is usually small compared to model inference and network latency. If performance matters, benchmark on your target provider/model/hardware.</p>
<p>Common levers:
- Provider choice and base URL latency
- Concurrency (async + connection pooling)
- Streaming vs non-streaming
- Structured output (schema size, retry behavior)
- Tool execution strategy (pass-through vs host execution)</p>
<h2 id="security-considerations">Security Considerations</h2>
<h3 id="1-tool-execution-safety">1. Tool Execution Safety</h3>
<ul>
<li><strong>Local execution (optional)</strong>: tool execution is local (never executed by the provider); by default tool calls are returned for your host/runtime to execute</li>
<li><strong>Event prevention</strong>: Stop dangerous tools before execution</li>
<li><strong>Input validation</strong>: Validate tool parameters</li>
</ul>
<h3 id="2-api-key-management">2. API Key Management</h3>
<ul>
<li><strong>Environment variables</strong>: Secure key storage</li>
<li><strong>Avoid logging</strong>: treat logs as sensitive; do not log secrets (AbstractCore tries to avoid printing keys in logs)</li>
<li><strong>Provider isolation</strong>: Keys scoped to specific providers</li>
</ul>
<h3 id="3-data-privacy">3. Data Privacy</h3>
<ul>
<li><strong>Local options</strong>: Support for local providers (Ollama, MLX)</li>
<li><strong>No persistent storage by default</strong>: conversation state lives in memory (for example <code>BasicSession</code>) unless you explicitly save it or enable tracing/logging</li>
<li><strong>Transparent processing</strong>: All operations are observable through events</li>
</ul>
<h2 id="testing-strategy">Testing Strategy</h2>
<p>The repo uses a mix of unit tests and integration tests. Some tests are provider-/network-/hardware-dependent and are opt-in.</p>
<p>Quick pointers:
- Run: <code>pytest -q</code>
- Vision tests: <code>tests/README_VISION_TESTING.md</code>
- Seed tests: <code>tests/README_SEED_TESTING.md</code>
- Streaming/tool parsing tests: <code>tests/streaming/</code> and <code>tests/test_agentic_cli_compatibility.py</code>
- Server/endpoint tests: <code>tests/server/</code> and <code>tests/test_abstractendpoint_singleton_provider.py</code></p>
<h2 id="integration-with-abstractframework">Integration with AbstractFramework</h2>
<p>AbstractCore is a core package in the <strong>AbstractFramework</strong> ecosystem:</p>
<ul>
<li>AbstractFramework (umbrella): https://github.com/lpalbou/AbstractFramework</li>
<li>AbstractCore (this repo): https://github.com/lpalbou/AbstractCore</li>
<li>AbstractRuntime: https://github.com/lpalbou/abstractruntime</li>
</ul>
<p>In this ecosystem, AbstractCore focuses on <strong>LLM I/O + provider abstraction</strong>, while AbstractRuntime focuses on <strong>durable execution</strong> (effects/tools/workflows/state). AbstractCore remains usable standalone; when you need durability/policy/sandboxing around tools, plug it into a runtime (for example AbstractRuntime).</p>
<div class="mermaid">graph TD
    subgraph &quot;UI Layer (peers)&quot;
        A[AbstractCode&lt;br/&gt;Terminal CLI]
        B[AbstractFlow Visual Editor&lt;br/&gt;React + ReactFlow]
    end

    A -.-&gt;|optional| F[AbstractFlow Engine]
    B --&gt; F

    F --&gt; C[AbstractAgent]
    A --&gt; C
    C --&gt; D[AbstractRuntime]
    D --&gt; E[AbstractCore]
    E --&gt; G[LLM Providers]

    style E fill:#e1f5fe
    style A fill:#fff3e0
    style B fill:#fff3e0
    style F fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5</div>

<h3 id="framework-layers">Framework Layers</h3>
<ul>
<li><strong>UI Layer</strong> (peers):</li>
<li>AbstractCode: Terminal CLI for interactive sessions</li>
<li>AbstractFlow Visual Editor: Web-based diagram editor (React + ReactFlow + FastAPI)</li>
<li><strong>AbstractFlow</strong>: Multi-agent orchestration engine + visual editor</li>
<li><strong>AbstractAgent</strong>: Agent patterns (ReactAgent, CodeActAgent) with durable execution</li>
<li><strong>AbstractRuntime</strong>: Effect system, workflows, state persistence</li>
</ul>
<p>AbstractCode can optionally use AbstractFlow for running flows. AbstractFlow includes its own visual editor for designing workflows.</p>
<h2 id="summary">Summary</h2>
<p>AbstractCore's architecture prioritizes:</p>
<ol>
<li><strong>Reliability</strong> - Production-grade error handling and retry logic</li>
<li><strong>Simplicity</strong> - Clean APIs that are easy to understand and use</li>
<li><strong>Universality</strong> - Same interface and features across all providers</li>
<li><strong>Extensibility</strong> - Clear extension points for advanced features</li>
<li><strong>Observability</strong> - Comprehensive events for monitoring and control</li>
<li><strong>Flexibility</strong> - Deploy as Python library or OpenAI-compatible HTTP server</li>
</ol>
<p>The result is a foundation that works reliably in production while remaining simple enough to learn quickly and flexible enough to build advanced applications on top of.</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>mermaid.initialize({ startOnLoad: true, theme: "dark" });</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
