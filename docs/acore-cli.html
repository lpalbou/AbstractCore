<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AbstractCore CLI (acore-cli) - AbstractCore</title>
    <meta name="description" content="AbstractCore includes a built-in CLI tool for testing, demonstration, and interactive conversations. This is AbstractCore&#x27;s internal testing CLI, not to be‚Ä¶">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">AbstractCore CLI (acore-cli)</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">AbstractCore includes a built-in CLI tool for testing, demonstration, and interactive conversations. This is AbstractCore&#x27;s internal testing CLI, not to be confused with external agentic CLIs like Codex or Gemini CLI.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#overview" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Overview</a>
<a href="#async-cli-demo-educational-reference" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Async CLI Demo (Educational Reference)</a>
<a href="#quick-start" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Start</a>
<a href="#media-attachments-path" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Media attachments (@/path)</a>
<a href="#new-commands" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">New Commands</a>
<a href="#usage-examples" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Usage Examples</a></div>

            <div class="doc-content">

<h2 id="overview">Overview</h2>
<p>The AbstractCore internal CLI provides chat history management and system control commands for interactive testing of LLM providers.</p>
<p><strong>Looking for external agentic CLI integration (Codex, Gemini CLI, Crush)?</strong>
‚Üí <strong>See <a href="server.html">Server Documentation</a></strong> for complete setup guides.</p>
<h2 id="async-cli-demo-educational-reference">Async CLI Demo (Educational Reference)</h2>
<p>‚ö†Ô∏è <strong>For developers learning async patterns in AbstractCore</strong></p>
<p>AbstractCore includes an educational async CLI demo that illustrates 8 core async/await patterns:</p>
<div class="code-block"><pre><code class="language-bash"># Run the async demo (educational only)
python examples/async_cli_demo.py --provider ollama --model qwen3:4b
python examples/async_cli_demo.py --provider lmstudio --model qwen/qwen3-vl-30b --stream
</code></pre></div>
<p><strong>What it demonstrates:</strong>
1. Event-driven progress (GlobalEventBus)
2. Async event handlers (on_async)
3. Non-blocking animations (create_task)
4. Async sleep for cooperative multitasking
5. Parallel execution (asyncio.gather)
6. Sync tools in async context (asyncio.to_thread)
7. Async streaming (await + async for)
8. Non-blocking input (asyncio.to_thread)</p>
<p><strong>Key features:</strong>
- ‚úÖ Real-time spinners during tool execution
- ‚úÖ Parallel tool execution with asyncio.gather()
- ‚úÖ Proper async streaming pattern (await first, then async for)
- ‚úÖ Event-driven progress feedback
- ‚úÖ Extensively commented code explaining each pattern</p>
<p><strong>This is for LEARNING ONLY.</strong> For production use, stick with the standard CLI below.</p>
<p><a href="../examples/async_cli_demo.py">View the demo code ‚Üí</a></p>
<h2 id="quick-start">Quick Start</h2>
<div class="code-block"><pre><code class="language-bash"># Start the internal CLI (installed as `abstractcore-chat`)
abstractcore-chat --provider ollama --model qwen3-coder:30b

# Or with any provider
abstractcore-chat --provider openai --model gpt-4o-mini
abstractcore-chat --provider anthropic --model claude-haiku-4-5
abstractcore-chat --provider openrouter --model openai/gpt-4o-mini

# With streaming enabled (--stream flag)
abstractcore-chat --provider ollama --model qwen3-coder:30b --stream

# OpenAI-compatible /v1 endpoints (LM Studio, vLLM, custom proxies)
# Note: include `/v1` in the base URL for OpenAI-compatible servers.
abstractcore-chat --provider lmstudio --model qwen/qwen3-4b-2507 --base-url http://localhost:1234/v1
abstractcore-chat --provider openai-compatible --model local-model --base-url http://localhost:1234/v1

# Control token budgets
abstractcore-chat --provider lmstudio --model qwen/qwen3-4b-2507 --max-tokens 16384 --max-output-tokens 1024
</code></pre></div>
<p>Tip: <code>python -m abstractcore.utils.cli ...</code> is equivalent if you prefer module execution.</p>
<p>Cloud providers require API keys:
- OpenAI: <code>OPENAI_API_KEY</code>
- Anthropic: <code>ANTHROPIC_API_KEY</code>
- OpenRouter: <code>OPENROUTER_API_KEY</code></p>
<h2 id="media-attachments-path">Media attachments (<code>@/path</code>)</h2>
<p>In <code>abstractcore-chat</code>, you can attach local files by referencing them in your prompt:</p>
<div class="code-block"><pre><code class="language-text">what is in this image? @/path/to/photo.jpg
what does it say? @/path/to/audio.wav
summarize this clip @/path/to/video.mp4
</code></pre></div>
<p>Fallback behavior is explicit and policy-driven:</p>
<ul>
<li><strong>Images</strong>: a vision-capable model can process images natively. For text-only models, AbstractCore can use <strong>vision fallback</strong> (caption ‚Üí inject short observations) when configured.</li>
<li><strong>Audio</strong>: if the model is not audio-capable, <code>abstractcore-chat</code> defaults to <code>audio_policy="auto"</code> when audio is attached, so speech-to-text can run when <code>abstractvoice</code> is installed.</li>
<li><strong>Video</strong>: <code>video_policy="auto"</code> uses native video when supported; otherwise it can sample frames via <code>ffmpeg</code>/<code>ffprobe</code> and route them through image/vision handling.</li>
</ul>
<p>Configure defaults (optional):</p>
<div class="code-block"><pre><code class="language-bash">abstractcore --config
abstractcore --set-vision-provider lmstudio qwen/qwen3-vl-4b
abstractcore --set-audio-strategy auto            # requires: pip install abstractvoice
abstractcore --set-video-strategy auto            # frames fallback requires ffmpeg
</code></pre></div>
<p>Per-run overrides:
- <code>abstractcore-chat --audio-policy native_only|speech_to_text|auto</code>
- <code>abstractcore-chat --audio-language en|fr|...</code> (STT hint)</p>
<h2 id="new-commands">New Commands</h2>
<h3 id="token-controls-max-tokens-max-output-tokens">Token Controls (<code>/max-tokens</code>, <code>/max-output-tokens</code>)</h3>
<p>Adjust token limits during a CLI session:</p>
<div class="code-block"><pre><code class="language-bash">/max-tokens 16384
/max-output-tokens 1024
/max-tokens auto
/max-output-tokens auto
</code></pre></div>
<h3 id="thinking-reasoning-control-thinking">Thinking / Reasoning Control (<code>/thinking</code>)</h3>
<p>Toggle unified thinking/reasoning mode (best-effort across providers/models):</p>
<div class="code-block"><pre><code class="language-bash">/thinking auto
/thinking on
/thinking off
/thinking low
/thinking medium
/thinking high
</code></pre></div>
<p>Notes:
- Not every provider/model supports request-side control; unsupported requests may warn and be ignored.
- Some models expose reasoning in <code>response.metadata["reasoning"]</code> (and AbstractCore normalizes/strips it from visible content when configured).</p>
<h3 id="reasoning-display-show-reasoning">Reasoning Display (<code>/show-reasoning</code>)</h3>
<p>Control whether <code>abstractcore-chat</code> displays captured reasoning separately from the assistant answer.</p>
<div class="code-block"><pre><code class="language-bash">/show-reasoning auto
/show-reasoning on
/show-reasoning off
</code></pre></div>
<p>Notes:
- <code>auto</code> shows reasoning whenever the provider returns it (unless you set <code>/thinking off</code> or <code>/show-reasoning off</code>).
- Reasoning is printed in a visually distinct style (best-effort: grey/italic when the terminal supports it).
- When shown, reasoning is printed <strong>before</strong> the assistant answer.
- In <code>--stream</code> mode, <code>abstractcore-chat</code> may buffer output for reasoning-capable models to preserve ‚Äúreasoning first‚Äù display.</p>
<h3 id="compact-chat-history-compaction"><code>/compact</code> - Chat History Compaction</h3>
<p>Compacts your chat history using the fast local <code>gemma3:1b</code> model to create a summary while preserving recent exchanges.</p>
<div class="code-block"><pre><code class="language-bash"># In the CLI
/compact

# Output example:
[COMPACT] Compacting chat history...
   Before: 15 messages (~450 tokens)
   Using gemma3:1b for compaction...
[OK] Compaction completed in 3.2s
   After: 5 messages (~280 tokens)
   Structure:
   1. [SYS] System prompt
   2. [SUMMARY] Conversation summary (1,200 chars)
   3. User: How do I handle errors in Python?
   4. Assistant: You can use try/except blocks...
   5. User: What about logging?
   [NOTE] Note: Token count may increase initially due to detailed summary
       but will decrease significantly as conversation continues
</code></pre></div>
<p><strong>When to use:</strong>
- Long conversations that slow down responses
- When you want to preserve context but reduce memory usage
- Before switching to a different topic</p>
<h3 id="facts-file-extract-facts-from-conversation"><code>/facts [file]</code> - Extract Facts from Conversation</h3>
<p>Extracts facts from your conversation history as simple triples (subject-predicate-object). Display them in chat or save as structured JSON-LD.</p>
<h3 id="judge-evaluate-conversation-quality"><code>/judge</code> - Evaluate Conversation Quality</h3>
<p><strong>NEW</strong>: Evaluates the quality and interest of the current discussion using LLM-as-a-judge. This is a demonstrator showing objective assessment capabilities with constructive feedback.</p>
<div class="code-block"><pre><code class="language-bash"># In the CLI
/judge

# Output example:
[JUDGE] Evaluating conversation quality...
   Analyzing 450 characters of conversation...
[OK] Evaluation completed in 35.2s

[STATS] Overall Discussion Quality: 4/5

[METRICS] Quality Dimensions:
   Clarity      : 5/5
   Coherence    : 4/5
   Actionability: 3/5
   Relevance    : 5/5
   Completeness : 4/5
   Soundness    : 4/5
   Simplicity   : 5/5

[OK] Conversation Strengths:
   ‚Ä¢ The discussion maintains clear focus and addresses questions directly
   ‚Ä¢ Explanations are well-structured and easy to follow
   ‚Ä¢ Technical concepts are explained in accessible language

[NOTE] Suggestions for Better Discussions:
   ‚Ä¢ Include more specific examples to illustrate key points
   ‚Ä¢ Add actionable next steps or recommendations where appropriate
   ‚Ä¢ Consider exploring alternative approaches or edge cases

[ANALYSIS] Assessment Summary:
   The conversation demonstrates strong clarity and relevance with well-structured exchanges.

[INFO] Note: This is a demonstrator showing LLM-as-a-judge capabilities for objective assessment.
</code></pre></div>
<h3 id="intent-participant-analyze-conversation-intents"><code>/intent [participant]</code> - Analyze Conversation Intents</h3>
<p><strong>NEW</strong>: Analyzes the psychological intents and motivations behind conversation messages, including deception detection. This provides insights into communication patterns and underlying goals.</p>
<div class="code-block"><pre><code class="language-bash"># In the CLI
/intent                    # Analyze all participants
/intent user              # Focus on user intents only
/intent assistant         # Focus on assistant intents only

# Output example:
[INTENT] Analyzing user intents in conversation...
   Processing 8 messages...
[OK] Intent analysis completed in 45.3s

[INTENT] CONVERSATION INTENT ANALYSIS
============================================================

User: USER INTENTS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[INTENT] Primary Intent: Problem Solving
   Description: The user is seeking to resolve a technical issue by requesting specific guidance and clarification.
   Underlying Goal: To gain competence and confidence in handling similar problems independently.
   Emotional Undertone: Curious and determined
   Confidence: 0.95 | Urgency: 0.70

[DETECT] DECEPTION ANALYSIS:
   Deception Likelihood: 0.05
   Narrative Consistency: 0.95
   Temporal Coherence: 0.98
   Emotional Congruence: 0.92
   Evidence Indicating Authenticity:
     ‚Ä¢ Direct questions show genuine information-seeking
     ‚Ä¢ Consistent follow-up questions demonstrate engagement
     ‚Ä¢ Acknowledgment of gaps in knowledge

[SECONDARY] Secondary Intents:
   1. Validation Seeking
      Goal: To confirm understanding and receive reassurance about approach
      Confidence: 0.80
   2. Relationship Building
      Goal: To establish rapport and demonstrate engagement
      Confidence: 0.65

[NOTE] Suggested Response Approach:
   Provide clear, step-by-step guidance with examples. Acknowledge their learning progress and offer additional resources for independent exploration.

[STATS] Analysis: 45 words | Complexity: 0.60 | Confidence: 0.92 | Time: 45.3s

============================================================
[NOTE] Note: This analysis identifies underlying motivations and goals behind communication
</code></pre></div>
<p><strong>Key Features:</strong>
- <strong>17 Intent Types</strong>: From basic information-seeking to complex psychological patterns like face-saving and blame deflection
- <strong>Deception Detection</strong>: Authenticity assessment with narrative consistency analysis
- <strong>Multi-Participant Support</strong>: Analyze conversations with multiple participants separately
- <strong>Psychological Depth</strong>: Surface, underlying, or comprehensive analysis levels
- <strong>Evidence-Based</strong>: Concrete textual evidence supporting assessments</p>
<p><strong>When to use:</strong>
- Understanding customer communication patterns and needs
- Analyzing team dynamics and collaboration effectiveness<br />
- Detecting potential deception or authenticity in communications
- Improving response strategies based on underlying motivations
- Training and research in communication analysis</p>
<div class="code-block"><pre><code class="language-bash"># In the CLI
/facts

# Output example:
[DETECT] Extracting facts from conversation history...
   Processing 415 characters of conversation...
[OK] Fact extraction completed in 4.5s

[FACTS] Facts extracted from conversation:
==================================================
 1. OpenAI creates GPT-4
 2. Microsoft Copilot uses GPT-4
 3. Google develops TensorFlow
==================================================
[STATS] Found 3 entities and 2 relationships
</code></pre></div>
<div class="code-block"><pre><code class="language-bash"># Save to JSON-LD file
/facts myconversation

# Output example:
[DETECT] Extracting facts from conversation history...
   Processing 415 characters of conversation...
[OK] Fact extraction completed in 3.3s
[SAVE] Facts saved to myconversation.jsonld
[STATS] Saved 3 entities and 2 relationships as JSON-LD
</code></pre></div>
<p><strong>When to use:</strong>
- Extract key information discussed in the conversation
- Create structured knowledge from chat sessions
- Document facts for later reference or analysis
- Generate semantic data for knowledge graphs</p>
<h3 id="stream-toggle-streaming-mode"><code>/stream</code> - Toggle Streaming Mode</h3>
<p>Toggle real-time streaming of responses. You can also start with streaming enabled using the <code>--stream</code> flag.</p>
<div class="code-block"><pre><code class="language-bash"># Toggle streaming mode in CLI
/stream

# Output examples:
[STREAM] Stream mode: ON
[STREAM] Stream mode: OFF
</code></pre></div>
<p><strong>Two ways to enable streaming:</strong></p>
<div class="code-block"><pre><code class="language-bash"># Method 1: Start with streaming enabled
abstractcore-chat --provider ollama --model qwen3-coder:30b --stream

# Method 2: Toggle during conversation
User: You: /stream
[STREAM] Stream mode: ON
User: You: Write a haiku about programming
Assistant: Assistant: Code flows like a stream... [appears word by word]
</code></pre></div>
<p><strong>When to use:</strong>
- Real-time response display for better user experience
- Immediate feedback during long responses
- Interactive conversations where responsiveness matters</p>
<h3 id="history-n-show-conversation-history"><code>/history [n]</code> - Show Conversation History</h3>
<p>Shows conversation history verbatim without truncation or numbering.</p>
<div class="code-block"><pre><code class="language-bash"># Show all conversation history
/history

# Show last 2 interactions (4 messages: Q, A, Q, A)
/history 2

# Show last 1 interaction (2 messages: Q, A)
/history 1
</code></pre></div>
<p><strong>Output example:</strong></p>
<div class="code-block"><pre><code class="language-bash">/history 2

üìú Last 2 interactions:

User: You:
How do I create a function in Python?

Assistant: Assistant:
You create functions in Python using the 'def' keyword:

def my_function(parameter1, parameter2):
    result = parameter1 + parameter2
    return result

Functions help organize code and avoid repetition.

User: You:
What about error handling?

Assistant: Assistant:
Python uses try/except blocks for error handling:

try:
    risky_operation()
except Exception as e:
    print(f&quot;Error occurred: {e}&quot;)

This allows you to handle errors gracefully.

[STATS] Total tokens estimate: ~150
</code></pre></div>
<p><strong>After using /compact, /history shows both summary and recent messages:</strong></p>
<div class="code-block"><pre><code class="language-bash">/history

üìú Conversation History:

[SUMMARY] Earlier Conversation Summary:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
The conversation covered Python basics, including variable types,
control structures, and recommended practices for writing clean code. The
user asked about functions and we discussed function definition,
parameters, and return values in detail.
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üí¨ Recent Conversation:

User: You:
What about error handling?

Assistant: Assistant:
Python uses try/except blocks for error handling:

try:
    risky_operation()
except Exception as e:
    print(f&quot;Error occurred: {e}&quot;)

This allows you to handle errors gracefully.

[STATS] Total tokens estimate: ~850
</code></pre></div>
<p><strong>Features:</strong>
- <strong>Verbatim display</strong> - Shows complete messages without truncation
- <strong>Summary visibility</strong> - Displays compacted conversation summaries when available
- <strong>Clean formatting</strong> - No message numbers, just clean conversation flow
- <strong>Full content</strong> - Preserves code blocks, formatting, and long responses
- <strong>Flexible viewing</strong> - Show all history or specific number of interactions
- <strong>Context continuity</strong> - Always shows earlier conversation context after compaction</p>
<p><strong>When to use:</strong>
- Review recent conversation context in full detail
- Copy/paste previous responses or code examples
- Verify exact wording of previous exchanges</p>
<h3 id="system-prompt-system-prompt-management"><code>/system [prompt]</code> - System Prompt Management</h3>
<p>Control the system prompt that guides the AI's behavior.</p>
<div class="code-block"><pre><code class="language-bash"># Show current system prompt
/system

# Change system prompt
/system You are a Python expert focused on data science and machine learning.

# Output example for show:
[SYS] Current System Prompt:
==================================================
üìù Fixed Part:
You are a helpful AI assistant.

üîß Full Prompt (as seen by LLM):
System Message 1 (Base):
You are a helpful AI assistant.
==================================================

# Output example for change:
[OK] System prompt updated!
üìù Old: You are a helpful AI assistant.
üìù New: You are a Python expert focused on data science and machine learning.
</code></pre></div>
<p><strong>Features:</strong>
- <strong>Show current prompt</strong> - Displays both fixed part and full prompt seen by LLM
- <strong>Change prompt</strong> - Updates the system prompt while preserving tool configurations
- <strong>Full visibility</strong> - Shows all system messages including summaries from compaction
- <strong>Safe updates</strong> - Only changes the core prompt, preserves conversation history</p>
<p><strong>When to use:</strong>
- Adapt AI behavior for specific tasks (coding, writing, analysis)
- Review what instructions the AI is currently following
- Switch between different AI personas during conversation
- Debug unexpected AI behavior by checking the active system prompt</p>
<h3 id="tooltag-opening_tag-closing_tag-test-tool-call-tag-rewriting"><code>/tooltag &lt;opening_tag&gt; &lt;closing_tag&gt;</code> - Test Tool Call Tag Rewriting</h3>
<p>Test tool call tag rewriting with custom tags to verify compatibility with different agentic CLIs.</p>
<div class="code-block"><pre><code class="language-bash"># Test LLaMA3 format (Crush CLI)
/tooltag '&lt;function_call&gt;' '&lt;/function_call&gt;'

# Test XML format (Gemini CLI)
/tooltag '&lt;tool_call&gt;' '&lt;/tool_call&gt;'

# Test Qwen3 format (Codex CLI)
/tooltag '&lt;|tool_call|&gt;' '&lt;/|tool_call|&gt;'

# Test Gemma format
/tooltag '```tool_code' '```'

# Test custom format
/tooltag '&lt;my_tool&gt;' '&lt;/my_tool&gt;'
</code></pre></div>
<p><strong>Features:</strong>
- Tests custom tool call tag formats
- Compares with default behavior
- Works with both streaming and non-streaming modes
- Verifies tag rewriting functionality
- Shows detailed results and analysis</p>
<p><strong>Example Output:</strong></p>
<div class="code-block"><pre><code>üè∑Ô∏è Testing Tool Call Tag Rewriting
üìù Opening Tag: '&lt;function_call&gt;'
üìù Closing Tag: '&lt;/function_call&gt;'
============================================================
Assistant: Testing with openai:gpt-4o-mini
[STREAM] Streaming: OFF
üìù Prompt: Please help me with two tasks:
1. Get the current weather for Paris
2. Calculate what 15 * 23 equals

Use the available tools to help me with these tasks.

============================================================
üîß Testing with custom tool call tags...
Assistant: Assistant (with custom tags): &lt;function_call&gt;{&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: {&quot;location&quot;: &quot;Paris&quot;}}&lt;/function_call&gt;

üîß Testing without custom tags (default behavior)...
Assistant: Assistant (default): &lt;|tool_call|&gt;{&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: {&quot;location&quot;: &quot;Paris&quot;}}&lt;/|tool_call|&gt;

============================================================
[STATS] Summary:
   Model: openai:gpt-4o-mini
   Streaming: OFF
   Custom tags: '&lt;function_call&gt;'...'&lt;/function_call&gt;'
   Custom tags found: [OK] YES
   Default tags found: [OK] YES
</code></pre></div>
<p><strong>When to use:</strong>
- Test compatibility with specific agentic CLIs (Codex, Crush, Gemini CLI)
- Verify tool call tag rewriting works correctly
- Debug tool call format issues
- Compare different tag formats side by side
- Test both streaming and non-streaming modes</p>
<h2 id="usage-examples">Usage Examples</h2>
<h3 id="starting-the-cli-with-auto-compact">Starting the CLI with Auto-Compact</h3>
<div class="code-block"><pre><code class="language-bash"># Start CLI with a model that supports auto-compaction
abstractcore-chat --provider ollama --model gemma3:1b

# Have a long conversation...
User: You: What is Python?
Assistant: Assistant: Python is a high-level programming language...

User: You: What about data types?
Assistant: Assistant: Python has several built-in data types...

# Check history
User: You: /history 2

# Change system prompt for specific task
User: You: /system You are an expert Python tutor focused on data science.

# Compact when needed
User: You: /compact

# Extract facts from the conversation
User: You: /facts

# Continue conversation with preserved context and new system prompt
User: You: Can you explain more about functions?
Assistant: Assistant: [Responds as Python tutor, refers to previous conversation context]
</code></pre></div>
<h3 id="best-practices">Best Practices</h3>
<ol>
<li><strong>Regular History Checks</strong>: Use <code>/history n</code> to review recent context</li>
<li><strong>Strategic Compaction</strong>: Use <code>/compact</code> when:</li>
<li>Conversation becomes slow</li>
<li>You've covered multiple topics</li>
<li>Before switching contexts</li>
<li><strong>System Prompt Control</strong>: Use <code>/system</code> to:</li>
<li>Prevent unwanted tool calls with specific instructions</li>
<li>Adapt AI behavior for different tasks</li>
<li>Debug unexpected responses by checking active prompts</li>
<li><strong>Context Verification</strong>: After compaction, use <code>/history</code> to verify preserved context</li>
</ol>
<h3 id="controlling-tool-usage">Controlling Tool Usage</h3>
<p>If the AI is calling tools unnecessarily (like the example where it called <code>list_files</code> just to demonstrate capabilities):</p>
<div class="code-block"><pre><code class="language-bash"># Check current system prompt to understand behavior
User: You: /system

# Update to prevent unnecessary tool calls
User: You: /system You are a helpful AI assistant. Only use tools when explicitly requested by the user or when necessary to answer a specific question. Do not demonstrate tool capabilities unprompted.

# Now the AI will be less likely to call tools unnecessarily
User: You: Who are you?
Assistant: Assistant: I am an AI assistant. [No tool calls]
</code></pre></div>
<h3 id="technical-details">Technical Details</h3>
<ul>
<li><strong>Compaction Model</strong>: Uses <code>gemma3:1b</code> for fast, local processing</li>
<li><strong>Preservation</strong>: Keeps last 6 messages (3 exchanges) by default</li>
<li><strong>Fallback</strong>: If <code>gemma3:1b</code> unavailable, uses current provider</li>
<li><strong>Token Estimation</strong>: Provides accurate token count estimates using centralized TokenUtils</li>
<li><strong>Message Types</strong>: Distinguishes between system, user, and assistant messages</li>
</ul>
<h3 id="error-handling">Error Handling</h3>
<p>The commands gracefully handle various scenarios:
- Not enough history to compact
- Invalid parameters for <code>/last</code>
- Missing models or providers
- Empty conversation history</p>
<p>All errors are clearly reported with helpful messages.</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
