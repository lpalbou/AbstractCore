<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API (Python) - AbstractCore</title>
    <meta name="description" content="This page is a user-facing map of the public Python API exposed from abstractcore (see abstractcore/__init__.py). For a complete listing of functions/classes…">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">API (Python)</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">This page is a user-facing map of the public Python API exposed from abstractcore (see abstractcore/__init__.py). For a complete listing of functions/classes (including events), see API Reference.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#core-entrypoints" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core entrypoints</a>
<a href="#responses-generateresponse" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Responses (GenerateResponse)</a>
<a href="#model-downloads-download_model-optional" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Model downloads (download_model, optional)</a>
<a href="#tool-calling" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Tool calling</a>
<a href="#structured-output" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Structured output</a>
<a href="#media-input" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Media input</a>
<a href="#http-api-optional" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">HTTP API (optional)</a></div>

            <div class="doc-content">

<p>New to AbstractCore? Start with <strong><a href="getting-started.html">Getting Started</a></strong>.</p>
<p>Implementation pointers (source of truth):
- <code>create_llm</code>: <code>abstractcore/core/factory.py</code> → <code>abstractcore/providers/registry.py</code>
- <code>BasicSession</code>: <code>abstractcore/core/session.py</code>
- Response/types: <code>abstractcore/core/types.py</code>
- Tool decorator: <code>abstractcore/tools/core.py</code></p>
<h2 id="core-entrypoints">Core entrypoints</h2>
<h3 id="create_llm"><code>create_llm(...)</code></h3>
<p>Create a provider instance:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)  # requires: pip install &quot;abstractcore[openai]&quot;
resp = llm.generate(&quot;Hello!&quot;)
print(resp.content)
</code></pre></div>
<p>Provider IDs (common): <code>openai</code>, <code>anthropic</code>, <code>openrouter</code>, <code>portkey</code>, <code>ollama</code>, <code>lmstudio</code>, <code>vllm</code>, <code>openai-compatible</code>, <code>huggingface</code>, <code>mlx</code>.</p>
<h3 id="gateway-providers-openrouter-portkey">Gateway providers (OpenRouter, Portkey)</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm_openrouter = create_llm(&quot;openrouter&quot;, model=&quot;openai/gpt-4o-mini&quot;)
llm_portkey = create_llm(&quot;portkey&quot;, model=&quot;gpt-5-mini&quot;, api_key=&quot;PORTKEY_API_KEY&quot;, config_id=&quot;pcfg_...&quot;)
</code></pre></div>
<p>Gateway notes:
- OpenRouter uses <code>OPENROUTER_API_KEY</code> (model names like <code>openai/...</code>).
- Portkey uses <code>PORTKEY_API_KEY</code> plus a config id (<code>PORTKEY_CONFIG</code>).
- Optional generation parameters (<code>temperature</code>, <code>top_p</code>, <code>max_output_tokens</code>, etc.) are only forwarded when explicitly set.</p>
<h3 id="basicsession"><code>BasicSession</code></h3>
<p>Keep conversation state:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import BasicSession, create_llm

session = BasicSession(create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;))  # requires: abstractcore[anthropic]
print(session.generate(&quot;Give me 3 name ideas.&quot;).content)
print(session.generate(&quot;Pick the best one.&quot;).content)
</code></pre></div>
<h3 id="tool-decorator"><code>tool</code> (decorator)</h3>
<p>Define tools in Python with a decorator, then pass them to <code>generate()</code> / <code>agenerate()</code>:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

@tool
def get_weather(city: str) -&gt; str:
    return f&quot;{city}: 22°C and sunny&quot;

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
resp = llm.generate(&quot;Use the tool.&quot;, tools=[get_weather])
print(resp.tool_calls)
</code></pre></div>
<h2 id="responses-generateresponse">Responses (<code>GenerateResponse</code>)</h2>
<p>Most calls return a <code>GenerateResponse</code> object (or an iterator of them for streaming). Common fields:</p>
<ul>
<li><code>content</code>: cleaned assistant text</li>
<li><code>tool_calls</code>: structured tool calls (pass-through by default)</li>
<li><code>usage</code>: token usage (provider-dependent)</li>
<li><code>metadata</code>: provider/model specific fields (for example extracted reasoning text when configured)</li>
</ul>
<h2 id="model-downloads-download_model-optional">Model downloads (<code>download_model</code>, optional)</h2>
<p><code>download_model(...)</code> is an <strong>async generator</strong> that yields <code>DownloadProgress</code> updates while a model is being fetched.</p>
<p>Supported providers:
- <code>ollama</code>: pulls via the Ollama HTTP API (<code>/api/pull</code>)
- <code>huggingface</code> / <code>mlx</code>: downloads from HuggingFace Hub (requires <code>pip install "abstractcore[huggingface]"</code>; pass <code>token=</code> for gated models)</p>
<p>Example:</p>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import download_model

async def main():
    async for p in download_model(&quot;ollama&quot;, &quot;qwen3:4b-instruct-2507-q4_K_M&quot;):
        print(p.status.value, p.message)

asyncio.run(main())
</code></pre></div>
<p>Implementation: <code>abstractcore/download.py</code>. For provider setup and base URLs, see <a href="prerequisites.html">Prerequisites</a>.</p>
<h2 id="tool-calling">Tool calling</h2>
<p>Tools are passed explicitly to <code>generate()</code> / <code>agenerate()</code>:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

@tool
def get_weather(city: str) -&gt; str:
    return f&quot;{city}: 22°C and sunny&quot;

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
resp = llm.generate(&quot;Use the tool.&quot;, tools=[get_weather])
print(resp.tool_calls)
</code></pre></div>
<p>See <strong><a href="tool-calling.html">Tool Calling</a></strong> and <strong><a href="tool-syntax-rewriting.html">Tool Syntax Rewriting</a></strong>.</p>
<h3 id="built-in-tools-optional">Built-in tools (optional)</h3>
<p>If you want a ready-made toolset (web + filesystem helpers), install:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[tools]&quot;
</code></pre></div>
<p>Then import from <code>abstractcore.tools.common_tools</code> (for example <code>web_search</code>, <code>skim_websearch</code>, <code>skim_url</code>, <code>fetch_url</code>). See <strong><a href="tool-calling.html">Tool Calling</a></strong> for usage patterns and when to use <code>skim_*</code> vs <code>fetch_*</code>.</p>
<h2 id="structured-output">Structured output</h2>
<p>Pass a Pydantic model via <code>response_model=...</code> to receive a typed result:</p>
<div class="code-block"><pre><code class="language-python">from pydantic import BaseModel
from abstractcore import create_llm

class Answer(BaseModel):
    title: str
    bullets: list[str]

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
result = llm.generate(&quot;Summarize HTTP/3 in 3 bullets.&quot;, response_model=Answer)
print(result.bullets)
</code></pre></div>
<p>See <strong><a href="structured-output.html">Structured Output</a></strong>.</p>
<h2 id="media-input">Media input</h2>
<p>Media handling is opt-in:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[media]&quot;
</code></pre></div>
<p>Then pass <code>media=[...]</code> to <code>generate()</code> / <code>agenerate()</code> (or use the media pipeline). Media behavior is <strong>policy-driven</strong>:</p>
<ul>
<li>Images: use a vision-capable model, or configure vision fallback (caption → inject short observations).</li>
<li>Video: controlled by <code>video_policy</code> (native when supported; otherwise frame sampling via <code>ffmpeg</code> + vision handling).</li>
<li>Audio: controlled by <code>audio_policy</code> (native when supported; otherwise optional speech-to-text via <code>abstractvoice</code>).</li>
</ul>
<p>See <strong><a href="media-handling-system.html">Media Handling</a></strong>, <strong><a href="vision-capabilities.html">Vision Capabilities</a></strong>, and <strong><a href="centralized-config.html">Centralized Config</a></strong>.</p>
<h2 id="http-api-optional">HTTP API (optional)</h2>
<p>If you want an OpenAI-compatible <code>/v1</code> gateway, install and run the server:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[server]&quot;
python -m abstractcore.server.app
</code></pre></div>
<p>See <strong><a href="server.html">Server</a></strong>.</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
