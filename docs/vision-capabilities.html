<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision in AbstractCore (Image/Video Input) - AbstractCore</title>
    <meta name="description" content="This document describes vision as an input modality in AbstractCore (images and video-understanding), and clarifies how it relates to: - vision fallback (caption → inject short…">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Vision in AbstractCore (Image/Video Input)</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">This document describes vision as an input modality in AbstractCore (images and video-understanding), and clarifies how it relates to: - vision fallback (caption → inject short observations), and - generative vision (image/video creation), which lives in abstractvision .</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#quick-requirements" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick requirements</a>
<a href="#1-imagevideo-input-modalities-owned-by-abstractcore" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">1) Image/video input modalities (owned by AbstractCore)</a>
<a href="#2-vision-fallback-for-text-only-models-optional-config-driven" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">2) Vision fallback for text-only models (optional; config-driven)</a>
<a href="#3-generative-vision-output-is-not-part-of-abstractcores-default-install" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">3) Generative vision output is not part of AbstractCore’s default install</a>
<a href="#troubleshooting-common" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Troubleshooting (common)</a>
<a href="#related" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Related</a></div>

            <div class="doc-content">


<h2 id="quick-requirements">Quick requirements</h2>
<ul>
<li><strong>Images</strong>: install <code>pip install "abstractcore[media]"</code> and use either:</li>
<li>a <strong>vision-capable model</strong> (VLM/VL), or</li>
<li>a text-only model with <strong>vision fallback</strong> configured (<code>abstractcore --set-vision-provider PROVIDER MODEL</code>).</li>
<li><strong>Video</strong>: native video input is model/provider dependent. For the portable frame-sampling path (<code>video_policy="frames_caption"</code> / <code>"auto"</code> fallback), you need:</li>
<li><code>ffmpeg</code>/<code>ffprobe</code> available on <code>PATH</code>, and</li>
<li>image/vision handling (a vision-capable model or configured vision fallback).</li>
</ul>
<h2 id="1-imagevideo-input-modalities-owned-by-abstractcore">1) Image/video input modalities (owned by AbstractCore)</h2>
<p>Attach media to an LLM call using <code>media=[...]</code>:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")  # example; pick a vision-capable model you have access to
resp = llm.generate("What is in this image?", media=["photo.jpg"])
print(resp.content)
</code></pre></div>
<p>Support depends on the selected provider/model and is normalized via:
- <code>abstractcore/assets/model_capabilities.json</code></p>
<p>Video attachments use the same <code>media=[...]</code> surface and are controlled by <code>video_policy</code> (see <code>abstractcore/providers/base.py</code>):</p>
<div class="code-block"><pre><code class="language-python">resp = llm.generate(
    "Summarize what happens in this clip.",
    media=["clip.mp4"],
    video_policy="auto",  # native when supported; otherwise sample frames
)
</code></pre></div>
<p>You can tune frame sampling defaults via the config CLI:</p>
<div class="code-block"><pre><code class="language-bash">abstractcore --set-video-strategy auto
abstractcore --set-video-max-frames 6
abstractcore --set-video-sampling-strategy keyframes
</code></pre></div>
<h2 id="2-vision-fallback-for-text-only-models-optional-config-driven">2) Vision fallback for text-only models (optional; config-driven)</h2>
<p>When a user attaches an image to a text-only model, AbstractCore can optionally run a <strong>two-stage fallback</strong>:
1) run a configured vision-capable backend to produce <strong>short grounded observations</strong>, then
2) inject those observations into the main request.</p>
<p>This is:
- <strong>explicit</strong> (config-driven; not a silent default), and
- <strong>transparent</strong> via response metadata (<code>metadata.media_enrichment[]</code>).</p>
<p>Code pointers:
- Fallback handler: <code>abstractcore/media/vision_fallback.py</code>
- Enrichment metadata: <code>abstractcore/media/enrichment.py</code></p>
<p>Configure vision fallback via the config CLI:</p>
<div class="code-block"><pre><code class="language-bash">abstractcore --set-vision-provider lmstudio qwen/qwen3-vl-4b
abstractcore --add-vision-fallback huggingface Salesforce/blip-image-captioning-base
</code></pre></div>
<h2 id="3-generative-vision-output-is-not-part-of-abstractcores-default-install">3) Generative vision output is not part of AbstractCore’s default install</h2>
<p>Creating/editing images and videos is a <strong>deterministic capability</strong> that lives in <code>abstractvision</code> and can be integrated in two ways:</p>
<p>1) <strong>Capability plugin (library mode)</strong>: install <code>abstractvision</code> and use <code>llm.vision.*</code> (e.g. <code>t2i</code>, <code>i2i</code>).<br/>
   See: <code>abstractvision/docs/reference/abstractcore-integration.md</code></p>
<p>2) <strong>AbstractCore Server (HTTP interop)</strong>: run the optional server and enable <code>/v1/images/*</code> endpoints delegated to <code>abstractvision</code>.<br/>
   See: <code>docs/server.md</code></p>
<p>This separation keeps <code>abstractcore</code> dependency-light (ADR-0001 / ADR-0028).</p>
<h2 id="troubleshooting-common">Troubleshooting (common)</h2>
<ul>
<li><strong>“Image input is not supported by model …”</strong>: choose a vision-capable model, or configure vision fallback.</li>
<li><strong>Vision fallback errors</strong>: confirm your AbstractCore config enables it and that the configured backend is reachable/works.</li>
<li><strong>Video frame fallback issues</strong>: frame extraction relies on <code>ffmpeg</code>/<code>ffprobe</code> availability in the runtime environment, and requires image/vision handling (vision-capable model or configured vision fallback).</li>
</ul>
<h2 id="related">Related</h2>
<ul>
<li>Media pipeline overview: <code>docs/media-handling-system.md</code></li>
<li>Server endpoints: <code>docs/server.md</code></li>
<li>Framework-level guide (input vs capabilities): <code>docs/guide/capabilities.md</code> (monorepo)</li>
</ul>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
