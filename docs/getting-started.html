<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Getting Started - AbstractCore</title>
    <meta name="description" content="AbstractCore is a unified Python interface for cloud + local LLM providers. The default install is lightweight; add features via extras.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Getting Started</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">AbstractCore is a unified Python interface for cloud + local LLM providers. The default install is lightweight; add features via extras.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#prerequisites" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Prerequisites</a>
<a href="#installation" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Installation</a>
<a href="#providers-and-models" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Providers and models</a>
<a href="#your-first-call" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Your first call</a>
<a href="#streaming" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Streaming</a>
<a href="#tool-calling" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Tool calling</a>
<a href="#structured-output" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Structured output</a>
<a href="#media-input-imagesaudiovideo-documents" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Media input (images/audio/video + documents)</a>
<a href="#async" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Async</a>
<a href="#cli-optional" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">CLI (optional)</a>
<a href="#next-steps" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Next steps</a></div>

            <div class="doc-content">

<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Python 3.9+</li>
<li><code>pip</code></li>
</ul>
<h2 id="installation">Installation</h2>
<div class="code-block"><pre><code class="language-bash"># Core (small, lightweight default)
pip install abstractcore

# Providers (install only what you use)
pip install &quot;abstractcore[openai]&quot;       # OpenAI SDK
pip install &quot;abstractcore[anthropic]&quot;    # Anthropic SDK
pip install &quot;abstractcore[huggingface]&quot;  # Transformers / torch (heavy)
pip install &quot;abstractcore[mlx]&quot;          # Apple Silicon local inference (heavy)
pip install &quot;abstractcore[vllm]&quot;         # GPU inference server integrations (heavy)

# Optional features
pip install &quot;abstractcore[tools]&quot;        # built-in tools (web/file/command helpers)
pip install &quot;abstractcore[media]&quot;        # images, PDFs, Office docs
pip install &quot;abstractcore[compression]&quot;  # glyph visual-text compression (Pillow renderer)
pip install &quot;abstractcore[embeddings]&quot;   # EmbeddingManager + local embedding models
pip install &quot;abstractcore[tokens]&quot;       # precise token counting (tiktoken)
pip install &quot;abstractcore[server]&quot;       # OpenAI-compatible HTTP gateway

# Combine extras (zsh: keep quotes)
pip install &quot;abstractcore[openai,media,tools]&quot;
</code></pre></div>
<p>Local OpenAI-compatible servers (Ollama, LMStudio, vLLM, llama.cpp, LocalAI, etc.) work with the core install; you just point AbstractCore at the server base URL. See <a href="prerequisites.html">Prerequisites</a> for provider setup.</p>
<p>Optional capability plugins (deterministic multimodal outputs):</p>
<div class="code-block"><pre><code class="language-bash">pip install abstractvoice   # enables llm.voice / llm.audio (TTS/STT)
pip install abstractvision  # enables llm.vision (generative vision; typically via an OpenAI-compatible images endpoint)
</code></pre></div>
<p>See: <a href="capabilities.html">Capabilities</a> and <a href="server.html">Server</a>.</p>
<h2 id="providers-and-models">Providers and models</h2>
<p>AbstractCore uses a provider ID plus a model name:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
# llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
# llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct-2507-q4_K_M&quot;)
# llm = create_llm(&quot;lmstudio&quot;, model=&quot;qwen/qwen3-4b-2507&quot;)
# llm = create_llm(&quot;openai-compatible&quot;, model=&quot;default&quot;, base_url=&quot;http://localhost:1234/v1&quot;)
</code></pre></div>
<p>Tip: you can omit <code>model=...</code>, but it’s usually better to pass an explicit model to avoid surprises when defaults change.</p>
<p>Open-source-first: start with local providers (Ollama, LMStudio, MLX, HuggingFace), then add cloud or gateway providers as needed.</p>
<p>Gateway providers (OpenRouter, Portkey) examples:</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm_openrouter = create_llm(&quot;openrouter&quot;, model=&quot;openai/gpt-4o-mini&quot;)
llm_portkey = create_llm(&quot;portkey&quot;, model=&quot;gpt-5-mini&quot;, api_key=&quot;PORTKEY_API_KEY&quot;, config_id=&quot;pcfg_...&quot;)
</code></pre></div>
<p>Note: gateway providers only forward optional generation params (e.g. <code>temperature</code>, <code>top_p</code>, <code>max_output_tokens</code>) when you explicitly set them.</p>
<h2 id="your-first-call">Your first call</h2>
<p>OpenAI example (requires <code>pip install "abstractcore[openai]"</code>):</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
resp = llm.generate(&quot;What is the capital of France?&quot;)
print(resp.content)
</code></pre></div>
<h2 id="streaming">Streaming</h2>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b-instruct-2507-q4_K_M&quot;)
for chunk in llm.generate(&quot;Write a short poem about distributed systems.&quot;, stream=True):
    print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)
</code></pre></div>
<h2 id="tool-calling">Tool calling</h2>
<p>AbstractCore supports native tool calling (when the provider supports it) and prompted tool syntax (when it doesn’t).</p>
<p>By default, tool execution is pass-through (<code>execute_tools=False</code>): you get tool calls in <code>resp.tool_calls</code>, and your host/runtime decides how to execute them.</p>
<p>In the AbstractFramework ecosystem, <strong>AbstractRuntime</strong> is the recommended runtime for executing tool calls durably (policy, retries, persistence). See <a href="architecture.html">Architecture</a> and <a href="tool-calling.html">Tool Calling</a>.</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, tool

@tool
def get_weather(city: str) -&gt; str:
    return f&quot;{city}: 22°C and sunny&quot;

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
resp = llm.generate(&quot;What's the weather in Paris? Use the tool.&quot;, tools=[get_weather])

print(resp.content)
print(resp.tool_calls)
</code></pre></div>
<p>See <a href="tool-calling.html">Tool Calling</a> and <a href="tool-syntax-rewriting.html">Tool Syntax Rewriting</a> (<code>tool_call_tags</code>, server <code>agent_format</code>).</p>
<h3 id="built-in-tools-optional">Built-in tools (optional)</h3>
<p>If you want a ready-made toolset for agentic scripts, install:</p>
<div class="code-block"><pre><code class="language-bash">pip install &quot;abstractcore[tools]&quot;
</code></pre></div>
<p>Then import from <code>abstractcore.tools.common_tools</code>:</p>
<ul>
<li><code>skim_websearch</code> vs <code>web_search</code>: compact/filtered links vs full results</li>
<li><code>skim_url</code> vs <code>fetch_url</code>: fast URL triage (small output) vs full fetch + parsing for text-first types (HTML/JSON/text)</li>
</ul>
<p>See <a href="tool-calling.html">Tool Calling</a> for a recommended workflow and the full built-in tool list.</p>
<h2 id="structured-output">Structured output</h2>
<p>Pass a Pydantic model via <code>response_model=...</code> to get a typed result back (instead of parsing JSON yourself):</p>
<div class="code-block"><pre><code class="language-python">from pydantic import BaseModel
from abstractcore import create_llm

class Answer(BaseModel):
    title: str
    bullets: list[str]

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
answer = llm.generate(&quot;Summarize HTTP/3 in 3 bullets.&quot;, response_model=Answer)
print(answer.bullets)
</code></pre></div>
<p>See <a href="structured-output.html">Structured Output</a> for strategy details and limitations.</p>
<h2 id="media-input-imagesaudiovideo-documents">Media input (images/audio/video + documents)</h2>
<p>Images and document extraction require <code>pip install "abstractcore[media]"</code> (Pillow + PDF/Office deps).</p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
resp = llm.generate(&quot;Describe the image.&quot;, media=[&quot;./image.png&quot;])
print(resp.content)
</code></pre></div>
<p>Audio and video attachments are also supported, but they are <strong>policy-driven</strong> (no silent semantic changes):
- audio: <code>audio_policy</code> (<code>native_only|speech_to_text|auto|caption</code>)
- video: <code>video_policy</code> (<code>native_only|frames_caption|auto</code>)</p>
<p>Speech-to-text fallback (<code>audio_policy="speech_to_text"</code>) typically requires installing <code>abstractvoice</code> (capability plugin).</p>
<p>What you need (quick checklist):
- <strong>Images</strong>: <code>abstractcore[media]</code> + either a vision-capable model (VLM/VL) <strong>or</strong> configured vision fallback (<code>abstractcore --set-vision-provider PROVIDER MODEL</code>).
- <strong>Video</strong>: <code>ffmpeg</code>/<code>ffprobe</code> on <code>PATH</code> + either a vision-capable model <strong>or</strong> configured vision fallback (for frame sampling). Native video input is model/provider dependent.
- <strong>Audio</strong>: either an audio-capable model <strong>or</strong> speech-to-text fallback via <code>abstractvoice</code> + <code>audio_policy="auto"</code>/<code>"speech_to_text"</code>.</p>
<p>Defaults can be configured via the config CLI (<code>abstractcore --config</code>, <code>abstractcore --status</code>). See <a href="centralized-config.html">Centralized Config</a>.</p>
<p>If your main model is text-only, you can configure vision fallback (two-stage captioning) so images are automatically described and injected as short observations. See <a href="media-handling-system.html">Media Handling</a>, <a href="vision-capabilities.html">Vision Capabilities</a>, and <a href="centralized-config.html">Centralized Config</a>.</p>
<p>For long documents, AbstractCore can optionally apply Glyph visual-text compression. Install <code>pip install "abstractcore[compression]"</code> (and <code>pip install "abstractcore[media]"</code> for PDFs) and see <a href="glyphs.html">Glyph Visual-Text Compression</a>.</p>
<h2 id="async">Async</h2>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    resp = await llm.agenerate(&quot;Give me 3 bullet points about HTTP caching.&quot;)
    print(resp.content)

asyncio.run(main())
</code></pre></div>
<h2 id="cli-optional">CLI (optional)</h2>
<div class="code-block"><pre><code class="language-bash"># Configure defaults and API keys
abstractcore --config
abstractcore --status

# Interactive chat
abstractcore-chat --provider openai --model gpt-4o-mini
</code></pre></div>
<h2 id="next-steps">Next steps</h2>
<ul>
<li><a href="prerequisites.html">Prerequisites</a> — provider setup (keys, base URLs, hardware notes)</li>
<li><a href="faq.html">FAQ</a> — common questions and setup gotchas</li>
<li><a href="examples.html">Examples</a> — end-to-end patterns and recipes</li>
<li><a href="api.html">API (Python)</a> — public API map and common patterns</li>
<li><a href="api-reference.html">API Reference</a> — complete function/class listing</li>
<li><a href="troubleshooting.html">Troubleshooting</a> — common errors and fixes</li>
<li><a href="server.html">Server</a> — OpenAI-compatible HTTP gateway</li>
<li><a href="endpoint.html">Endpoint</a> — single-model OpenAI-compatible endpoint (one provider/model per worker)</li>
</ul>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
