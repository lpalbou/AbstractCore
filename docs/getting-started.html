<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Getting Started - AbstractCore</title>
    <meta name="description" content="Get started with AbstractCore in 5 minutes. Learn how to install and make your first LLM call.">
    <link rel="icon" type="image/x-icon" href="../assets/favicon.ico">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    
    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [
                    { text: 'Features', href: '/docs/capabilities.html' },
                    { text: 'Quick Start', href: '/docs/getting-started.html' },
                    { text: 'Documentation', href: '/#docs' },
                    { text: 'Examples', href: '/docs/examples.html' },
                    {
                        text: 'GitHub',
                        href: 'https://github.com/lpalbou/AbstractCore',
                        target: '_blank',
                        icon: 'github'
                    },
                    {
                        text: 'PyPI',
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 800px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Getting Started with AbstractCore</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    This guide will get you up and running with AbstractCore in 5 minutes. You'll learn how to install it, make your first LLM call, and explore the key features.
                </p>
            </div>

            <div class="doc-content">
                <h2>Prerequisites</h2>
                <ul>
                    <li>Python 3.9 or higher</li>
                    <li>pip package manager</li>
                    <li>(Optional) API keys for cloud providers</li>
                </ul>

                <h2>Installation</h2>

                <h3>Basic Installation</h3>
                <div class="code-block">
                    <pre><code class="language-bash">pip install abstractcore

# For media handling (images, PDFs, Office docs)
pip install abstractcore[media]</code></pre>
                </div>

                <h3>Provider-Specific Installation</h3>
                <p>Choose based on which LLM providers you want to use:</p>
                
                <div class="code-block">
                    <pre><code class="language-bash"># OpenAI
pip install abstractcore[openai]

# Anthropic
pip install abstractcore[anthropic]

# Ollama (local models)
pip install abstractcore[ollama]

# LMStudio (local models)
pip install abstractcore[lmstudio]

# MLX (Apple Silicon)
pip install abstractcore[mlx]

# HuggingFace
pip install abstractcore[huggingface]

# Server support
pip install abstractcore[server]

# Embeddings
pip install abstractcore[embeddings]

# Everything
pip install abstractcore[all]</code></pre>
                </div>

                <h3>Quick Setup Examples</h3>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
                    <div style="background: var(--bg-secondary); padding: 1.5rem; border-radius: 0.75rem;">
                        <h4 style="margin: 0 0 1rem 0; color: var(--accent-color);">Cloud Provider</h4>
                        <div class="code-block">
                            <pre><code class="language-bash"># OpenAI setup
pip install abstractcore[openai]
export OPENAI_API_KEY="your-key-here"</code></pre>
                        </div>
                    </div>
                    <div style="background: var(--bg-secondary); padding: 1.5rem; border-radius: 0.75rem;">
                        <h4 style="margin: 0 0 1rem 0; color: var(--accent-color);">Local Models</h4>
                        <div class="code-block">
                            <pre><code class="language-bash"># Ollama setup
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b
pip install abstractcore[ollama]</code></pre>
                        </div>
                    </div>
                </div>

                <h2>Your First Program</h2>
                <p>Create a file called <code>first_llm.py</code>:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

# Choose your provider (uncomment one):
llm = create_llm("openai", model="gpt-4o-mini", temperature=0.7, seed=42)        # Cloud
# llm = create_llm("anthropic", model="claude-3-5-haiku-latest", temperature=0.0)  # Cloud (use temp=0 for consistency)
# llm = create_llm("ollama", model="llama3.2:3b", seed=42)   # Local

# Generate your first response
response = llm.generate("What is the capital of France?")
print(response.content)

# For deterministic outputs across calls
deterministic_llm = create_llm("openai", model="gpt-4o-mini", temperature=0.0, seed=42)
response1 = deterministic_llm.generate("Write exactly 3 words about AI")
response2 = deterministic_llm.generate("Write exactly 3 words about AI")
# response1.content == response2.content (identical outputs)</code></pre>
                </div>

                <p>Run it:</p>
                <div class="code-block">
                    <pre><code class="language-bash">python first_llm.py
# Output: The capital of France is Paris.</code></pre>
                </div>

                <div style="background: linear-gradient(135deg, #10b981, #3b82f6); padding: 1.5rem; border-radius: 0.75rem; margin: 2rem 0; color: white;">
                    <h3 style="margin: 0 0 0.5rem 0; color: white;">‚úì Complete</h3>
                    <p style="margin: 0; color: rgba(255,255,255,0.9);">Your first AbstractCore LLM call is working.</p>
                </div>

                <h2>Recommended: Centralized Configuration</h2>
                <p>Configure AbstractCore once to avoid specifying provider/model every time:</p>

                <div class="code-block">
                    <pre><code class="language-bash"># Check current status
abstractcore --status

# Set global default
abstractcore --set-global-default ollama/llama3:8b

# Set app-specific defaults
abstractcore --set-app-default cli ollama llama3.2:3b

# Configure API keys (stored in config file)
abstractcore --set-api-key openai sk-your-key-here</code></pre>
                </div>

                <p><strong>Learn more:</strong> <a href="centralized-config.html">Centralized Configuration Guide</a></p>

                <h2>Core Concepts (5-Minute Tour)</h2>

                <h3>1. Provider Discovery (Centralized Registry)</h3>
                <p>AbstractCore provides a centralized registry to discover all available providers and models:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore.providers import get_all_providers_with_models

# Get comprehensive information about all providers with available models
providers = get_all_providers_with_models()
for provider in providers:
    print(f"{provider['display_name']}: {provider['model_count']} models")
    print(f"Features: {', '.join(provider['supported_features'])}")
    print(f"Local: {provider['local_provider']}")
    print(f"Auth Required: {provider['authentication_required']}")
    print()</code></pre>
                </div>

                <h3>2. Providers and Models</h3>
                <p>AbstractCore supports multiple providers with the same interface:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

# Same interface, different providers - all support deterministic generation
openai_llm = create_llm("openai", model="gpt-4o-mini", seed=42)
claude_llm = create_llm("anthropic", model="claude-3-5-haiku-latest", temperature=0.0)  # Use temp=0 for consistency
local_llm = create_llm("ollama", model="llama3.2:3b", seed=42)

question = "Explain Python list comprehensions"

# All work the same way
for name, llm in [("OpenAI", openai_llm), ("Claude", claude_llm), ("Ollama", local_llm)]:
    response = llm.generate(question)
    print(f"{name}: {response.content[:50]}...")</code></pre>
                </div>

                <h3>3. Structured Output (Game Changer)</h3>
                <p>Instead of parsing strings, get typed objects directly:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from pydantic import BaseModel
from abstractcore import create_llm

class MovieReview(BaseModel):
    title: str
    rating: int  # 1-5 stars
    summary: str

llm = create_llm("openai", model="gpt-4o-mini")

# Get structured data automatically
review = llm.generate(
    "Review the movie Inception",
    response_model=MovieReview
)

print(f"Title: {review.title}")
print(f"Rating: {review.rating}/5")
print(f"Summary: {review.summary}")</code></pre>
                </div>

                <p><strong>No more string parsing!</strong> AbstractCore handles JSON validation and retries automatically.</p>

                <h3>4. Tool Calling (LLM with Superpowers)</h3>
                <p>Let your LLM call functions with the <code>@tool</code> decorator:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm, tool

@tool
def get_weather(city: str) -> str:
    """Get current weather for a specified city."""
    # In a real scenario, you'd call an actual weather API
    return f"The weather in {city} is sunny, 72¬∞F"

@tool
def calculate(expression: str) -> float:
    """Perform a mathematical calculation."""
    try:
        result = eval(expression)  # Simplified for demo - don't use eval in production!
        return result
    except Exception:
        return float('nan')

# Instantiate the LLM
llm = create_llm("openai", model="gpt-4o-mini")

# Automatically extract tool definitions from decorated functions
response = llm.generate(
    "What's the weather in Tokyo and what's 15 * 23?",
    tools=[get_weather, calculate]  # Pass tool functions directly
)

print(response.content)
# Output: The weather in Tokyo is sunny, 72¬∞F and 15 * 23 = 345.</code></pre>
                </div>

                <h3>5. Streaming (Real-Time Responses)</h3>
                <p>Show responses as they're generated:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o-mini")

print("AI: ", end="", flush=True)
for chunk in llm.generate("Write a haiku about programming", stream=True):
    print(chunk.content, end="", flush=True)
print("\n")
# Output appears word by word in real-time</code></pre>
                </div>

                <h3>6. Conversations with Memory</h3>
                <p>Maintain context across multiple turns:</p>
                
                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm, BasicSession

llm = create_llm("openai", model="gpt-4o-mini")
session = BasicSession(provider=llm, system_prompt="You are a helpful coding tutor.")

# First exchange
response1 = session.generate("My name is Alex and I'm learning Python.")
print("AI:", response1.content)

# Second exchange - remembers context
response2 = session.generate("What's my name and what am I learning?")
print("AI:", response2.content)
# Output: Your name is Alex and you're learning Python.</code></pre>
                </div>

                <h3>7. Media Handling (Attach Any File)</h3>
                <p>Attach images, PDFs, Office docs with simple syntax:</p>

                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o")

# Attach any file type with media parameter
response = llm.generate(
    "What's in this image and document?",
    media=["photo.jpg", "report.pdf"]
)

# Or from CLI with @filename syntax
# python -m abstractcore.utils.cli --prompt "Analyze @report.pdf"</code></pre>
                </div>

                <p><strong>Supported:</strong> Images (PNG, JPEG, GIF, WEBP), Documents (PDF, DOCX, XLSX, PPTX), Data (CSV, TSV, TXT, MD, JSON)</p>
                <p><strong>Learn more:</strong> <a href="media-handling-system.html">Media Handling System Guide</a></p>

                <h3>8. Vision Capabilities (Image Analysis)</h3>
                <p>Analyze images across all providers with the same code:</p>

                <div class="code-block">
                    <pre><code class="language-python">from abstractcore import create_llm

# Works with any vision-capable provider
llm = create_llm("openai", model="gpt-4o")

response = llm.generate(
    "What objects do you see in this image?",
    media=["photo.jpg"]
)

# Same code works with local models
llm = create_llm("ollama", model="qwen2.5vl:7b")
response = llm.generate("Describe this image", media=["scene.jpg"])</code></pre>
                </div>

                <p><strong>Vision Fallback:</strong> Text-only models can process images through transparent captioning. Configure once: <code>abstractcore --download-vision-model</code></p>
                <p><strong>Learn more:</strong> <a href="vision-capabilities.html">Vision Capabilities Guide</a></p>

                <h2>Next Steps</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">Centralized Configuration</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">Set up once, use everywhere</p>
                        <a href="centralized-config.html" style="color: var(--primary-color); font-weight: 500;">Configure AbstractCore ‚Üí</a>
                    </div>

                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">Media Handling</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">Universal file attachment</p>
                        <a href="media-handling-system.html" style="color: var(--primary-color); font-weight: 500;">Learn Media Handling ‚Üí</a>
                    </div>

                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">Vision Capabilities</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">Image analysis everywhere</p>
                        <a href="vision-capabilities.html" style="color: var(--primary-color); font-weight: 500;">Explore Vision ‚Üí</a>
                    </div>

                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">üìñ API Reference</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">Complete Python API documentation</p>
                        <a href="api-reference.html" style="color: var(--primary-color); font-weight: 500;">View API Reference ‚Üí</a>
                    </div>

                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">üíª Examples</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">Real-world code examples</p>
                        <a href="examples.html" style="color: var(--primary-color); font-weight: 500;">View Examples ‚Üí</a>
                    </div>

                    <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                        <h4 style="margin: 0 0 0.5rem 0;">üåê Server Guide</h4>
                        <p style="margin: 0 0 1rem 0; color: var(--text-secondary); font-size: 0.875rem;">HTTP API server setup</p>
                        <a href="server.html" style="color: var(--primary-color); font-weight: 500;">View Server Guide ‚Üí</a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
