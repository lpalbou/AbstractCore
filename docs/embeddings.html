<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vector Embeddings - AbstractCore</title>
    <meta name="description" content="Vector embeddings for semantic search and RAG across HuggingFace, Ollama, and LMStudio.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [
                    { text: 'Features', href: '/docs/capabilities.html' },
                    { text: 'Quick Start', href: '/docs/getting-started.html' },
                    { text: 'Documentation', href: '/#docs' },
                    { text: 'Examples', href: '/docs/examples.html' },
                    {
                        text: 'GitHub',
                        href: 'https://github.com/lpalbou/abstractcore',
                        target: '_blank',
                        icon: 'github'
                    },
                    {
                        text: 'PyPI',
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Vector Embeddings</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Generate high-quality embeddings for semantic search, clustering, and RAG with a unified API.
                </p>
            </div>

            <div style="background: linear-gradient(135deg, var(--primary-color), var(--secondary-color)); padding: 2rem; border-radius: 0.75rem; color: white; margin-bottom: 3rem;">
                <h2 style="margin: 0 0 1rem 0;">What you get</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.25rem;">
                    <div style="background: rgba(255, 255, 255, 0.12); padding: 1.25rem; border-radius: 0.75rem;">
                        <h3 style="margin: 0 0 0.5rem 0;">Multi-provider</h3>
                        <p style="margin: 0; opacity: 0.95;">Use HuggingFace (local), Ollama, or LMStudio with the same code.</p>
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.12); padding: 1.25rem; border-radius: 0.75rem;">
                        <h3 style="margin: 0 0 0.5rem 0;">Simple API</h3>
                        <p style="margin: 0; opacity: 0.95;"><code>embed()</code>, <code>embed_batch()</code>, and similarity helpers.</p>
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.12); padding: 1.25rem; border-radius: 0.75rem;">
                        <h3 style="margin: 0 0 0.5rem 0;">RAG-ready</h3>
                        <p style="margin: 0; opacity: 0.95;">Build semantic search + context injection patterns in minutes.</p>
                    </div>
                </div>
            </div>

            <!-- Table of Contents -->
            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;">
                <h2 style="margin: 0 0 1rem 0;">Table of Contents</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 0.75rem;">
                    <div>
                        <a href="#quick-start" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Start</a>
                        <a href="#providers" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Providers & Models</a>
                        <a href="#core-api" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core API</a>
                    </div>
                    <div>
                        <a href="#semantic-search" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Semantic Search</a>
                        <a href="#rag" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Simple RAG Pipeline</a>
                        <a href="#server-api" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Server /v1/embeddings</a>
                    </div>
                    <div>
                        <a href="#performance" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Performance Tips</a>
                    </div>
                </div>
            </div>

            <div class="doc-content">
                <section id="quick-start">
                    <h2>Quick Start</h2>
                    <p>Install embeddings support:</p>

                    <div class="code-block">
                        <pre><code class="language-bash">pip install "abstractcore[embeddings]"</code></pre>
                    </div>

                    <p>Generate your first embeddings:</p>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

# Option 1: HuggingFace (default) - local models
embedder = EmbeddingManager()  # all-MiniLM-L6-v2 by default

# Option 2: Ollama - local models via Ollama API
# embedder = EmbeddingManager(provider="ollama", model="granite-embedding:278m")

# Option 3: LMStudio - local models via LMStudio API
# embedder = EmbeddingManager(provider="lmstudio", model="text-embedding-all-minilm-l6-v2")

embedding = embedder.embed("Machine learning transforms how we process information")
print(f"Embedding dimension: {len(embedding)}")

similarity = embedder.compute_similarity("artificial intelligence", "machine learning")
print(f"Similarity: {similarity:.3f}")</code></pre>
                    </div>
                </section>

                <section id="providers" style="margin-top: 3rem;">
                    <h2>Providers & Models</h2>
                    <p>AbstractCore supports multiple embedding providers. Pick what matches your environment:</p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: var(--background-secondary); padding: 1.25rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                            <h3 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">HuggingFace (default)</h3>
                            <p style="margin: 0;">Local sentence-transformers models. Great default for development and production.</p>
                        </div>
                        <div style="background: var(--background-secondary); padding: 1.25rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                            <h3 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Ollama</h3>
                            <p style="margin: 0;">Local embeddings via the Ollama API (requires Ollama running).</p>
                        </div>
                        <div style="background: var(--background-secondary); padding: 1.25rem; border-radius: 0.75rem; border: 1px solid var(--border-light);">
                            <h3 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">LMStudio</h3>
                            <p style="margin: 0;">Local embeddings via LM Studio (GUI-friendly model management).</p>
                        </div>
                    </div>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

# HuggingFace (default)
embedder = EmbeddingManager()

# Ollama
embedder = EmbeddingManager(provider="ollama", model="granite-embedding:278m")

# LMStudio
embedder = EmbeddingManager(provider="lmstudio", model="text-embedding-all-minilm-l6-v2")</code></pre>
                    </div>
                </section>

                <section id="core-api" style="margin-top: 3rem;">
                    <h2>Core API</h2>
                    <p>Key methods you’ll use most often:</p>
                    <ul>
                        <li><code>embed(text)</code> → a single embedding vector</li>
                        <li><code>embed_batch(texts)</code> → batch embeddings (faster)</li>
                        <li><code>compute_similarity(a, b)</code> → cosine similarity for two strings</li>
                        <li><code>compute_similarities(query, docs)</code> → one-to-many similarity</li>
                        <li><code>compute_similarities_matrix(a, b=None)</code> → similarity matrix</li>
                    </ul>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager()

texts = [
    "Python programming language",
    "JavaScript for web development",
    "Machine learning with Python",
]

embeddings = embedder.embed_batch(texts)
print(f"Generated {len(embeddings)} embeddings")

query = "learn python"
scores = embedder.compute_similarities(query, texts)
print([round(s, 3) for s in scores])</code></pre>
                    </div>
                </section>

                <section id="semantic-search" style="margin-top: 3rem;">
                    <h2>Semantic Search</h2>
                    <p>A minimal semantic search loop:</p>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager()

documents = [
    "Python is strong for data science and machine learning applications",
    "JavaScript enables interactive web pages and modern frontend development",
    "SQL databases store and query structured data efficiently",
]

def semantic_search(query: str, docs: list[str], top_k: int = 2):
    scored = [(i, embedder.compute_similarity(query, doc), doc) for i, doc in enumerate(docs)]
    scored.sort(key=lambda t: t[1], reverse=True)
    return scored[:top_k]

for i, score, doc in semantic_search("web development frameworks", documents):
    print(f"{score:.3f} → {doc}")</code></pre>
                    </div>
                </section>

                <section id="rag" style="margin-top: 3rem;">
                    <h2>Simple RAG Pipeline</h2>
                    <p>Retrieve relevant context with embeddings, then pass it to any LLM provider:</p>

                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm
from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager()
llm = create_llm("openai", model="gpt-4o-mini")

knowledge_base = [
    "The Eiffel Tower is 330 meters tall and was completed in 1889.",
    "Paris is the capital city of France with over 2 million inhabitants.",
    "The Louvre Museum in Paris houses the Mona Lisa painting.",
]

def rag_answer(question: str) -> str:
    scored = sorted(
        ((embedder.compute_similarity(question, doc), doc) for doc in knowledge_base),
        reverse=True,
    )
    context = "\n".join([doc for _, doc in scored[:2]])
    prompt = f"""Context:
{context}

Question: {question}
Answer:"""
    return llm.generate(prompt).content

print(rag_answer("How tall is the Eiffel Tower?"))</code></pre>
                    </div>
                </section>

                <section id="server-api" style="margin-top: 3rem;">
                    <h2>Server: OpenAI-compatible <code>/v1/embeddings</code></h2>
                    <p>If you run the AbstractCore HTTP server, you can also generate embeddings over REST.</p>

                    <div class="code-block">
                        <pre><code class="language-bash">pip install "abstractcore[server]"
python -m abstractcore.server.app --port 8000</code></pre>
                    </div>

                    <div class="code-block">
                        <pre><code class="language-bash">curl -X POST http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": ["text 1", "text 2", "text 3"],
    "model": "ollama/granite-embedding:278m"
  }'</code></pre>
                    </div>

                    <div style="background: var(--background-tertiary); padding: 1.25rem; border-radius: 0.75rem; margin: 1rem 0;">
                        <p style="margin: 0;"><strong>Tip:</strong> See the <a href="server.html">Server guide</a> for model discovery (<code>/v1/models</code>) and provider status (<code>/providers</code>).</p>
                    </div>
                </section>

                <section id="performance" style="margin-top: 3rem;">
                    <h2>Performance Tips</h2>
                    <ul>
                        <li>Prefer <code>embed_batch()</code> when embedding many texts.</li>
                        <li>Cache embeddings for your knowledge base; only re-embed changed documents.</li>
                        <li>For local providers, keep the embedding model “warm” (avoid frequent reloads).</li>
                    </ul>
                </section>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Related Documentation</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="getting-started.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Getting Started</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">First LLM call + core concepts</p>
                        </a>

                        <a href="api-reference.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">API Reference</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">EmbeddingManager API</p>
                        </a>

                        <a href="examples.html#rag-embeddings" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Examples</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">RAG example and recipes</p>
                        </a>

                        <a href="server.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">HTTP Server</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Expose embeddings via OpenAI-compatible endpoints</p>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
