<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vector Embeddings Guide - AbstractCore</title>
    <meta name="description" content="AbstractCore includes built-in support for vector embeddings with multiple providers (HuggingFace, Ollama, LMStudio). This guide shows you how to use…">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Vector Embeddings Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">AbstractCore includes built-in support for vector embeddings with multiple providers (HuggingFace, Ollama, LMStudio). This guide shows you how to use embeddings for semantic search, RAG applications, and similarity analysis.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#quick-start" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Quick Start</a>
<a href="#available-providers-models" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Available Providers &amp; Models</a>
<a href="#core-features" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Features</a>
<a href="#practical-applications" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Practical Applications</a>
<a href="#performance-optimization" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Performance Optimization</a>
<a href="#integration-with-llm-providers" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Integration with LLM Providers</a>
<a href="#production-considerations" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Production Considerations</a>
<a href="#when-to-use-embeddings" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">When to Use Embeddings</a>
<a href="#using-embeddings-via-rest-api" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Using Embeddings via REST API</a>
<a href="#provider-specific-features" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Provider-Specific Features</a>
<a href="#next-steps" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Next Steps</a>
<a href="#related-documentation" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Related Documentation</a></div>

            <div class="doc-content">

<p><strong>Two ways to use embeddings:</strong>
1. <strong>Python Library</strong> (this guide) - Direct programmatic usage via <code>EmbeddingManager</code>
2. <strong>REST API</strong> - HTTP endpoints via AbstractCore server (see <a href="server.html#embeddings">Server API Reference</a>)</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="installation">Installation</h3>
<div class="code-block"><pre><code class="language-bash"># Install with embeddings support
pip install &quot;abstractcore[embeddings]&quot;
</code></pre></div>
<h3 id="first-embeddings">First Embeddings</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

# Option 1: HuggingFace (default) - Local models with optional ONNX acceleration
embedder = EmbeddingManager()  # Uses all-MiniLM-L6-v2 by default

# Option 2: Ollama - Local models via Ollama API
embedder = EmbeddingManager(
    provider=&quot;ollama&quot;,
    model=&quot;granite-embedding:278m&quot;
)

# Option 3: LMStudio - Local models via LMStudio API
embedder = EmbeddingManager(
    provider=&quot;lmstudio&quot;,
    model=&quot;text-embedding-all-minilm-l6-v2&quot;
)

# Generate embedding for a single text (works with all providers)
embedding = embedder.embed(&quot;Machine learning transforms how we process information&quot;)
print(f&quot;Embedding dimension: {len(embedding)}&quot;)  # 384 for MiniLM

# Compute similarity between texts (works with all providers)
similarity = embedder.compute_similarity(
    &quot;artificial intelligence&quot;,
    &quot;machine learning&quot;
)
print(f&quot;Similarity: {similarity:.3f}&quot;)  # 0.847
</code></pre></div>
<h2 id="available-providers-models">Available Providers &amp; Models</h2>
<p>AbstractCore supports multiple embedding providers:</p>
<h3 id="huggingface-provider-default">HuggingFace Provider (Default)</h3>
<p>Local sentence-transformers models with optional ONNX acceleration (when available).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Dimensions</th>
<th>Languages</th>
<th>Primary Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>all-minilm</strong> (default)</td>
<td>90M</td>
<td>384</td>
<td>English</td>
<td>Fast local development, testing</td>
</tr>
<tr>
<td><strong>qwen3-embedding</strong></td>
<td>1.5B</td>
<td>1536</td>
<td>100+</td>
<td>Qwen-based multilingual, instruction-tuned</td>
</tr>
<tr>
<td><strong>embeddinggemma</strong></td>
<td>300M</td>
<td>768</td>
<td>100+</td>
<td>General purpose, multilingual</td>
</tr>
<tr>
<td><strong>granite</strong></td>
<td>278M</td>
<td>768</td>
<td>100+</td>
<td>Enterprise applications</td>
</tr>
</tbody>
</table>
<div class="code-block"><pre><code class="language-python"># Default: all-MiniLM-L6-v2 (fast and lightweight)
embedder = EmbeddingManager()

# Qwen-based embedding model for multilingual support
embedder = EmbeddingManager(model=&quot;qwen3-embedding&quot;)

# Google's EmbeddingGemma for multilingual support
embedder = EmbeddingManager(model=&quot;embeddinggemma&quot;)

# Direct HuggingFace model ID
embedder = EmbeddingManager(model=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
</code></pre></div>
<h3 id="ollama-provider">Ollama Provider</h3>
<p>Local embedding models via Ollama API. Requires Ollama running locally.</p>
<div class="code-block"><pre><code class="language-python"># Setup: Install Ollama and pull an embedding model
# ollama pull granite-embedding:278m

# Use Ollama embeddings
embedder = EmbeddingManager(
    provider=&quot;ollama&quot;,
    model=&quot;granite-embedding:278m&quot;
)

# Other popular Ollama embedding models:
# - nomic-embed-text (274MB)
# - granite-embedding:107m (smaller, faster)
</code></pre></div>
<h3 id="lmstudio-provider">LMStudio Provider</h3>
<p>Local embedding models via LMStudio API. Requires LMStudio running with a loaded model.</p>
<div class="code-block"><pre><code class="language-python"># Setup: Start LMStudio and load an embedding model

# Use LMStudio embeddings
embedder = EmbeddingManager(
    provider=&quot;lmstudio&quot;,
    model=&quot;text-embedding-all-minilm-l6-v2&quot;
)
</code></pre></div>
<h3 id="provider-comparison">Provider Comparison</h3>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Speed</th>
<th>Setup</th>
<th>Privacy</th>
<th>Cost</th>
<th>Primary Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>HuggingFace</strong></td>
<td>Fast</td>
<td>Easy</td>
<td>Full</td>
<td>Free</td>
<td>Development, production</td>
</tr>
<tr>
<td><strong>Ollama</strong></td>
<td>Medium</td>
<td>Medium</td>
<td>Full</td>
<td>Free</td>
<td>Privacy, custom models</td>
</tr>
<tr>
<td><strong>LMStudio</strong></td>
<td>Medium</td>
<td>Easy (GUI)</td>
<td>Full</td>
<td>Free</td>
<td>GUI management, testing</td>
</tr>
</tbody>
</table>
<h2 id="core-features">Core Features</h2>
<h3 id="single-text-embeddings">Single Text Embeddings</h3>
<div class="code-block"><pre><code class="language-python">embedder = EmbeddingManager()

text = &quot;Python is a versatile programming language&quot;
embedding = embedder.embed(text)

print(f&quot;Text: {text}&quot;)
print(f&quot;Embedding: {len(embedding)} dimensions&quot;)
print(f&quot;First 5 values: {embedding[:5]}&quot;)
</code></pre></div>
<h3 id="batch-processing-more-efficient">Batch Processing (More Efficient)</h3>
<div class="code-block"><pre><code class="language-python">texts = [
    &quot;Python programming language&quot;,
    &quot;JavaScript for web development&quot;,
    &quot;Machine learning with Python&quot;,
    &quot;Data science and analytics&quot;
]

# Process multiple texts at once (much faster)
embeddings = embedder.embed_batch(texts)

print(f&quot;Generated {len(embeddings)} embeddings&quot;)
for i, embedding in enumerate(embeddings):
    print(f&quot;Text {i+1}: {len(embedding)} dimensions&quot;)
</code></pre></div>
<h3 id="similarity-analysis">Similarity Analysis</h3>
<div class="code-block"><pre><code class="language-python"># Basic similarity between two texts
similarity = embedder.compute_similarity(&quot;cat&quot;, &quot;kitten&quot;)
print(f&quot;Similarity: {similarity:.3f}&quot;)  # 0.804

# NEW: Batch similarity - compare one text against many
query = &quot;Python programming&quot;
docs = [&quot;Learn Python basics&quot;, &quot;JavaScript guide&quot;, &quot;Cooking recipes&quot;, &quot;Data science with Python&quot;]
similarities = embedder.compute_similarities(query, docs)
print(f&quot;Batch similarities: {[f'{s:.3f}' for s in similarities]}&quot;)
# Output: ['0.785', '0.155', '0.145', '0.580']

# NEW: Similarity matrix - compare all texts against all texts
texts = [&quot;Python programming&quot;, &quot;JavaScript development&quot;, &quot;Python data science&quot;, &quot;Web frameworks&quot;]
matrix = embedder.compute_similarities_matrix(texts)
print(f&quot;Matrix shape: {matrix.shape}&quot;)  # (4, 4) symmetric matrix

# NEW: Asymmetric matrix for query-document matching
queries = [&quot;Learn Python&quot;, &quot;Web development guide&quot;]
knowledge_base = [&quot;Python tutorial&quot;, &quot;JavaScript guide&quot;, &quot;React framework&quot;, &quot;Python for beginners&quot;]
search_matrix = embedder.compute_similarities_matrix(queries, knowledge_base)
print(f&quot;Search matrix: {search_matrix.shape}&quot;)  # (2, 4) - 2 queries × 4 documents
</code></pre></div>
<h2 id="practical-applications">Practical Applications</h2>
<h3 id="semantic-search">Semantic Search</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager()

# Document collection
documents = [
    &quot;Python is strong for data science and machine learning applications&quot;,
    &quot;JavaScript enables interactive web pages and modern frontend development&quot;,
    &quot;React is a popular library for building user interfaces with JavaScript&quot;,
    &quot;SQL databases store and query structured data efficiently&quot;,
    &quot;Machine learning algorithms can predict patterns from historical data&quot;
]

def semantic_search(query, documents, top_k=3):
    &quot;&quot;&quot;Find most relevant documents for a query.&quot;&quot;&quot;
    similarities = []

    for i, doc in enumerate(documents):
        similarity = embedder.compute_similarity(query, doc)
        similarities.append((i, similarity, doc))

    # Sort by similarity (highest first)
    similarities.sort(key=lambda x: x[1], reverse=True)

    return similarities[:top_k]

# Search for relevant documents
query = &quot;web development frameworks&quot;
results = semantic_search(query, documents)

print(f&quot;Query: {query}\n&quot;)
for rank, (idx, similarity, doc) in enumerate(results, 1):
    print(f&quot;{rank}. Score: {similarity:.3f}&quot;)
    print(f&quot;   {doc}\n&quot;)
</code></pre></div>
<h3 id="simple-rag-pipeline">Simple RAG Pipeline</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.embeddings import EmbeddingManager

# Setup
embedder = EmbeddingManager()
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# Knowledge base
knowledge_base = [
    &quot;The Eiffel Tower is 330 meters tall and was completed in 1889.&quot;,
    &quot;Paris is the capital city of France with over 2 million inhabitants.&quot;,
    &quot;The Louvre Museum in Paris houses the famous Mona Lisa painting.&quot;,
    &quot;French cuisine is known for its wine, cheese, and pastries.&quot;,
    &quot;The Seine River flows through central Paris.&quot;
]

def rag_query(question, knowledge_base, llm, embedder):
    &quot;&quot;&quot;Answer question using relevant context from knowledge base.&quot;&quot;&quot;

    # Step 1: Find most relevant context
    similarities = []
    for doc in knowledge_base:
        similarity = embedder.compute_similarity(question, doc)
        similarities.append((similarity, doc))

    # Get top 2 most relevant documents
    similarities.sort(reverse=True)
    top_contexts = [doc for _, doc in similarities[:2]]
    context = &quot;\n&quot;.join(top_contexts)

    # Step 2: Generate answer using context
    prompt = f&quot;&quot;&quot;Context:
{context}

Question: {question}

Based on the context above, please answer the question:&quot;&quot;&quot;

    response = llm.generate(prompt)
    return response.content, top_contexts

# Usage
question = &quot;How tall is the Eiffel Tower?&quot;
answer, contexts = rag_query(question, knowledge_base, llm, embedder)

print(f&quot;Question: {question}&quot;)
print(f&quot;Answer: {answer}&quot;)
print(f&quot;\nUsed context:&quot;)
for ctx in contexts:
    print(f&quot;- {ctx}&quot;)
</code></pre></div>
<h3 id="document-clustering-new">Document Clustering (NEW)</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager()

# Documents to cluster
documents = [
    &quot;Python programming tutorial for beginners&quot;,
    &quot;Introduction to machine learning concepts&quot;,
    &quot;JavaScript web development guide&quot;,
    &quot;Advanced Python data structures&quot;,
    &quot;Machine learning with neural networks&quot;,
    &quot;Building web apps with JavaScript&quot;,
    &quot;Python for data analysis&quot;,
    &quot;Deep learning fundamentals&quot;,
    &quot;React.js frontend development&quot;,
    &quot;Statistical analysis with Python&quot;
]

# NEW: Automatic semantic clustering
clusters = embedder.find_similar_clusters(
    documents,
    threshold=0.6,      # 60% similarity required
    min_cluster_size=2  # At least 2 documents per cluster
)

print(f&quot;Found {len(clusters)} clusters:&quot;)
for i, cluster in enumerate(clusters):
    print(f&quot;\nCluster {i+1} ({len(cluster)} documents):&quot;)
    for idx in cluster:
        print(f&quot;  - {documents[idx]}&quot;)

# Example output:
# Cluster 1 (4 documents): Python-related content
# Cluster 2 (2 documents): JavaScript-related content
# Cluster 3 (2 documents): Machine learning content
</code></pre></div>
<h2 id="performance-optimization">Performance Optimization</h2>
<h3 id="onnx-backend-optional">ONNX Backend (optional)</h3>
<div class="code-block"><pre><code class="language-python"># Enable ONNX for faster inference
embedder = EmbeddingManager(
    model=&quot;embeddinggemma&quot;,
    backend=&quot;onnx&quot;  # optional
)

# Performance comparison
import time

texts = [&quot;Sample text for performance testing&quot;] * 100

# Time the embedding generation
start_time = time.time()
embeddings = embedder.embed_batch(texts)
duration = time.time() - start_time

print(f&quot;Generated {len(embeddings)} embeddings in {duration:.2f} seconds&quot;)
print(f&quot;Speed: {len(embeddings)/duration:.1f} embeddings/second&quot;)
</code></pre></div>
<h3 id="dimension-truncation-memoryspeed-trade-off">Dimension Truncation (Memory/Speed Trade-off)</h3>
<div class="code-block"><pre><code class="language-python"># Truncate embeddings for faster processing
embedder = EmbeddingManager(
    model=&quot;embeddinggemma&quot;,
    output_dims=256  # Reduce from 768 to 256 dimensions
)

embedding = embedder.embed(&quot;Test text&quot;)
print(f&quot;Truncated embedding dimension: {len(embedding)}&quot;)  # 256
</code></pre></div>
<h3 id="advanced-caching-new">Advanced Caching (NEW)</h3>
<div class="code-block"><pre><code class="language-python"># Configure dual-layer caching system
embedder = EmbeddingManager(
    cache_size=5000,  # Larger memory cache
    cache_dir=&quot;./embeddings_cache&quot;  # Persistent disk cache
)

# Regular embedding with standard caching
embedding1 = embedder.embed(&quot;Machine learning text&quot;)

# NEW: Normalized embedding with dedicated cache (unit-length vectors for cosine similarity)
normalized = embedder.embed_normalized(&quot;Machine learning text&quot;)
print(f&quot;Normalized embedding length: {sum(x*x for x in normalized)**0.5:.3f}&quot;)  # 1.0 (unit length)

# Check comprehensive cache stats
stats = embedder.get_cache_stats()
print(f&quot;Regular cache: {stats['persistent_cache_size']} embeddings&quot;)
print(f&quot;Normalized cache: {stats['normalized_cache_size']} embeddings&quot;)
print(f&quot;Memory cache hits: {stats['memory_cache_info']['hits']}&quot;)
</code></pre></div>
<h2 id="integration-with-llm-providers">Integration with LLM Providers</h2>
<h3 id="enhanced-context-selection">Enhanced Context Selection</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.embeddings import EmbeddingManager

def smart_context_selection(query, documents, max_context_length=2000):
    &quot;&quot;&quot;Select most relevant context that fits within token limits.&quot;&quot;&quot;
    embedder = EmbeddingManager()

    # Score all documents
    scored_docs = []
    for doc in documents:
        similarity = embedder.compute_similarity(query, doc)
        scored_docs.append((similarity, doc))

    # Sort by relevance
    scored_docs.sort(reverse=True)

    # Select documents that fit within context limit
    selected_context = &quot;&quot;
    for similarity, doc in scored_docs:
        test_context = selected_context + &quot;\n&quot; + doc
        if len(test_context) &lt;= max_context_length:
            selected_context = test_context
        else:
            break

    return selected_context.strip()

# Usage with LLM
llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

documents = [
    &quot;Long document about machine learning...&quot;,
    &quot;Another document about data science...&quot;,
    # ... many more documents
]

query = &quot;What is supervised learning?&quot;
context = smart_context_selection(query, documents)

response = llm.generate(f&quot;Context: {context}\n\nQuestion: {query}&quot;)
print(response.content)
</code></pre></div>
<h3 id="multi-language-support">Multi-language Support</h3>
<div class="code-block"><pre><code class="language-python"># EmbeddingGemma supports 100+ languages
embedder = EmbeddingManager(model=&quot;embeddinggemma&quot;)

# Cross-language similarity
similarity = embedder.compute_similarity(
    &quot;Hello world&quot;,      # English
    &quot;Bonjour le monde&quot;  # French
)
print(f&quot;Cross-language similarity: {similarity:.3f}&quot;)

# Multilingual semantic search
documents_multilingual = [
    &quot;Machine learning is transforming technology&quot;,  # English
    &quot;L'intelligence artificielle change le monde&quot;,  # French
    &quot;人工智能正在改变世界&quot;,                        # Chinese
    &quot;Künstliche Intelligenz verändert die Welt&quot;    # German
]

query = &quot;artificial intelligence&quot;
for doc in documents_multilingual:
    similarity = embedder.compute_similarity(query, doc)
    print(f&quot;{similarity:.3f}: {doc}&quot;)
</code></pre></div>
<h2 id="production-considerations">Production Considerations</h2>
<h3 id="error-handling">Error Handling</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

def safe_embedding(text, embedder, fallback_value=None):
    &quot;&quot;&quot;Generate embedding with error handling.&quot;&quot;&quot;
    try:
        return embedder.embed(text)
    except Exception as e:
        print(f&quot;Embedding failed for text: {text[:50]}...&quot;)
        print(f&quot;Error: {e}&quot;)
        return fallback_value or [0.0] * 768  # Return zero vector as fallback

embedder = EmbeddingManager()

# Safe embedding generation
text = &quot;Some text that might cause issues&quot;
embedding = safe_embedding(text, embedder)

if embedding:
    print(f&quot;Successfully generated embedding: {len(embedding)} dimensions&quot;)
else:
    print(&quot;Using fallback embedding&quot;)
</code></pre></div>
<h3 id="monitoring-and-metrics">Monitoring and Metrics</h3>
<div class="code-block"><pre><code class="language-python">import time
from abstractcore.embeddings import EmbeddingManager

class MonitoredEmbeddingManager:
    def __init__(self, *args, **kwargs):
        self.embedder = EmbeddingManager(*args, **kwargs)
        self.stats = {
            'total_calls': 0,
            'total_time': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    def embed(self, text):
        start_time = time.time()
        result = self.embedder.embed(text)
        duration = time.time() - start_time

        self.stats['total_calls'] += 1
        self.stats['total_time'] += duration

        return result

    def get_stats(self):
        avg_time = self.stats['total_time'] / max(self.stats['total_calls'], 1)
        return {
            **self.stats,
            'average_time': avg_time,
            'calls_per_second': 1 / avg_time if avg_time &gt; 0 else 0
        }

# Usage
monitored_embedder = MonitoredEmbeddingManager()

# Generate some embeddings
for i in range(10):
    monitored_embedder.embed(f&quot;Test text number {i}&quot;)

# Check performance
stats = monitored_embedder.get_stats()
print(f&quot;Total calls: {stats['total_calls']}&quot;)
print(f&quot;Average time per call: {stats['average_time']:.3f}s&quot;)
print(f&quot;Calls per second: {stats['calls_per_second']:.1f}&quot;)
</code></pre></div>
<h2 id="when-to-use-embeddings">When to Use Embeddings</h2>
<h3 id="good-use-cases">Good Use Cases</h3>
<ul>
<li><strong>Semantic Search</strong>: Find relevant documents based on meaning, not keywords</li>
<li><strong>RAG Applications</strong>: Select relevant context for language model queries</li>
<li><strong>Content Recommendation</strong>: Find similar articles, products, or content</li>
<li><strong>Clustering</strong>: Group similar documents or texts together</li>
<li><strong>Duplicate Detection</strong>: Find near-duplicate content</li>
<li><strong>Multi-language Search</strong>: Search across different languages</li>
</ul>
<h3 id="not-ideal-for">Not Ideal For</h3>
<ul>
<li><strong>Exact Matching</strong>: Use traditional text search for exact matches</li>
<li><strong>Structured Data</strong>: Use SQL databases for structured queries</li>
<li><strong>Real-time Critical Applications</strong>: Embedding computation has latency</li>
<li><strong>Very Short Texts</strong>: Embeddings work better with meaningful content</li>
<li><strong>High-frequency Operations</strong>: Consider caching for repeated queries</li>
</ul>
<h2 id="using-embeddings-via-rest-api">Using Embeddings via REST API</h2>
<p>If you prefer HTTP endpoints over Python code, use the AbstractCore server:</p>
<div class="code-block"><pre><code class="language-bash"># Start the server
pip install &quot;abstractcore[server]&quot;
python -m abstractcore.server.app
</code></pre></div>
<p><strong>HTTP Request:</strong></p>
<div class="code-block"><pre><code class="language-bash">curl -X POST http://localhost:8000/v1/embeddings \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;input&quot;: &quot;Machine learning is fascinating&quot;,
    &quot;model&quot;: &quot;huggingface/sentence-transformers/all-MiniLM-L6-v2&quot;
  }'
</code></pre></div>
<p><strong>Model IDs via REST API (examples):</strong>
- <code>huggingface/model-name</code>
- <code>ollama/model-name</code>
- <code>lmstudio/model-name</code></p>
<p><strong>Complete REST API documentation:</strong> <a href="server.html#embeddings">Server API Reference</a></p>
<h2 id="provider-specific-features">Provider-Specific Features</h2>
<h3 id="huggingface-features">HuggingFace Features</h3>
<ul>
<li><strong>ONNX Acceleration</strong> (when available)</li>
<li><strong>Matryoshka Truncation</strong>: Reduce dimensions for efficiency</li>
<li><strong>Persistent Caching</strong>: Automatic disk caching of embeddings</li>
</ul>
<h3 id="ollama-features">Ollama Features</h3>
<ul>
<li><strong>Simple Setup</strong>: Just <code>ollama pull &lt;model&gt;</code></li>
<li><strong>Full Privacy</strong>: No data leaves your machine</li>
<li><strong>Custom Models</strong>: Use any Ollama-compatible model</li>
</ul>
<h3 id="lmstudio-features">LMStudio Features</h3>
<ul>
<li><strong>GUI Management</strong>: Easy model loading via GUI</li>
<li><strong>Testing Friendly</strong>: Suitable for experimentation</li>
<li><strong>OpenAI Compatible</strong>: Standard API format</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<ul>
<li><strong>Start Simple</strong>: Try the semantic search example with your own data</li>
<li><strong>Experiment with Providers</strong>: Compare HuggingFace, Ollama, and LMStudio</li>
<li><strong>Optimize Performance</strong>: Use batch processing and caching for production</li>
<li><strong>Build RAG</strong>: Combine embeddings with AbstractCore LLMs for RAG applications</li>
<li><strong>Use REST API</strong>: Deploy embeddings as HTTP service with the server</li>
</ul>
<h2 id="related-documentation">Related Documentation</h2>
<p><strong>Core Library:</strong>
- <strong><a href="api-reference.html">Python API Reference</a></strong> - Complete EmbeddingManager API
- <strong><a href="getting-started.html">Getting Started</a></strong> - Basic AbstractCore setup
- <strong><a href="examples.html">Examples</a></strong> - More practical examples</p>
<p><strong>Server (REST API):</strong>
- <strong><a href="server.html">Server Guide</a></strong> - Server setup and deployment
- <strong><a href="server.html">Server API Reference</a></strong> - REST API endpoints including embeddings
- <strong><a href="troubleshooting.html">Troubleshooting</a></strong> - Common embedding issues</p>
<hr />
<p><strong>Remember</strong>: Embeddings are the foundation for semantic understanding. Combined with AbstractCore's multi-provider LLM capabilities, you can build sophisticated AI applications that understand meaning, not just keywords.</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
