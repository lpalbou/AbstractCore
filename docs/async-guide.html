<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Async/Await Guide - AbstractCore</title>
    <meta name="description" content="Complete guide to using async/await with AbstractCore for concurrent LLM operations.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">Async/Await Guide</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">Complete guide to using async/await with AbstractCore for concurrent LLM operations.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#overview" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Overview</a>
<a href="#provider-support" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Provider support</a>
<a href="#basic-usage" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Basic Usage</a>
<a href="#async-streaming" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Async Streaming</a>
<a href="#session-async" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Session Async</a>
<a href="#multi-provider-comparisons" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Multi-Provider Comparisons</a>
<a href="#fastapi-integration" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">FastAPI Integration</a>
<a href="#batch-document-processing" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Batch Document Processing</a>
<a href="#error-handling" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Error Handling</a>
<a href="#practical-tips" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Practical tips</a>
<a href="#common-patterns" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Common Patterns</a>
<a href="#why-mlxhuggingface-use-fallback" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Why MLX/HuggingFace Use Fallback</a>
<a href="#best-practices" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Best Practices</a>
<a href="#learning-resources" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Learning Resources</a>
<a href="#summary" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Summary</a></div>

            <div class="doc-content">

<h2 id="overview">Overview</h2>
<p>AbstractCore exposes <code>agenerate()</code> for async generation across providers.</p>
<ul>
<li><strong>HTTP-based providers</strong> (OpenAI-compatible endpoints, OpenRouter, Ollama, LMStudio, vLLM, etc.) implement native async I/O.</li>
<li><strong>In-process local inference</strong> providers (MLX, HuggingFace) use an <code>asyncio.to_thread()</code> fallback to avoid blocking the event loop.</li>
</ul>
<p>Concurrency can improve throughput when requests are <strong>I/O-bound</strong> (network calls). For local inference, throughput is limited by your hardware and the model runtime.</p>
<h2 id="provider-support">Provider support</h2>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Async implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>openai</code>, <code>anthropic</code></td>
<td>Native async SDK clients (when installed)</td>
</tr>
<tr>
<td>HTTP-based providers (<code>ollama</code>, <code>lmstudio</code>, <code>openrouter</code>, <code>vllm</code>, <code>openai-compatible</code>, …)</td>
<td><code>httpx.AsyncClient</code> (native async HTTP)</td>
</tr>
<tr>
<td><code>mlx</code>, <code>huggingface</code></td>
<td><code>asyncio.to_thread()</code> fallback (keeps the event loop responsive)</td>
</tr>
</tbody>
</table>
<h2 id="basic-usage">Basic Usage</h2>
<h3 id="single-async-request">Single Async Request</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

    # Single async request
    response = await llm.agenerate(&quot;What is Python?&quot;)
    print(response.content)

asyncio.run(main())
</code></pre></div>
<h3 id="concurrent-requests">Concurrent Requests</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b&quot;)

    # Execute 3 requests concurrently
    tasks = [
        llm.agenerate(f&quot;Summarize {topic}&quot;)
        for topic in [&quot;Python&quot;, &quot;JavaScript&quot;, &quot;Rust&quot;]
    ]

    # Gather runs all tasks concurrently
    responses = await asyncio.gather(*tasks)

    for i, response in enumerate(responses):
        print(f&quot;\n{['Python', 'JavaScript', 'Rust'][i]}:&quot;)
        print(response.content)

asyncio.run(main())
</code></pre></div>
<h2 id="async-streaming">Async Streaming</h2>
<h3 id="basic-streaming">Basic Streaming</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

    # Step 1: await the generator
    stream_gen = await llm.agenerate(
        &quot;Write a haiku about coding&quot;,
        stream=True
    )

    # Step 2: async for over the chunks
    async for chunk in stream_gen:
        if chunk.content:
            print(chunk.content, end=&quot;&quot;, flush=True)
    print()

asyncio.run(main())
</code></pre></div>
<h3 id="concurrent-streaming">Concurrent Streaming</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def stream_response(llm, topic, label):
    &quot;&quot;&quot;Stream a single response with label.&quot;&quot;&quot;
    print(f&quot;\n{label}:&quot;)

    stream_gen = await llm.agenerate(f&quot;Explain {topic} in one sentence&quot;, stream=True)

    async for chunk in stream_gen:
        if chunk.content:
            print(chunk.content, end=&quot;&quot;, flush=True)
    print()

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

    # Stream 3 responses concurrently
    await asyncio.gather(
        stream_response(llm, &quot;Python&quot;, &quot;Python&quot;),
        stream_response(llm, &quot;JavaScript&quot;, &quot;JavaScript&quot;),
        stream_response(llm, &quot;Rust&quot;, &quot;Rust&quot;)
    )

asyncio.run(main())
</code></pre></div>
<h2 id="session-async">Session Async</h2>
<h3 id="async-conversation-management">Async Conversation Management</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm
from abstractcore.core.session import BasicSession

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    session = BasicSession(provider=llm)

    # Maintain conversation history with async
    response1 = await session.agenerate(&quot;What is Python?&quot;)
    print(response1.content)

    response2 = await session.agenerate(&quot;What are its main use cases?&quot;)
    print(response2.content)

    # Session tracks full conversation history
    print(f&quot;\nConversation length: {len(session.conversation_history)} messages&quot;)

asyncio.run(main())
</code></pre></div>
<h3 id="concurrent-sessions">Concurrent Sessions</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm
from abstractcore.core.session import BasicSession

async def chat_session(llm, topic, name):
    &quot;&quot;&quot;Run independent chat session.&quot;&quot;&quot;
    session = BasicSession(provider=llm)

    response1 = await session.agenerate(f&quot;What is {topic}?&quot;)
    response2 = await session.agenerate(&quot;Give me a simple example&quot;)

    print(f&quot;\n{name}:&quot;)
    print(f&quot;  Question 1: {response1.content[:50]}...&quot;)
    print(f&quot;  Question 2: {response2.content[:50]}...&quot;)

async def main():
    llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

    # Run 3 independent conversations concurrently
    await asyncio.gather(
        chat_session(llm, &quot;Python&quot;, &quot;Session 1&quot;),
        chat_session(llm, &quot;JavaScript&quot;, &quot;Session 2&quot;),
        chat_session(llm, &quot;Rust&quot;, &quot;Session 3&quot;)
    )

asyncio.run(main())
</code></pre></div>
<h2 id="multi-provider-comparisons">Multi-Provider Comparisons</h2>
<h3 id="concurrent-provider-queries">Concurrent Provider Queries</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def query_provider(provider_name, model, prompt):
    &quot;&quot;&quot;Query a single provider.&quot;&quot;&quot;
    llm = create_llm(provider_name, model=model)
    response = await llm.agenerate(prompt)
    return {
        &quot;provider&quot;: provider_name,
        &quot;model&quot;: model,
        &quot;response&quot;: response.content
    }

async def main():
    prompt = &quot;What is the capital of France?&quot;

    # Query multiple providers simultaneously
    results = await asyncio.gather(
        query_provider(&quot;openai&quot;, &quot;gpt-4o-mini&quot;, prompt),
        query_provider(&quot;anthropic&quot;, &quot;claude-haiku-4-5&quot;, prompt),
        query_provider(&quot;ollama&quot;, &quot;qwen3:4b&quot;, prompt)
    )

    for result in results:
        print(f&quot;\n{result['provider']} ({result['model']}):&quot;)
        print(result['response'])

asyncio.run(main())
</code></pre></div>
<h3 id="provider-consensus">Provider Consensus</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    prompt = &quot;Is the Earth flat? Answer yes or no.&quot;

    # Get consensus from 3 providers
    llm_openai = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    llm_anthropic = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)
    llm_ollama = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b&quot;)

    responses = await asyncio.gather(
        llm_openai.agenerate(prompt),
        llm_anthropic.agenerate(prompt),
        llm_ollama.agenerate(prompt)
    )

    answers = [r.content.strip().lower() for r in responses]
    print(f&quot;Answers: {answers}&quot;)
    print(f&quot;Consensus: {'Yes' if answers.count('no') &gt;= 2 else 'No'}&quot;)

asyncio.run(main())
</code></pre></div>
<h2 id="fastapi-integration">FastAPI Integration</h2>
<h3 id="async-http-endpoints">Async HTTP Endpoints</h3>
<div class="code-block"><pre><code class="language-python">from fastapi import FastAPI
from abstractcore import create_llm

app = FastAPI()
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

@app.post(&quot;/generate&quot;)
async def generate(prompt: str):
    &quot;&quot;&quot;Non-blocking LLM generation endpoint.&quot;&quot;&quot;
    response = await llm.agenerate(prompt)
    return {&quot;response&quot;: response.content}

@app.post(&quot;/batch&quot;)
async def batch_generate(prompts: list[str]):
    &quot;&quot;&quot;Process multiple prompts concurrently.&quot;&quot;&quot;
    tasks = [llm.agenerate(p) for p in prompts]
    responses = await asyncio.gather(*tasks)

    return {
        &quot;responses&quot;: [r.content for r in responses]
    }

# Run with: uvicorn your_app:app --reload
</code></pre></div>
<h3 id="streaming-endpoint">Streaming Endpoint</h3>
<div class="code-block"><pre><code class="language-python">from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from abstractcore import create_llm
import asyncio

app = FastAPI()
llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

async def stream_response(prompt: str):
    &quot;&quot;&quot;Generate streaming response.&quot;&quot;&quot;
    stream_gen = await llm.agenerate(prompt, stream=True)

    async for chunk in stream_gen:
        if chunk.content:
            yield f&quot;data: {chunk.content}\n\n&quot;

@app.post(&quot;/stream&quot;)
async def stream_generate(prompt: str):
    &quot;&quot;&quot;Streaming LLM generation endpoint.&quot;&quot;&quot;
    return StreamingResponse(
        stream_response(prompt),
        media_type=&quot;text/event-stream&quot;
    )
</code></pre></div>
<h2 id="batch-document-processing">Batch Document Processing</h2>
<h3 id="concurrent-document-summaries">Concurrent Document Summaries</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm
from abstractcore.processing import Summarizer

async def summarize_document(summarizer, doc_path):
    &quot;&quot;&quot;Summarize single document.&quot;&quot;&quot;
    result = summarizer.summarize(
        input_source=doc_path,
        style=&quot;executive&quot;,
        length=&quot;brief&quot;
    )
    return {
        &quot;path&quot;: doc_path,
        &quot;summary&quot;: result.summary
    }

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    summarizer = Summarizer(llm)

    documents = [
        &quot;report1.pdf&quot;,
        &quot;report2.pdf&quot;,
        &quot;report3.pdf&quot;
    ]

    # Summarize all documents concurrently
    tasks = [summarize_document(summarizer, doc) for doc in documents]
    results = await asyncio.gather(*tasks)

    for result in results:
        print(f&quot;\n{result['path']}:&quot;)
        print(result['summary'])

asyncio.run(main())
</code></pre></div>
<h2 id="error-handling">Error Handling</h2>
<h3 id="graceful-error-recovery">Graceful Error Recovery</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm
from abstractcore.exceptions import ProviderAPIError

async def safe_generate(llm, prompt, label):
    &quot;&quot;&quot;Generate with error handling.&quot;&quot;&quot;
    try:
        response = await llm.agenerate(prompt)
        return {&quot;label&quot;: label, &quot;content&quot;: response.content, &quot;error&quot;: None}
    except ProviderAPIError as e:
        return {&quot;label&quot;: label, &quot;content&quot;: None, &quot;error&quot;: str(e)}

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

    # Some requests may fail - continue processing others
    results = await asyncio.gather(
        safe_generate(llm, &quot;Valid prompt 1&quot;, &quot;Task 1&quot;),
        safe_generate(llm, &quot;Valid prompt 2&quot;, &quot;Task 2&quot;),
        safe_generate(llm, &quot;Valid prompt 3&quot;, &quot;Task 3&quot;)
    )

    for result in results:
        if result[&quot;error&quot;]:
            print(f&quot;{result['label']}: ERROR - {result['error']}&quot;)
        else:
            print(f&quot;{result['label']}: {result['content']}&quot;)

asyncio.run(main())
</code></pre></div>
<h2 id="practical-tips">Practical tips</h2>
<h3 id="1-prefer-native-async-providers-when-possible">1. Prefer native-async providers when possible</h3>
<div class="code-block"><pre><code class="language-python"># ✅ Native async HTTP (I/O-bound)
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b&quot;)

# ✅ Native async SDK (cloud APIs)
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# ⚠️ Fallback: runs sync generation in a thread (keeps the event loop responsive)
llm = create_llm(&quot;mlx&quot;, model=&quot;mlx-community/Qwen3-4B-4bit&quot;)
</code></pre></div>
<h3 id="2-batch-similar-operations">2. Batch Similar Operations</h3>
<div class="code-block"><pre><code class="language-python"># ✅ GOOD: Single gather for all tasks
tasks = [llm.agenerate(f&quot;Task {i}&quot;) for i in range(10)]
results = await asyncio.gather(*tasks)

# ❌ BAD: Sequential awaits lose concurrency benefit
results = []
for i in range(10):
    result = await llm.agenerate(f&quot;Task {i}&quot;)
    results.append(result)
</code></pre></div>
<h3 id="3-mix-async-with-sync-io">3. Mix Async with Sync I/O</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

    # Concurrent: LLM generation + file I/O
    llm_task = llm.agenerate(&quot;Explain async&quot;)
    file_task = asyncio.to_thread(read_large_file, &quot;data.txt&quot;)

    response, data = await asyncio.gather(llm_task, file_task)
    # Both completed concurrently!
</code></pre></div>
<h2 id="common-patterns">Common Patterns</h2>
<h3 id="retry-with-exponential-backoff">Retry with Exponential Backoff</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def generate_with_retry(llm, prompt, max_retries=3):
    &quot;&quot;&quot;Generate with exponential backoff retry.&quot;&quot;&quot;
    for attempt in range(max_retries):
        try:
            return await llm.agenerate(prompt)
        except Exception as e:
            if attempt == max_retries - 1:
                raise

            wait_time = 2 ** attempt
            print(f&quot;Retry {attempt + 1} after {wait_time}s...&quot;)
            await asyncio.sleep(wait_time)

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    response = await generate_with_retry(llm, &quot;What is Python?&quot;)
    print(response.content)

asyncio.run(main())
</code></pre></div>
<h3 id="rate-limiting">Rate Limiting</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

class RateLimiter:
    def __init__(self, max_per_second):
        self.max_per_second = max_per_second
        self.semaphore = asyncio.Semaphore(max_per_second)
        self.reset_task = None

    async def acquire(self):
        await self.semaphore.acquire()

        # Release after 1 second
        if not self.reset_task or self.reset_task.done():
            self.reset_task = asyncio.create_task(self._release_after_delay())

    async def _release_after_delay(self):
        await asyncio.sleep(1.0)
        self.semaphore.release()

async def main():
    llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    limiter = RateLimiter(max_per_second=5)

    # Process 20 requests with 5 requests/second limit
    async def limited_generate(prompt):
        await limiter.acquire()
        return await llm.agenerate(prompt)

    tasks = [limited_generate(f&quot;Task {i}&quot;) for i in range(20)]
    results = await asyncio.gather(*tasks)

asyncio.run(main())
</code></pre></div>
<h3 id="progress-tracking">Progress Tracking</h3>
<div class="code-block"><pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def generate_with_progress(llm, prompts):
    &quot;&quot;&quot;Generate with real-time progress tracking.&quot;&quot;&quot;
    completed = 0
    total = len(prompts)

    async def track_task(prompt):
        nonlocal completed
        response = await llm.agenerate(prompt)
        completed += 1
        print(f&quot;Progress: {completed}/{total} ({completed/total*100:.1f}%)&quot;)
        return response

    tasks = [track_task(p) for p in prompts]
    return await asyncio.gather(*tasks)

async def main():
    llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3:4b&quot;)
    prompts = [f&quot;Task {i}&quot; for i in range(10)]

    results = await generate_with_progress(llm, prompts)
    print(f&quot;\nCompleted {len(results)} tasks!&quot;)

asyncio.run(main())
</code></pre></div>
<h2 id="why-mlxhuggingface-use-fallback">Why MLX/HuggingFace Use Fallback</h2>
<p>MLX and HuggingFace providers use <code>asyncio.to_thread()</code> fallback because:</p>
<ol>
<li><strong>No Async Library APIs</strong>: Neither <code>mlx_lm</code> nor <code>transformers</code> expose async Python APIs</li>
<li><strong>Direct Function Calls</strong>: No HTTP layer to enable concurrent I/O</li>
<li><strong>Industry Standard</strong>: Same pattern used by LangChain, Pydantic-AI for CPU-bound operations</li>
<li><strong>Event Loop Responsive</strong>: Fallback keeps event loop responsive for mixing with I/O</li>
</ol>
<div class="code-block"><pre><code class="language-python"># MLX/HF async example (fallback keeps event loop responsive)
import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm(&quot;mlx&quot;, model=&quot;mlx-community/Qwen3-4B-4bit&quot;)

    # Can mix MLX inference with async I/O
    inference_task = llm.agenerate(&quot;What is Python?&quot;)
    io_task = fetch_data_from_api()  # Async I/O

    # Both run concurrently - event loop not blocked!
    response, data = await asyncio.gather(inference_task, io_task)

asyncio.run(main())
</code></pre></div>
<p>If you run local inference behind an OpenAI-compatible HTTP server (for example, via LM Studio), you can use the <code>lmstudio</code> (or <code>openai-compatible</code>) provider for native async I/O to the server:</p>
<div class="code-block"><pre><code class="language-python">llm = create_llm(&quot;lmstudio&quot;, model=&quot;local-model&quot;, base_url=&quot;http://localhost:1234/v1&quot;)
</code></pre></div>
<h2 id="best-practices">Best Practices</h2>
<h3 id="1-always-use-asynciogather-for-concurrent-tasks">1. Always Use asyncio.gather() for Concurrent Tasks</h3>
<div class="code-block"><pre><code class="language-python"># ✅ CORRECT: All tasks run concurrently
results = await asyncio.gather(*[llm.agenerate(p) for p in prompts])

# ❌ WRONG: Sequential execution (no concurrency)
results = [await llm.agenerate(p) for p in prompts]
</code></pre></div>
<h3 id="2-await-stream-generator-first">2. Await Stream Generator First</h3>
<div class="code-block"><pre><code class="language-python"># ✅ CORRECT: Two-step pattern
stream_gen = await llm.agenerate(prompt, stream=True)
async for chunk in stream_gen:
    print(chunk.content, end=&quot;&quot;)

# ❌ WRONG: Missing await before async for
async for chunk in llm.agenerate(prompt, stream=True):  # Error!
    print(chunk.content, end=&quot;&quot;)
</code></pre></div>
<h3 id="3-close-resources-properly">3. Close Resources Properly</h3>
<div class="code-block"><pre><code class="language-python"># ✅ GOOD: Clean shutdown
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
try:
    response = await llm.agenerate(&quot;Test&quot;)
finally:
    llm.unload_model(llm.model)  # Closes async client
</code></pre></div>
<h3 id="4-handle-errors-in-concurrent-operations">4. Handle Errors in Concurrent Operations</h3>
<div class="code-block"><pre><code class="language-python"># ✅ GOOD: Catch errors per-task
async def safe_task(prompt):
    try:
        return await llm.agenerate(prompt)
    except Exception as e:
        return f&quot;Error: {e}&quot;

results = await asyncio.gather(*[safe_task(p) for p in prompts])
</code></pre></div>
<h2 id="learning-resources">Learning Resources</h2>
<ul>
<li><strong>Educational Demo</strong>: <a href="../examples/async_cli_demo.py">examples/async_cli_demo.py</a> - 8 core async/await patterns</li>
<li><strong>Test Suite</strong>: <code>tests/async/test_async_providers.py</code> - real implementation examples</li>
<li><strong>Concurrency &amp; Throughput</strong>: <a href="concurrency.html">concurrency.md</a> - practical guidance for local inference</li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li>✅ <code>agenerate()</code> works across providers</li>
<li>✅ Use <code>asyncio.gather()</code> for concurrent (I/O-bound) requests</li>
<li>✅ HTTP-based providers use native async; MLX/HuggingFace use a thread fallback to keep the event loop responsive</li>
<li>✅ Async streaming uses a 2-step pattern: <code>stream_gen = await llm.agenerate(..., stream=True)</code> then <code>async for ...</code></li>
<li>✅ Works well in FastAPI and other async frameworks</li>
</ul>
<p><strong>Get Started</strong>:</p>
<div class="code-block"><pre><code class="language-bash">pip install abstractcore

# Try the educational async demo
python examples/async_cli_demo.py --provider ollama --model qwen3:4b
</code></pre></div>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
