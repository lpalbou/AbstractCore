<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Reference - AbstractCore</title>
    <meta name="description" content="Complete reference for the AbstractCore API. All examples work across any provider.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">API Reference</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">Complete reference for the AbstractCore API. All examples work across any provider.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#table-of-contents" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Table of Contents</a>
<a href="#core-functions" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Functions</a>
<a href="#classes" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Classes</a>
<a href="#event-system" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Event System</a>
<a href="#retry-configuration" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Retry Configuration</a>
<a href="#embeddings" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Embeddings</a>
<a href="#exceptions" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Exceptions</a>
<a href="#advanced-usage-patterns" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Advanced Usage Patterns</a></div>

            <div class="doc-content">


<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#core-functions">Core Functions</a></li>
<li><a href="#classes">Classes</a></li>
<li><a href="#abstractcoreinterface">AbstractCoreInterface</a><ul>
<li><a href="#generate">generate()</a></li>
<li><a href="#agenerate">agenerate()</a></li>
</ul>
</li>
<li><a href="#basicsession">BasicSession</a><ul>
<li><a href="#generate-1">generate()</a></li>
<li><a href="#agenerate-1">agenerate()</a></li>
</ul>
</li>
<li><a href="#event-system">Event System</a></li>
<li><a href="#retry-configuration">Retry Configuration</a></li>
<li><a href="#embeddings">Embeddings</a></li>
<li><a href="#exceptions">Exceptions</a></li>
</ul>
<h2 id="core-functions">Core Functions</h2>
<h3 id="createllm">create_llm()</h3>
<p>Creates an LLM provider instance.</p>
<div class="code-block"><pre><code class="language-python">def create_llm(
    provider: str,
    model: Optional[str] = None,
    retry_config: Optional[RetryConfig] = None,
    **kwargs
) -&gt; AbstractCoreInterface
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>provider</code> (str): Provider name ("openai", "anthropic", "ollama", "mlx", "lmstudio", "huggingface")
- <code>model</code> (str, optional): Model name. If not provided, uses provider default
- <code>retry_config</code> (RetryConfig, optional): Custom retry configuration
- <code>**kwargs</code>: Provider-specific parameters</p>
<p><strong>Provider-specific parameters:</strong>
- <code>api_key</code> (str): API key for cloud providers
- <code>base_url</code> (str): Custom endpoint URL
- <code>temperature</code> (float): Sampling temperature (0.0-1.0, controls creativity)
- <code>seed</code> (int): Random seed for deterministic outputs (‚úÖ OpenAI, Ollama, MLX, HuggingFace, LMStudio; ‚ö†Ô∏è Anthropic issues warning)
- <code>max_tokens</code> (int): Maximum output tokens
- <code>timeout</code> (int): Request timeout in seconds
- <code>top_p</code> (float): Nucleus sampling parameter</p>
<p><strong>Returns:</strong> AbstractCoreInterface instance</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Basic usage
llm = create_llm("openai", model="gpt-4o-mini")

# With configuration
llm = create_llm(
    "anthropic",
    model="claude-haiku-4-5",
    temperature=0.7,
    max_tokens=1000,
    timeout=30
)

# Local provider
llm = create_llm("ollama", model="qwen2.5-coder:7b", base_url="http://localhost:11434")
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="abstractcoreinterface">AbstractCoreInterface</h3>
<p>Base interface for all LLM providers. All providers implement this interface.</p>
<h4 id="generate">generate()</h4>
<p>Generate text response from the LLM.</p>
<div class="code-block"><pre><code class="language-python">def generate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[List[Dict]] = None,
    response_model: Optional[BaseModel] = None,
    retry_strategy: Optional[Retry] = None,
    stream: bool = False,
    thinking: Optional[bool | str] = None,
    **kwargs
) -&gt; Union[GenerateResponse, Iterator[GenerateResponse]]
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>prompt</code> (str): Text prompt to generate from
- <code>messages</code> (List[Dict], optional): Conversation messages in OpenAI format
- <code>system_prompt</code> (str, optional): System prompt to set context
- <code>tools</code> (List[Dict], optional): Tools the LLM can call
- <code>response_model</code> (BaseModel, optional): Pydantic model for structured output
- <code>retry_strategy</code> (Retry, optional): Custom retry strategy for structured output
- <code>stream</code> (bool): Enable streaming response
- <code>thinking</code> (bool | str, optional): Unified thinking/reasoning control (<code>"auto"|"on"|"off"</code> or <code>"low"|"medium"|"high"</code> when supported)
- <code>**kwargs</code>: Additional generation parameters</p>
<p><strong>Returns:</strong>
- If <code>stream=False</code>: GenerateResponse
- If <code>stream=True</code>: Iterator[GenerateResponse]</p>
<p><strong>Examples:</strong></p>
<p><strong>Basic Generation:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate("What is machine learning?")
print(response.content)
</code></pre></div>
<p><strong>With System Prompt:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(
    "Explain Python decorators",
    system_prompt="You are a Python expert. Always provide code examples."
)
</code></pre></div>
<p><strong>Structured Output:</strong></p>
<div class="code-block"><pre><code class="language-python">from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

person = llm.generate(
    "Extract: John Doe is 25 years old",
    response_model=Person
)
print(f"{person.name}, age {person.age}")
</code></pre></div>
<blockquote>
<p><strong>See</strong>: <a href="structured-output.html">Structured Output Guide</a> for comprehensive documentation</p>
</blockquote>
<p><strong>Tool Calling:</strong></p>
<div class="code-block"><pre><code class="language-python">def get_weather(city: str) -&gt; str:
    return f"Weather in {city}: sunny, 22¬∞C"

tools = [{
    "name": "get_weather",
    "description": "Get weather for a city",
    "parameters": {
        "type": "object",
        "properties": {"city": {"type": "string"}},
        "required": ["city"]
    }
}]

response = llm.generate("What's the weather in Paris?", tools=tools)
</code></pre></div>
<p><strong>Streaming (Unified 2025 Implementation):</strong></p>
<div class="code-block"><pre><code class="language-python"># Streaming works identically across ALL providers
print("AI: ", end="")
for chunk in llm.generate(
    "Create a Python function with a tool",
    stream=True,
    tools=[code_analysis_tool]
):
    # Real-time chunk processing
    print(chunk.content, end="", flush=True)

    # Tool calls are surfaced as structured dicts; execute them in your host/runtime.
    if chunk.tool_calls:
        print(f"\nTool calls: {chunk.tool_calls}")
</code></pre></div>
<p><strong>Streaming Features</strong>:
- First chunk in &lt;10ms
- üîß Unified strategy across providers
- üõ†Ô∏è Real-time tool call detection
- üí® Zero buffering overhead
- Supports: OpenAI, Anthropic, Ollama, MLX, LMStudio, HuggingFace
- üîí Robust error handling for malformed responses</p>
<h4 id="agenerate">agenerate()</h4>
<p>Async version of <code>generate()</code> for concurrent request execution.</p>
<div class="code-block"><pre><code class="language-python">async def agenerate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[List[Dict]] = None,
    response_model: Optional[BaseModel] = None,
    stream: bool = False,
    **kwargs
) -&gt; Union[GenerateResponse, AsyncIterator[GenerateResponse]]
</code></pre></div>
<p><strong>Parameters:</strong> Same as <code>generate()</code></p>
<p><strong>Returns:</strong>
- If <code>stream=False</code>: GenerateResponse
- If <code>stream=True</code>: AsyncIterator[GenerateResponse]</p>
<p><strong>Examples:</strong></p>
<p><strong>Basic Async:</strong></p>
<div class="code-block"><pre><code class="language-python">import asyncio

async def main():
    response = await llm.agenerate("What is quantum computing?")
    print(response.content)

asyncio.run(main())
</code></pre></div>
<p><strong>Concurrent Requests:</strong></p>
<div class="code-block"><pre><code class="language-python">async def batch_process():
    tasks = [
        llm.agenerate("Summarize Python"),
        llm.agenerate("Summarize JavaScript"),
        llm.agenerate("Summarize Rust")
    ]
    responses = await asyncio.gather(*tasks)

    for response in responses:
        print(response.content)

asyncio.run(batch_process())
</code></pre></div>
<p><strong>Async Streaming:</strong></p>
<div class="code-block"><pre><code class="language-python">async def stream_response():
    async for chunk in llm.agenerate("Tell me a story", stream=True):
        print(chunk.content, end='', flush=True)

asyncio.run(stream_response())
</code></pre></div>
<p><strong>Multi-Provider Comparison:</strong></p>
<div class="code-block"><pre><code class="language-python">async def compare_providers():
    openai = create_llm("openai", model="gpt-4o-mini")
    claude = create_llm("anthropic", model="claude-haiku-4-5")

    responses = await asyncio.gather(
        openai.agenerate("What is 2+2?"),
        claude.agenerate("What is 2+2?")
    )

    print(f"OpenAI: {responses[0].content}")
    print(f"Claude: {responses[1].content}")

asyncio.run(compare_providers())
</code></pre></div>
<p><strong>Features:</strong>
- Works across AbstractCore providers (cloud + local); some use native async, others fall back to <code>asyncio.to_thread()</code>
- Faster batch operations via concurrent execution (depends on provider, network, and hardware)
- Full streaming support with AsyncIterator
- Compatible with FastAPI and async web frameworks
- Zero breaking changes to sync API</p>
<h4 id="getcapabilities">get_capabilities()</h4>
<p>Get provider capabilities.</p>
<div class="code-block"><pre><code class="language-python">def get_capabilities(self) -&gt; List[str]
</code></pre></div>
<p><strong>Returns:</strong> List of capability strings</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">capabilities = llm.get_capabilities()
print(capabilities)  # ['text_generation', 'tool_calling', 'streaming', 'vision']
</code></pre></div>
<h4 id="unloadmodelmodelname">unload_model(model_name)</h4>
<p>Unload/cleanup resources for a specific model (best-effort).</p>
<div class="code-block"><pre><code class="language-python">def unload_model(self, model_name: str) -&gt; None
</code></pre></div>
<p>For local providers (Ollama, MLX, HuggingFace, LMStudio), this explicitly frees model memory or releases client resources. For API providers (OpenAI, Anthropic), this is typically a no-op but safe to call.</p>
<p><strong>Provider-specific behavior:</strong>
- <strong>Ollama</strong>: Sends <code>keep_alive=0</code> to immediately unload from server
- <strong>MLX</strong>: Clears model/tokenizer references and forces garbage collection
- <strong>HuggingFace</strong>: Closes llama.cpp resources (GGUF) or clears model references
- <strong>LMStudio</strong>: Closes HTTP connection (server auto-manages via TTL)
- <strong>OpenAI/Anthropic</strong>: No-op (safe to call)</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python"># Load and use a large model
llm = create_llm("ollama", model="qwen3-coder:30b")
response = llm.generate("Hello world")

# Explicitly free memory when done
llm.unload_model(llm.model)
del llm

# Now safe to load another large model
llm2 = create_llm("mlx", model="mlx-community/Qwen3-30B-4bit")
</code></pre></div>
<p><strong>Use cases:</strong>
- Test suites testing multiple models sequentially
- Memory-constrained environments (&lt;32GB RAM)
- Sequential model loading in production systems</p>
<h3 id="generateresponse">GenerateResponse</h3>
<p>Response object from LLM generation with <strong>consistent token terminology</strong> and <strong>generation time tracking</strong>.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class GenerateResponse:
    content: Optional[str]
    raw_response: Any
    model: Optional[str]
    finish_reason: Optional[str]
    usage: Optional[Dict[str, int]]
    tool_calls: Optional[List[Dict]]
    metadata: Optional[Dict]
    gen_time: Optional[float]  # Generation time in milliseconds

    # Consistent token access properties
    @property
    def input_tokens(self) -&gt; Optional[int]:
        """Get input tokens with consistent terminology."""

    @property
    def output_tokens(self) -&gt; Optional[int]:
        """Get output tokens with consistent terminology."""

    @property
    def total_tokens(self) -&gt; Optional[int]:
        """Get total tokens."""
</code></pre></div>
<p><strong>Attributes:</strong>
- <code>content</code> (str): Generated text content
- <code>raw_response</code> (Any): Raw provider response
- <code>model</code> (str): Model used for generation
- <code>finish_reason</code> (str): Why generation stopped ("stop", "length", "tool_calls")
- <code>usage</code> (Dict): Token usage information
- <code>tool_calls</code> (List[Dict]): Tools called by the LLM
- <code>metadata</code> (Dict): Additional metadata (notably <code>metadata["reasoning"]</code> when a provider/model exposes thinking/reasoning)
- <code>gen_time</code> (float): Generation time in milliseconds, rounded to 1 decimal place</p>
<p><strong>Token and Timing Access Examples:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate("Explain quantum computing")

# Consistent access across ALL providers
print(f"Input tokens: {response.input_tokens}")      # Always available
print(f"Output tokens: {response.output_tokens}")    # Always available  
print(f"Total tokens: {response.total_tokens}")      # Always available
print(f"Generation time: {response.gen_time}ms")  # Always available

# Comprehensive summary
print(f"Summary: {response.get_summary()}")  # Model | Tokens | Time | Tools

# Raw usage dictionary (provider-specific format)
print(f"Usage details: {response.usage}")
</code></pre></div>
<p><strong>Token Count Sources:</strong>
- <strong>Provider APIs</strong>: OpenAI, Anthropic, LMStudio (native API token counts)
- <strong>AbstractCore Calculation</strong>: MLX, HuggingFace (using <code>token_utils.py</code>)
- <strong>Mixed Sources</strong>: Ollama (combination of provider and calculated tokens)</p>
<p><strong>Backward Compatibility</strong>: Legacy <code>prompt_tokens</code> and <code>completion_tokens</code> keys remain available in <code>response.usage</code> dictionary.</p>
<p><strong>Methods:</strong></p>
<h4 id="hastoolcalls">has_tool_calls()</h4>
<div class="code-block"><pre><code class="language-python">def has_tool_calls(self) -&gt; bool
</code></pre></div>
<p>Returns True if the response contains tool calls.</p>
<h4 id="gettoolsexecuted">get_tools_executed()</h4>
<div class="code-block"><pre><code class="language-python">def get_tools_executed(self) -&gt; List[str]
</code></pre></div>
<p>Returns list of tool names that were executed.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate("What's 2+2?", tools=[calculator_tool])

print(f"Content: {response.content}")
print(f"Model: {response.model}")
print(f"Tokens: {response.usage}")

if response.has_tool_calls():
    print(f"Tools used: {response.get_tools_executed()}")
</code></pre></div>
<h3 id="basicsession">BasicSession</h3>
<p>Manages conversation context and history.</p>
<div class="code-block"><pre><code class="language-python">class BasicSession:
    def __init__(
        self,
        provider: AbstractCoreInterface,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        seed: Optional[int] = None,
        **kwargs
    ):
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>provider</code> (AbstractCoreInterface): LLM provider instance
- <code>system_prompt</code> (str, optional): System prompt for the conversation
- <code>temperature</code> (float, optional): Default temperature for all generations (0.0-1.0)
- <code>seed</code> (int, optional): Default seed for deterministic outputs (provider support varies)
- <code>**kwargs</code>: Additional session parameters (tools, timeouts, etc.)</p>
<p><strong>Attributes:</strong>
- <code>messages</code> (List[Message]): Conversation history
- <code>provider</code> (AbstractCoreInterface): LLM provider
- <code>system_prompt</code> (str): System prompt</p>
<p><strong>Methods:</strong></p>
<h4 id="generate_1">generate()</h4>
<div class="code-block"><pre><code class="language-python">def generate(self, prompt: str, **kwargs) -&gt; GenerateResponse
</code></pre></div>
<p>Generate response and add to conversation history.</p>
<h4 id="agenerate_1">agenerate()</h4>
<div class="code-block"><pre><code class="language-python">async def agenerate(
    self,
    prompt: str,
    name: Optional[str] = None,
    location: Optional[str] = None,
    **kwargs
) -&gt; Union[GenerateResponse, AsyncIterator[GenerateResponse]]
</code></pre></div>
<p>Async version of <code>generate()</code>. Maintains conversation history with async execution.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">import asyncio

async def chat():
    session = BasicSession(provider=llm)

    # Async conversation
    response1 = await session.agenerate("My name is Alice")
    response2 = await session.agenerate("What's my name?")

    print(response2.content)  # References Alice

asyncio.run(chat())
</code></pre></div>
<h4 id="addmessage">add_message()</h4>
<div class="code-block"><pre><code class="language-python">def add_message(self, role: str, content: str, **metadata) -&gt; Message
</code></pre></div>
<p>Add message to conversation history.</p>
<h4 id="clearhistory">clear_history()</h4>
<div class="code-block"><pre><code class="language-python">def clear_history(self, keep_system: bool = True) -&gt; None
</code></pre></div>
<p>Clear conversation history, optionally keeping system prompt.</p>
<h4 id="save">save()</h4>
<div class="code-block"><pre><code class="language-python">def save(self, filepath: Path) -&gt; None
</code></pre></div>
<p>Save session to JSON file.</p>
<h4 id="load">load()</h4>
<div class="code-block"><pre><code class="language-python">@classmethod
def load(cls, filepath: Path, provider: AbstractCoreInterface) -&gt; "BasicSession"
</code></pre></div>
<p>Load session from JSON file.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, BasicSession

llm = create_llm("openai", model="gpt-4o-mini")
session = BasicSession(
    provider=llm,
    system_prompt="You are a helpful coding tutor.",
    temperature=0.3,  # Focused responses
    seed=42          # Consistent outputs
)

# Multi-turn conversation
response1 = session.generate("What are Python decorators?")
response2 = session.generate("Show me an example", temperature=0.7)  # Override for this call

print(f"Conversation has {len(session.messages)} messages")

# Save session
session.save(Path("conversation.json"))

# Load later
loaded_session = BasicSession.load(Path("conversation.json"), llm)
</code></pre></div>
<h3 id="message">Message</h3>
<p>Represents a conversation message.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class Message:
    role: str
    content: str
    timestamp: Optional[datetime] = None
    name: Optional[str] = None
    metadata: Optional[Dict] = None
</code></pre></div>
<p><strong>Methods:</strong></p>
<h4 id="todict">to_dict()</h4>
<div class="code-block"><pre><code class="language-python">def to_dict(self) -&gt; Dict
</code></pre></div>
<p>Convert message to dictionary.</p>
<h4 id="fromdict">from_dict()</h4>
<div class="code-block"><pre><code class="language-python">@classmethod
def from_dict(cls, data: Dict) -&gt; "Message"
</code></pre></div>
<p>Create message from dictionary.</p>
<h2 id="event-system">Event System</h2>
<h3 id="eventtype">EventType</h3>
<p>Available event types for monitoring.</p>
<div class="code-block"><pre><code class="language-python">class EventType(Enum):
    # Generation events
    GENERATION_STARTED = "generation_started"
    GENERATION_COMPLETED = "generation_completed"

    # Tool events
    TOOL_STARTED = "tool_started"
    TOOL_PROGRESS = "tool_progress"
    TOOL_COMPLETED = "tool_completed"

    # Error handling
    ERROR = "error"

    # Retry and resilience events
    RETRY_ATTEMPTED = "retry_attempted"
    RETRY_EXHAUSTED = "retry_exhausted"

    # Useful events
    VALIDATION_FAILED = "validation_failed"
    SESSION_CREATED = "session_created"
    SESSION_CLEARED = "session_cleared"
    COMPACTION_STARTED = "compaction_started"
    COMPACTION_COMPLETED = "compaction_completed"

    # Runtime/workflow events
    WORKFLOW_STEP_STARTED = "workflow_step_started"
    WORKFLOW_STEP_COMPLETED = "workflow_step_completed"
    WORKFLOW_STEP_WAITING = "workflow_step_waiting"
    WORKFLOW_STEP_FAILED = "workflow_step_failed"
</code></pre></div>
<h3 id="onglobal">on_global()</h3>
<p>Register global event handler.</p>
<div class="code-block"><pre><code class="language-python">def on_global(event_type: EventType, handler: Callable[[Event], None]) -&gt; None
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>event_type</code> (EventType): Event type to listen for
- <code>handler</code> (Callable): Function to call when event occurs</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global

def cost_monitor(event):
    cost = event.data.get("cost_usd")
    if cost:
        # NOTE: `cost_usd` is a best-effort estimate based on token usage.
        print(f"Estimated cost: ${cost:.4f}")

def tool_monitor(event):
    # Tool event payload shape varies by emitter.
    # - Single-tool execution: {"tool_name": ..., "success": ..., ...}
    # - Batch execution: {"tool_results": [{"name": ..., "success": ...}, ...], ...}
    tool_name = event.data.get("tool_name")
    if tool_name:
        print(f"Tool completed: {tool_name} success={event.data.get('success')}")
        return

    for r in event.data.get("tool_results", []) or []:
        print(f"Tool completed: {r.get('name')} success={r.get('success')} error={r.get('error')}")

# Register handlers
on_global(EventType.GENERATION_COMPLETED, cost_monitor)
on_global(EventType.TOOL_COMPLETED, tool_monitor)

# Now all LLM operations will trigger these handlers
llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate("Hello world")
</code></pre></div>
<h3 id="event">Event</h3>
<p>Event object passed to handlers.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class Event:
    type: EventType
    timestamp: datetime
    data: Dict[str, Any]
    source: Optional[str] = None
</code></pre></div>
<h2 id="retry-configuration">Retry Configuration</h2>
<h3 id="retryconfig">RetryConfig</h3>
<p>Configuration for provider-level retry behavior.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class RetryConfig:
    max_attempts: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    use_jitter: bool = True
    failure_threshold: int = 5
    recovery_timeout: float = 60.0
    half_open_max_calls: int = 2
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>max_attempts</code> (int): Maximum retry attempts
- <code>initial_delay</code> (float): Initial delay in seconds
- <code>max_delay</code> (float): Maximum delay in seconds
- <code>exponential_base</code> (float): Base for exponential backoff
- <code>use_jitter</code> (bool): Add randomness to delays
- <code>failure_threshold</code> (int): Circuit breaker failure threshold
- <code>recovery_timeout</code> (float): Circuit breaker recovery timeout
- <code>half_open_max_calls</code> (int): Max calls in half-open state</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

config = RetryConfig(
    max_attempts=5,
    initial_delay=2.0,
    use_jitter=True,
    failure_threshold=3
)

llm = create_llm("openai", model="gpt-4o-mini", retry_config=config)
</code></pre></div>
<h3 id="feedbackretry">FeedbackRetry</h3>
<p>Retry strategy for structured output validation failures.</p>
<div class="code-block"><pre><code class="language-python">class FeedbackRetry:
    def __init__(self, max_attempts: int = 3):
        self.max_attempts = max_attempts
</code></pre></div>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.structured import FeedbackRetry
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int

custom_retry = FeedbackRetry(max_attempts=5)

user = llm.generate(
    "Extract user: John Doe, 25",
    response_model=User,
    retry_strategy=custom_retry
)
</code></pre></div>
<h2 id="embeddings">Embeddings</h2>
<h3 id="embeddingmanager">EmbeddingManager</h3>
<p>Manages text embeddings using SOTA models.</p>
<div class="code-block"><pre><code class="language-python">class EmbeddingManager:
    def __init__(
        self,
        model: str = "embeddinggemma",
        backend: str = "auto",
        output_dims: Optional[int] = None,
        cache_size: int = 1000,
        cache_dir: Optional[str] = None
    ):
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>model</code> (str): Model name ("embeddinggemma", "granite", "stella-400m")
- <code>backend</code> (str): Backend ("auto", "pytorch", "onnx")
- <code>output_dims</code> (int, optional): Truncate output dimensions
- <code>cache_size</code> (int): Memory cache size
- <code>cache_dir</code> (str, optional): Disk cache directory</p>
<p><strong>Methods:</strong></p>
<h4 id="embed">embed()</h4>
<div class="code-block"><pre><code class="language-python">def embed(self, text: str) -&gt; List[float]
</code></pre></div>
<p>Generate embedding for single text.</p>
<h4 id="embedbatch">embed_batch()</h4>
<div class="code-block"><pre><code class="language-python">def embed_batch(self, texts: List[str]) -&gt; List[List[float]]
</code></pre></div>
<p>Generate embeddings for multiple texts (more efficient).</p>
<h4 id="computesimilarity">compute_similarity()</h4>
<div class="code-block"><pre><code class="language-python">def compute_similarity(self, text1: str, text2: str) -&gt; float
</code></pre></div>
<p>Compute cosine similarity between two texts.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager(model="embeddinggemma")

# Single embedding
embedding = embedder.embed("Hello world")
print(f"Embedding dimension: {len(embedding)}")

# Batch embeddings
embeddings = embedder.embed_batch(["Hello", "World", "AI"])

# Similarity
similarity = embedder.compute_similarity("cat", "kitten")
print(f"Similarity: {similarity:.3f}")
</code></pre></div>
<h2 id="exceptions">Exceptions</h2>
<h3 id="base-exceptions">Base Exceptions</h3>
<h4 id="abstractcoreerror">AbstractCoreError</h4>
<div class="code-block"><pre><code class="language-python">class AbstractCoreError(Exception):
    """Base exception for AbstractCore."""
</code></pre></div>
<h4 id="providerapierror">ProviderAPIError</h4>
<div class="code-block"><pre><code class="language-python">class ProviderAPIError(AbstractCoreError):
    """Provider API error."""
</code></pre></div>
<h4 id="modelnotfounderror">ModelNotFoundError</h4>
<div class="code-block"><pre><code class="language-python">class ModelNotFoundError(AbstractCoreError):
    """Model not found error."""
</code></pre></div>
<h4 id="authenticationerror">AuthenticationError</h4>
<div class="code-block"><pre><code class="language-python">class AuthenticationError(ProviderAPIError):
    """Authentication error."""
</code></pre></div>
<h4 id="ratelimiterror">RateLimitError</h4>
<div class="code-block"><pre><code class="language-python">class RateLimitError(ProviderAPIError):
    """Rate limit error."""
</code></pre></div>
<h3 id="usage">Usage</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.exceptions import ProviderAPIError, RateLimitError

try:
    response = llm.generate("Hello world")
except RateLimitError:
    print("Rate limited, wait and retry")
except ProviderAPIError as e:
    print(f"API error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
</code></pre></div>
<h2 id="advanced-usage-patterns">Advanced Usage Patterns</h2>
<h3 id="custom-provider-configuration">Custom Provider Configuration</h3>
<div class="code-block"><pre><code class="language-python"># Provider with all options
llm = create_llm(
    provider="openai",
    model="gpt-4o-mini",
    api_key="your-key",
    temperature=0.7,
    max_tokens=1000,
    top_p=0.9,
    timeout=30,
    retry_config=RetryConfig(max_attempts=5)
)
</code></pre></div>
<h3 id="multi-provider-setup">Multi-Provider Setup</h3>
<div class="code-block"><pre><code class="language-python">providers = {
    "fast": create_llm("openai", model="gpt-4o-mini"),
    "smart": create_llm("openai", model="gpt-4o"),
    "long_context": create_llm("anthropic", model="claude-haiku-4-5"),
    "local": create_llm("ollama", model="qwen2.5-coder:7b")
}

def route_request(prompt, task_type="general"):
    if task_type == "simple":
        return providers["fast"].generate(prompt)
    elif task_type == "complex":
        return providers["smart"].generate(prompt)
    elif len(prompt) &gt; 50000:
        return providers["long_context"].generate(prompt)
    else:
        return providers["local"].generate(prompt)
</code></pre></div>
<h3 id="production-monitoring">Production Monitoring</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cost tracking
total_cost = 0.0

def production_monitor(event):
    global total_cost

    if event.type == EventType.GENERATION_COMPLETED:
        cost = event.data.get("cost_usd")
        if cost:
            # NOTE: `cost_usd` is a best-effort estimate based on token usage.
            total_cost += float(cost)
            logger.info(f"Estimated cost: ${float(cost):.4f}, Total: ${total_cost:.4f}")

        duration_ms = event.data.get("duration_ms")
        if isinstance(duration_ms, (int, float)) and duration_ms &gt; 10_000:
            logger.warning(f"Slow request: {float(duration_ms):.0f}ms")

    elif event.type == EventType.ERROR:
        logger.error(f"Error: {event.data.get('error')}")

    elif event.type == EventType.RETRY_ATTEMPTED:
        logger.info(f"Retrying due to: {event.data.get('error_type')}")

on_global(EventType.GENERATION_COMPLETED, production_monitor)
on_global(EventType.ERROR, production_monitor)
on_global(EventType.RETRY_ATTEMPTED, production_monitor)
</code></pre></div>
<hr/>
<p>For more examples and use cases, see:
- <a href="getting-started.html">Getting Started</a> - Basic setup and usage
- <a href="examples.html">Examples</a> - Practical use cases
- <a href="prerequisites.html">Prerequisites</a> - Provider setup and configuration
- <a href="capabilities.html">Capabilities</a> - What AbstractCore can do</p>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
