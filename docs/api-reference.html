<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Reference - AbstractCore</title>
    <meta name="description" content="Complete reference for the AbstractCore API. All examples work across any provider.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [{'text': 'Features', 'href': '/docs/capabilities.html'}, {'text': 'Quick Start', 'href': '/docs/getting-started.html'}, {'text': 'Documentation', 'href': '/#docs'}, {'text': 'Examples', 'href': '/docs/examples.html'}, {'text': 'GitHub', 'href': 'https://github.com/lpalbou/abstractcore', 'target': '_blank', 'icon': 'github'}, {'text': 'PyPI', 'href': 'https://pypi.org/project/abstractcore/', 'target': '_blank', 'icon': 'pypi'}]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1100px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">API Reference</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">Complete reference for the AbstractCore API. All examples work across any provider.</p>
            </div>

            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;"><h2 style="margin: 0 0 1rem 0;">Table of Contents</h2><a href="#table-of-contents" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Table of Contents</a>
<a href="#core-functions" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Core Functions</a>
<a href="#classes" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Classes</a>
<a href="#event-system" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Event System</a>
<a href="#retry-configuration" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Retry Configuration</a>
<a href="#embeddings" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Embeddings</a>
<a href="#exceptions" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Exceptions</a>
<a href="#advanced-usage-patterns" style="display: block; padding: 0.4rem 0; color: var(--primary-color);">Advanced Usage Patterns</a></div>

            <div class="doc-content">

<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#core-functions">Core Functions</a></li>
<li><a href="#classes">Classes</a></li>
<li><a href="#abstractcoreinterface">AbstractCoreInterface</a><ul>
<li><a href="#generate">generate()</a></li>
<li><a href="#agenerate">agenerate()</a></li>
</ul>
</li>
<li><a href="#basicsession">BasicSession</a><ul>
<li><a href="#generate-1">generate()</a></li>
<li><a href="#agenerate-1">agenerate()</a></li>
</ul>
</li>
<li><a href="#event-system">Event System</a></li>
<li><a href="#retry-configuration">Retry Configuration</a></li>
<li><a href="#embeddings">Embeddings</a></li>
<li><a href="#exceptions">Exceptions</a></li>
</ul>
<h2 id="core-functions">Core Functions</h2>
<h3 id="create_llm">create_llm()</h3>
<p>Creates an LLM provider instance.</p>
<div class="code-block"><pre><code class="language-python">def create_llm(
    provider: str,
    model: Optional[str] = None,
    retry_config: Optional[RetryConfig] = None,
    **kwargs
) -&gt; AbstractCoreInterface
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>provider</code> (str): Provider name ("openai", "anthropic", "ollama", "mlx", "lmstudio", "huggingface")
- <code>model</code> (str, optional): Model name. If not provided, uses provider default
- <code>retry_config</code> (RetryConfig, optional): Custom retry configuration
- <code>**kwargs</code>: Provider-specific parameters</p>
<p><strong>Provider-specific parameters:</strong>
- <code>api_key</code> (str): API key for cloud providers
- <code>base_url</code> (str): Custom endpoint URL
- <code>temperature</code> (float): Sampling temperature (0.0-1.0, controls creativity)
- <code>seed</code> (int): Random seed for deterministic outputs (✅ OpenAI, Ollama, MLX, HuggingFace, LMStudio; ⚠️ Anthropic issues warning)
- <code>max_tokens</code> (int): Maximum output tokens
- <code>timeout</code> (int): Request timeout in seconds
- <code>top_p</code> (float): Nucleus sampling parameter</p>
<p><strong>Returns:</strong> AbstractCoreInterface instance</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm

# Basic usage
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)

# With configuration
llm = create_llm(
    &quot;anthropic&quot;,
    model=&quot;claude-haiku-4-5&quot;,
    temperature=0.7,
    max_tokens=1000,
    timeout=30
)

# Local provider
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen2.5-coder:7b&quot;, base_url=&quot;http://localhost:11434&quot;)
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="abstractcoreinterface">AbstractCoreInterface</h3>
<p>Base interface for all LLM providers. All providers implement this interface.</p>
<h4 id="generate">generate()</h4>
<p>Generate text response from the LLM.</p>
<div class="code-block"><pre><code class="language-python">def generate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[List[Dict]] = None,
    response_model: Optional[BaseModel] = None,
    retry_strategy: Optional[Retry] = None,
    stream: bool = False,
    thinking: Optional[bool | str] = None,
    **kwargs
) -&gt; Union[GenerateResponse, Iterator[GenerateResponse]]
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>prompt</code> (str): Text prompt to generate from
- <code>messages</code> (List[Dict], optional): Conversation messages in OpenAI format
- <code>system_prompt</code> (str, optional): System prompt to set context
- <code>tools</code> (List[Dict], optional): Tools the LLM can call
- <code>response_model</code> (BaseModel, optional): Pydantic model for structured output
- <code>retry_strategy</code> (Retry, optional): Custom retry strategy for structured output
- <code>stream</code> (bool): Enable streaming response
- <code>thinking</code> (bool | str, optional): Unified thinking/reasoning control (<code>"auto"|"on"|"off"</code> or <code>"low"|"medium"|"high"</code> when supported)
- <code>**kwargs</code>: Additional generation parameters</p>
<p><strong>Returns:</strong>
- If <code>stream=False</code>: GenerateResponse
- If <code>stream=True</code>: Iterator[GenerateResponse]</p>
<p><strong>Examples:</strong></p>
<p><strong>Basic Generation:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(&quot;What is machine learning?&quot;)
print(response.content)
</code></pre></div>
<p><strong>With System Prompt:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(
    &quot;Explain Python decorators&quot;,
    system_prompt=&quot;You are a Python expert. Always provide code examples.&quot;
)
</code></pre></div>
<p><strong>Structured Output:</strong></p>
<div class="code-block"><pre><code class="language-python">from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

person = llm.generate(
    &quot;Extract: John Doe is 25 years old&quot;,
    response_model=Person
)
print(f&quot;{person.name}, age {person.age}&quot;)
</code></pre></div>
<blockquote>
<p><strong>See</strong>: <a href="structured-output.html">Structured Output Guide</a> for comprehensive documentation</p>
</blockquote>
<p><strong>Tool Calling:</strong></p>
<div class="code-block"><pre><code class="language-python">def get_weather(city: str) -&gt; str:
    return f&quot;Weather in {city}: sunny, 22°C&quot;

tools = [{
    &quot;name&quot;: &quot;get_weather&quot;,
    &quot;description&quot;: &quot;Get weather for a city&quot;,
    &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {&quot;city&quot;: {&quot;type&quot;: &quot;string&quot;}},
        &quot;required&quot;: [&quot;city&quot;]
    }
}]

response = llm.generate(&quot;What's the weather in Paris?&quot;, tools=tools)
</code></pre></div>
<p><strong>Streaming:</strong></p>
<div class="code-block"><pre><code class="language-python">print(&quot;AI: &quot;, end=&quot;&quot;)
for chunk in llm.generate(
    &quot;Create a Python function with a tool&quot;,
    stream=True,
    tools=tools
):
    # Real-time chunk processing
    print(chunk.content or &quot;&quot;, end=&quot;&quot;, flush=True)

    # Tool calls are surfaced as structured dicts; execute them in your host/runtime.
    if chunk.tool_calls:
        print(f&quot;\nTool calls: {chunk.tool_calls}&quot;)
</code></pre></div>
<p><strong>Streaming notes</strong>:
- Streaming uses a unified processor across providers; exact chunking behavior depends on the backend.
- Tool calls are surfaced as structured dicts in <code>chunk.tool_calls</code>; execute them in your host/runtime (pass-through by default).
- If you need tool-call markup preserved/re-written in <code>chunk.content</code>, pass <code>tool_call_tags=...</code> (see <a href="tool-syntax-rewriting.html">Tool Call Syntax Rewriting</a>).
- In streaming mode, AbstractCore records a best-effort TTFT metric in <code>chunk.metadata["_timing"]["ttft_ms"]</code> when available (for debugging/observability).</p>
<h4 id="agenerate">agenerate()</h4>
<p>Async version of <code>generate()</code> for concurrent request execution.</p>
<div class="code-block"><pre><code class="language-python">async def agenerate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[List[Dict]] = None,
    response_model: Optional[BaseModel] = None,
    stream: bool = False,
    **kwargs
) -&gt; Union[GenerateResponse, AsyncIterator[GenerateResponse]]
</code></pre></div>
<p><strong>Parameters:</strong> Same as <code>generate()</code></p>
<p><strong>Returns:</strong>
- If <code>stream=False</code>: GenerateResponse
- If <code>stream=True</code>: AsyncIterator[GenerateResponse]</p>
<p><strong>Examples:</strong></p>
<p><strong>Basic Async:</strong></p>
<div class="code-block"><pre><code class="language-python">import asyncio

async def main():
    response = await llm.agenerate(&quot;What is quantum computing?&quot;)
    print(response.content)

asyncio.run(main())
</code></pre></div>
<p><strong>Concurrent Requests:</strong></p>
<div class="code-block"><pre><code class="language-python">async def batch_process():
    tasks = [
        llm.agenerate(&quot;Summarize Python&quot;),
        llm.agenerate(&quot;Summarize JavaScript&quot;),
        llm.agenerate(&quot;Summarize Rust&quot;)
    ]
    responses = await asyncio.gather(*tasks)

    for response in responses:
        print(response.content)

asyncio.run(batch_process())
</code></pre></div>
<p><strong>Async Streaming:</strong></p>
<div class="code-block"><pre><code class="language-python">async def stream_response():
    async for chunk in llm.agenerate(&quot;Tell me a story&quot;, stream=True):
        print(chunk.content, end='', flush=True)

asyncio.run(stream_response())
</code></pre></div>
<p><strong>Multi-Provider Comparison:</strong></p>
<div class="code-block"><pre><code class="language-python">async def compare_providers():
    openai = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
    claude = create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;)

    responses = await asyncio.gather(
        openai.agenerate(&quot;What is 2+2?&quot;),
        claude.agenerate(&quot;What is 2+2?&quot;)
    )

    print(f&quot;OpenAI: {responses[0].content}&quot;)
    print(f&quot;Claude: {responses[1].content}&quot;)

asyncio.run(compare_providers())
</code></pre></div>
<p><strong>Features:</strong>
- Works across AbstractCore providers (cloud + local); some use native async, others fall back to <code>asyncio.to_thread()</code>
- Faster batch operations via concurrent execution (depends on provider, network, and hardware)
- Full streaming support with AsyncIterator
- Compatible with FastAPI and async web frameworks
- Zero breaking changes to sync API</p>
<h4 id="get_capabilities">get_capabilities()</h4>
<p>Get provider capabilities.</p>
<div class="code-block"><pre><code class="language-python">def get_capabilities(self) -&gt; List[str]
</code></pre></div>
<p><strong>Returns:</strong> List of capability strings</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">capabilities = llm.get_capabilities()
print(capabilities)  # ['text_generation', 'tool_calling', 'streaming', 'vision']
</code></pre></div>
<h4 id="unload_modelmodel_name">unload_model(model_name)</h4>
<p>Unload/cleanup resources for a specific model (best-effort).</p>
<div class="code-block"><pre><code class="language-python">def unload_model(self, model_name: str) -&gt; None
</code></pre></div>
<p>For local providers (Ollama, MLX, HuggingFace, LMStudio), this explicitly frees model memory or releases client resources. For API providers (OpenAI, Anthropic), this is typically a no-op but safe to call.</p>
<p><strong>Provider-specific behavior:</strong>
- <strong>Ollama</strong>: Sends <code>keep_alive=0</code> to immediately unload from server
- <strong>MLX</strong>: Clears model/tokenizer references and forces garbage collection
- <strong>HuggingFace</strong>: Closes llama.cpp resources (GGUF) or clears model references
- <strong>LMStudio</strong>: Closes HTTP connection (server auto-manages via TTL)
- <strong>OpenAI/Anthropic</strong>: No-op (safe to call)</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python"># Load and use a large model
llm = create_llm(&quot;ollama&quot;, model=&quot;qwen3-coder:30b&quot;)
response = llm.generate(&quot;Hello world&quot;)

# Explicitly free memory when done
llm.unload_model(llm.model)
del llm

# Now safe to load another large model
llm2 = create_llm(&quot;mlx&quot;, model=&quot;mlx-community/Qwen3-30B-4bit&quot;)
</code></pre></div>
<p><strong>Use cases:</strong>
- Test suites testing multiple models sequentially
- Memory-constrained environments (&lt;32GB RAM)
- Sequential model loading in production systems</p>
<h3 id="generateresponse">GenerateResponse</h3>
<p>Response object from LLM generation with <strong>consistent token terminology</strong> and <strong>generation time tracking</strong>.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class GenerateResponse:
    content: Optional[str]
    raw_response: Any
    model: Optional[str]
    finish_reason: Optional[str]
    usage: Optional[Dict[str, int]]
    tool_calls: Optional[List[Dict]]
    metadata: Optional[Dict]
    gen_time: Optional[float]  # Generation time in milliseconds

    # Consistent token access properties
    @property
    def input_tokens(self) -&gt; Optional[int]:
        &quot;&quot;&quot;Get input tokens with consistent terminology.&quot;&quot;&quot;

    @property
    def output_tokens(self) -&gt; Optional[int]:
        &quot;&quot;&quot;Get output tokens with consistent terminology.&quot;&quot;&quot;

    @property
    def total_tokens(self) -&gt; Optional[int]:
        &quot;&quot;&quot;Get total tokens.&quot;&quot;&quot;
</code></pre></div>
<p><strong>Attributes:</strong>
- <code>content</code> (str): Generated text content
- <code>raw_response</code> (Any): Raw provider response
- <code>model</code> (str): Model used for generation
- <code>finish_reason</code> (str): Why generation stopped ("stop", "length", "tool_calls")
- <code>usage</code> (Dict): Token usage information
- <code>tool_calls</code> (List[Dict]): Tools called by the LLM
- <code>metadata</code> (Dict): Additional metadata (notably <code>metadata["reasoning"]</code> when a provider/model exposes thinking/reasoning)
- <code>gen_time</code> (float): Generation time in milliseconds, rounded to 1 decimal place</p>
<p><strong>Token and Timing Access Examples:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(&quot;Explain quantum computing&quot;)

# Best-effort access across supported providers (may be None depending on backend/config)
print(f&quot;Input tokens: {response.input_tokens}&quot;)      # None if usage isn't reported/estimated
print(f&quot;Output tokens: {response.output_tokens}&quot;)    # None if usage isn't reported/estimated
print(f&quot;Total tokens: {response.total_tokens}&quot;)      # None if usage isn't reported/estimated
print(f&quot;Generation time: {response.gen_time}ms&quot;)     # None if timing wasn't captured

# Comprehensive summary
print(f&quot;Summary: {response.get_summary()}&quot;)  # Model | Tokens | Time | Tools

# Raw usage dictionary (provider-specific format)
print(f&quot;Usage details: {response.usage}&quot;)
</code></pre></div>
<p><strong>Token Count Sources:</strong>
- <strong>Provider APIs</strong>: OpenAI, Anthropic, LMStudio (native API token counts)
- <strong>AbstractCore Calculation</strong>: MLX, HuggingFace (using <code>token_utils.py</code>)
- <strong>Mixed Sources</strong>: Ollama (combination of provider and calculated tokens)</p>
<p><strong>Backward Compatibility</strong>: Legacy <code>prompt_tokens</code> and <code>completion_tokens</code> keys remain available in <code>response.usage</code> dictionary.</p>
<p><strong>Methods:</strong></p>
<h4 id="has_tool_calls">has_tool_calls()</h4>
<div class="code-block"><pre><code class="language-python">def has_tool_calls(self) -&gt; bool
</code></pre></div>
<p>Returns True if the response contains tool calls.</p>
<h4 id="get_tools_executed">get_tools_executed()</h4>
<div class="code-block"><pre><code class="language-python">def get_tools_executed(self) -&gt; List[str]
</code></pre></div>
<p>Returns list of tool names that were executed.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">response = llm.generate(&quot;What's 2+2?&quot;, tools=[calculator_tool])

print(f&quot;Content: {response.content}&quot;)
print(f&quot;Model: {response.model}&quot;)
print(f&quot;Tokens: {response.usage}&quot;)

if response.has_tool_calls():
    print(f&quot;Tools used: {response.get_tools_executed()}&quot;)
</code></pre></div>
<h3 id="basicsession">BasicSession</h3>
<p>Manages conversation context and history.</p>
<div class="code-block"><pre><code class="language-python">class BasicSession:
    def __init__(
        self,
        provider: AbstractCoreInterface,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        seed: Optional[int] = None,
        **kwargs
    ):
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>provider</code> (AbstractCoreInterface): LLM provider instance
- <code>system_prompt</code> (str, optional): System prompt for the conversation
- <code>temperature</code> (float, optional): Default temperature for all generations (0.0-1.0)
- <code>seed</code> (int, optional): Default seed for deterministic outputs (provider support varies)
- <code>**kwargs</code>: Additional session parameters (tools, timeouts, etc.)</p>
<p><strong>Attributes:</strong>
- <code>messages</code> (List[Message]): Conversation history
- <code>provider</code> (AbstractCoreInterface): LLM provider
- <code>system_prompt</code> (str): System prompt</p>
<p><strong>Methods:</strong></p>
<h4 id="generate_1">generate()</h4>
<div class="code-block"><pre><code class="language-python">def generate(self, prompt: str, **kwargs) -&gt; GenerateResponse
</code></pre></div>
<p>Generate response and add to conversation history.</p>
<h4 id="agenerate_1">agenerate()</h4>
<div class="code-block"><pre><code class="language-python">async def agenerate(
    self,
    prompt: str,
    name: Optional[str] = None,
    location: Optional[str] = None,
    **kwargs
) -&gt; Union[GenerateResponse, AsyncIterator[GenerateResponse]]
</code></pre></div>
<p>Async version of <code>generate()</code>. Maintains conversation history with async execution.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">import asyncio

async def chat():
    session = BasicSession(provider=llm)

    # Async conversation
    response1 = await session.agenerate(&quot;My name is Alice&quot;)
    response2 = await session.agenerate(&quot;What's my name?&quot;)

    print(response2.content)  # References Alice

asyncio.run(chat())
</code></pre></div>
<h4 id="add_message">add_message()</h4>
<div class="code-block"><pre><code class="language-python">def add_message(self, role: str, content: str, **metadata) -&gt; Message
</code></pre></div>
<p>Add message to conversation history.</p>
<h4 id="clear_history">clear_history()</h4>
<div class="code-block"><pre><code class="language-python">def clear_history(self, keep_system: bool = True) -&gt; None
</code></pre></div>
<p>Clear conversation history, optionally keeping system prompt.</p>
<h4 id="save">save()</h4>
<div class="code-block"><pre><code class="language-python">def save(self, filepath: Path) -&gt; None
</code></pre></div>
<p>Save session to JSON file.</p>
<h4 id="load">load()</h4>
<div class="code-block"><pre><code class="language-python">@classmethod
def load(cls, filepath: Path, provider: AbstractCoreInterface) -&gt; &quot;BasicSession&quot;
</code></pre></div>
<p>Load session from JSON file.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm, BasicSession

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
session = BasicSession(
    provider=llm,
    system_prompt=&quot;You are a helpful coding tutor.&quot;,
    temperature=0.3,  # Focused responses
    seed=42          # Consistent outputs
)

# Multi-turn conversation
response1 = session.generate(&quot;What are Python decorators?&quot;)
response2 = session.generate(&quot;Show me an example&quot;, temperature=0.7)  # Override for this call

print(f&quot;Conversation has {len(session.messages)} messages&quot;)

# Save session
session.save(Path(&quot;conversation.json&quot;))

# Load later
loaded_session = BasicSession.load(Path(&quot;conversation.json&quot;), llm)
</code></pre></div>
<h3 id="message">Message</h3>
<p>Represents a conversation message.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class Message:
    role: str
    content: str
    timestamp: Optional[datetime] = None
    name: Optional[str] = None
    metadata: Optional[Dict] = None
</code></pre></div>
<p><strong>Methods:</strong></p>
<h4 id="to_dict">to_dict()</h4>
<div class="code-block"><pre><code class="language-python">def to_dict(self) -&gt; Dict
</code></pre></div>
<p>Convert message to dictionary.</p>
<h4 id="from_dict">from_dict()</h4>
<div class="code-block"><pre><code class="language-python">@classmethod
def from_dict(cls, data: Dict) -&gt; &quot;Message&quot;
</code></pre></div>
<p>Create message from dictionary.</p>
<h2 id="event-system">Event System</h2>
<h3 id="eventtype">EventType</h3>
<p>Available event types for monitoring.</p>
<div class="code-block"><pre><code class="language-python">class EventType(Enum):
    # Generation events
    GENERATION_STARTED = &quot;generation_started&quot;
    GENERATION_COMPLETED = &quot;generation_completed&quot;

    # Tool events
    TOOL_STARTED = &quot;tool_started&quot;
    TOOL_PROGRESS = &quot;tool_progress&quot;
    TOOL_COMPLETED = &quot;tool_completed&quot;

    # Error handling
    ERROR = &quot;error&quot;

    # Retry and resilience events
    RETRY_ATTEMPTED = &quot;retry_attempted&quot;
    RETRY_EXHAUSTED = &quot;retry_exhausted&quot;

    # Useful events
    VALIDATION_FAILED = &quot;validation_failed&quot;
    SESSION_CREATED = &quot;session_created&quot;
    SESSION_CLEARED = &quot;session_cleared&quot;
    COMPACTION_STARTED = &quot;compaction_started&quot;
    COMPACTION_COMPLETED = &quot;compaction_completed&quot;

    # Runtime/workflow events
    WORKFLOW_STEP_STARTED = &quot;workflow_step_started&quot;
    WORKFLOW_STEP_COMPLETED = &quot;workflow_step_completed&quot;
    WORKFLOW_STEP_WAITING = &quot;workflow_step_waiting&quot;
    WORKFLOW_STEP_FAILED = &quot;workflow_step_failed&quot;
</code></pre></div>
<h3 id="on_global">on_global()</h3>
<p>Register global event handler.</p>
<div class="code-block"><pre><code class="language-python">def on_global(event_type: EventType, handler: Callable[[Event], None]) -&gt; None
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>event_type</code> (EventType): Event type to listen for
- <code>handler</code> (Callable): Function to call when event occurs</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global

def cost_monitor(event):
    cost = event.data.get(&quot;cost_usd&quot;)
    if cost:
        # NOTE: `cost_usd` is a best-effort estimate based on token usage.
        print(f&quot;Estimated cost: ${cost:.4f}&quot;)

def tool_monitor(event):
    # Tool event payload shape varies by emitter.
    # - Single-tool execution: {&quot;tool_name&quot;: ..., &quot;success&quot;: ..., ...}
    # - Batch execution: {&quot;tool_results&quot;: [{&quot;name&quot;: ..., &quot;success&quot;: ...}, ...], ...}
    tool_name = event.data.get(&quot;tool_name&quot;)
    if tool_name:
        print(f&quot;Tool completed: {tool_name} success={event.data.get('success')}&quot;)
        return

    for r in event.data.get(&quot;tool_results&quot;, []) or []:
        print(f&quot;Tool completed: {r.get('name')} success={r.get('success')} error={r.get('error')}&quot;)

# Register handlers
on_global(EventType.GENERATION_COMPLETED, cost_monitor)
on_global(EventType.TOOL_COMPLETED, tool_monitor)

# Now all LLM operations will trigger these handlers
llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;)
response = llm.generate(&quot;Hello world&quot;)
</code></pre></div>
<h3 id="event">Event</h3>
<p>Event object passed to handlers.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class Event:
    type: EventType
    timestamp: datetime
    data: Dict[str, Any]
    source: Optional[str] = None
</code></pre></div>
<h2 id="retry-configuration">Retry Configuration</h2>
<h3 id="retryconfig">RetryConfig</h3>
<p>Configuration for provider-level retry behavior.</p>
<div class="code-block"><pre><code class="language-python">@dataclass
class RetryConfig:
    max_attempts: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    use_jitter: bool = True
    failure_threshold: int = 5
    recovery_timeout: float = 60.0
    half_open_max_calls: int = 2
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>max_attempts</code> (int): Maximum retry attempts
- <code>initial_delay</code> (float): Initial delay in seconds
- <code>max_delay</code> (float): Maximum delay in seconds
- <code>exponential_base</code> (float): Base for exponential backoff
- <code>use_jitter</code> (bool): Add randomness to delays
- <code>failure_threshold</code> (int): Circuit breaker failure threshold
- <code>recovery_timeout</code> (float): Circuit breaker recovery timeout
- <code>half_open_max_calls</code> (int): Max calls in half-open state</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

config = RetryConfig(
    max_attempts=5,
    initial_delay=2.0,
    use_jitter=True,
    failure_threshold=3
)

llm = create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;, retry_config=config)
</code></pre></div>
<h3 id="feedbackretry">FeedbackRetry</h3>
<p>Retry strategy for structured output validation failures.</p>
<div class="code-block"><pre><code class="language-python">class FeedbackRetry:
    def __init__(self, max_attempts: int = 3):
        self.max_attempts = max_attempts
</code></pre></div>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.structured import FeedbackRetry
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int

custom_retry = FeedbackRetry(max_attempts=5)

user = llm.generate(
    &quot;Extract user: John Doe, 25&quot;,
    response_model=User,
    retry_strategy=custom_retry
)
</code></pre></div>
<h2 id="embeddings">Embeddings</h2>
<h3 id="embeddingmanager">EmbeddingManager</h3>
<p>Manages text embeddings using SOTA models.</p>
<div class="code-block"><pre><code class="language-python">class EmbeddingManager:
    def __init__(
        self,
        model: str = &quot;embeddinggemma&quot;,
        backend: str = &quot;auto&quot;,
        output_dims: Optional[int] = None,
        cache_size: int = 1000,
        cache_dir: Optional[str] = None
    ):
</code></pre></div>
<p><strong>Parameters:</strong>
- <code>model</code> (str): Model name ("embeddinggemma", "granite", "stella-400m")
- <code>backend</code> (str): Backend ("auto", "pytorch", "onnx")
- <code>output_dims</code> (int, optional): Truncate output dimensions
- <code>cache_size</code> (int): Memory cache size
- <code>cache_dir</code> (str, optional): Disk cache directory</p>
<p><strong>Methods:</strong></p>
<h4 id="embed">embed()</h4>
<div class="code-block"><pre><code class="language-python">def embed(self, text: str) -&gt; List[float]
</code></pre></div>
<p>Generate embedding for single text.</p>
<h4 id="embed_batch">embed_batch()</h4>
<div class="code-block"><pre><code class="language-python">def embed_batch(self, texts: List[str]) -&gt; List[List[float]]
</code></pre></div>
<p>Generate embeddings for multiple texts (more efficient).</p>
<h4 id="compute_similarity">compute_similarity()</h4>
<div class="code-block"><pre><code class="language-python">def compute_similarity(self, text1: str, text2: str) -&gt; float
</code></pre></div>
<p>Compute cosine similarity between two texts.</p>
<p><strong>Example:</strong></p>
<div class="code-block"><pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager

embedder = EmbeddingManager(model=&quot;embeddinggemma&quot;)

# Single embedding
embedding = embedder.embed(&quot;Hello world&quot;)
print(f&quot;Embedding dimension: {len(embedding)}&quot;)

# Batch embeddings
embeddings = embedder.embed_batch([&quot;Hello&quot;, &quot;World&quot;, &quot;AI&quot;])

# Similarity
similarity = embedder.compute_similarity(&quot;cat&quot;, &quot;kitten&quot;)
print(f&quot;Similarity: {similarity:.3f}&quot;)
</code></pre></div>
<h2 id="exceptions">Exceptions</h2>
<h3 id="base-exceptions">Base Exceptions</h3>
<h4 id="abstractcoreerror">AbstractCoreError</h4>
<div class="code-block"><pre><code class="language-python">class AbstractCoreError(Exception):
    &quot;&quot;&quot;Base exception for AbstractCore.&quot;&quot;&quot;
</code></pre></div>
<h4 id="providerapierror">ProviderAPIError</h4>
<div class="code-block"><pre><code class="language-python">class ProviderAPIError(AbstractCoreError):
    &quot;&quot;&quot;Provider API error.&quot;&quot;&quot;
</code></pre></div>
<h4 id="modelnotfounderror">ModelNotFoundError</h4>
<div class="code-block"><pre><code class="language-python">class ModelNotFoundError(AbstractCoreError):
    &quot;&quot;&quot;Model not found error.&quot;&quot;&quot;
</code></pre></div>
<h4 id="authenticationerror">AuthenticationError</h4>
<div class="code-block"><pre><code class="language-python">class AuthenticationError(ProviderAPIError):
    &quot;&quot;&quot;Authentication error.&quot;&quot;&quot;
</code></pre></div>
<h4 id="ratelimiterror">RateLimitError</h4>
<div class="code-block"><pre><code class="language-python">class RateLimitError(ProviderAPIError):
    &quot;&quot;&quot;Rate limit error.&quot;&quot;&quot;
</code></pre></div>
<h3 id="usage">Usage</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.exceptions import ProviderAPIError, RateLimitError

try:
    response = llm.generate(&quot;Hello world&quot;)
except RateLimitError:
    print(&quot;Rate limited, wait and retry&quot;)
except ProviderAPIError as e:
    print(f&quot;API error: {e}&quot;)
except Exception as e:
    print(f&quot;Unexpected error: {e}&quot;)
</code></pre></div>
<h2 id="advanced-usage-patterns">Advanced Usage Patterns</h2>
<h3 id="custom-provider-configuration">Custom Provider Configuration</h3>
<div class="code-block"><pre><code class="language-python"># Provider with all options
llm = create_llm(
    provider=&quot;openai&quot;,
    model=&quot;gpt-4o-mini&quot;,
    api_key=&quot;your-key&quot;,
    temperature=0.7,
    max_tokens=1000,
    top_p=0.9,
    timeout=30,
    retry_config=RetryConfig(max_attempts=5)
)
</code></pre></div>
<h3 id="multi-provider-setup">Multi-Provider Setup</h3>
<div class="code-block"><pre><code class="language-python">providers = {
    &quot;fast&quot;: create_llm(&quot;openai&quot;, model=&quot;gpt-4o-mini&quot;),
    &quot;smart&quot;: create_llm(&quot;openai&quot;, model=&quot;gpt-4o&quot;),
    &quot;long_context&quot;: create_llm(&quot;anthropic&quot;, model=&quot;claude-haiku-4-5&quot;),
    &quot;local&quot;: create_llm(&quot;ollama&quot;, model=&quot;qwen2.5-coder:7b&quot;)
}

def route_request(prompt, task_type=&quot;general&quot;):
    if task_type == &quot;simple&quot;:
        return providers[&quot;fast&quot;].generate(prompt)
    elif task_type == &quot;complex&quot;:
        return providers[&quot;smart&quot;].generate(prompt)
    elif len(prompt) &gt; 50000:
        return providers[&quot;long_context&quot;].generate(prompt)
    else:
        return providers[&quot;local&quot;].generate(prompt)
</code></pre></div>
<h3 id="production-monitoring">Production Monitoring</h3>
<div class="code-block"><pre><code class="language-python">from abstractcore.events import EventType, on_global
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cost tracking
total_cost = 0.0

def production_monitor(event):
    global total_cost

    if event.type == EventType.GENERATION_COMPLETED:
        cost = event.data.get(&quot;cost_usd&quot;)
        if cost:
            # NOTE: `cost_usd` is a best-effort estimate based on token usage.
            total_cost += float(cost)
            logger.info(f&quot;Estimated cost: ${float(cost):.4f}, Total: ${total_cost:.4f}&quot;)

        duration_ms = event.data.get(&quot;duration_ms&quot;)
        if isinstance(duration_ms, (int, float)) and duration_ms &gt; 10_000:
            logger.warning(f&quot;Slow request: {float(duration_ms):.0f}ms&quot;)

    elif event.type == EventType.ERROR:
        logger.error(f&quot;Error: {event.data.get('error')}&quot;)

    elif event.type == EventType.RETRY_ATTEMPTED:
        logger.info(f&quot;Retrying due to: {event.data.get('error_type')}&quot;)

on_global(EventType.GENERATION_COMPLETED, production_monitor)
on_global(EventType.ERROR, production_monitor)
on_global(EventType.RETRY_ATTEMPTED, production_monitor)
</code></pre></div>
<hr />
<p>For more examples and use cases, see:
- <a href="getting-started.html">Getting Started</a> - Basic setup and usage
- <a href="examples.html">Examples</a> - Practical use cases
- <a href="prerequisites.html">Prerequisites</a> - Provider setup and configuration
- <a href="capabilities.html">Capabilities</a> - What AbstractCore can do</p>

            </div>
        </div>
    </main>

    <!-- Scripts -->
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
