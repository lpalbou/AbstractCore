<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Reference - AbstractCore</title>
    <meta name="description" content="Complete Python API reference for AbstractCore. All examples work across any provider.">
    <link rel="icon" type="image/x-icon" href="../assets/favicon.ico">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">
                <a href="../index.html" class="brand-link">
                    <img src="../assets/logo.svg" alt="AbstractCore" class="brand-logo">
                    <span class="brand-text">AbstractCore</span>
                </a>
            </div>
            
            <div class="nav-menu" id="nav-menu">
                <a href="../index.html#features" class="nav-link">Features</a>
                <a href="../index.html#quickstart" class="nav-link">Quick Start</a>
                <a href="../index.html#docs" class="nav-link">Documentation</a>
                <a href="../index.html#examples" class="nav-link">Examples</a>
                <a href="https://github.com/lpalbou/AbstractCore" class="nav-link" target="_blank">
                    <svg class="github-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                    </svg>
                    GitHub
                </a>
            </div>
            
            <div class="nav-toggle" id="nav-toggle">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1000px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">API Reference</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Complete reference for the AbstractCore API. All examples work across any provider.
                </p>
            </div>

            <!-- Table of Contents -->
            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;">
                <h2 style="margin: 0 0 1rem 0;">Table of Contents</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                    <div>
                        <a href="#core-functions" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Core Functions</a>
                        <a href="#classes" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Classes</a>
                        <a href="#event-system" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Event System</a>
                    </div>
                    <div>
                        <a href="#retry-configuration" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Retry Configuration</a>
                        <a href="#embeddings" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Embeddings</a>
                        <a href="#exceptions" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Exceptions</a>
                    </div>
                </div>
            </div>

            <div class="doc-content">
                <section id="core-functions">
                    <h2>Core Functions</h2>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">create_llm()</h3>
                        <p>Creates an LLM provider instance using the factory pattern.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">def create_llm(
    provider: str,
    model: Optional[str] = None,
    retry_config: Optional[RetryConfig] = None,
    **kwargs
) -> AbstractLLMInterface</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>provider</code> (str): Provider name ("openai", "anthropic", "ollama", "mlx", "lmstudio", "huggingface")</li>
                                <li><code>model</code> (str, optional): Model name. If not provided, uses provider default</li>
                                <li><code>retry_config</code> (RetryConfig, optional): Custom retry configuration</li>
                                <li><code>**kwargs</code>: Provider-specific parameters</li>
                            </ul>
                        </div>

                        <h4>Provider-specific Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>api_key</code> (str): API key for cloud providers</li>
                                <li><code>base_url</code> (str): Custom endpoint URL</li>
                                <li><code>temperature</code> (float): Sampling temperature (0-2)</li>
                                <li><code>max_tokens</code> (int): Maximum total tokens (unified across providers)</li>
                                <li><code>max_output_tokens</code> (int): Maximum output tokens (unified parameter)</li>
                                <li><code>timeout</code> (int): Request timeout in seconds</li>
                                <li><code>top_p</code> (float): Nucleus sampling parameter</li>
                            </ul>
                        </div>

                        <h4>Returns</h4>
                        <p><code>AbstractLLMInterface</code> instance</p>

                        <h4>Example</h4>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractllm import create_llm

# Basic usage
llm = create_llm("openai", model="gpt-4o-mini")

# With unified token management
llm = create_llm(
    "anthropic",
    model="claude-3-5-haiku-latest",
    temperature=0.7,
    max_tokens=8000,        # Total budget
    max_output_tokens=2000, # Reserve for output
    timeout=30
)

# Local provider
llm = create_llm("ollama", model="qwen2.5-coder:7b", base_url="http://localhost:11434")</code></pre>
                        </div>
                    </div>
                </section>

                <section id="classes">
                    <h2>Classes</h2>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">AbstractLLMInterface</h3>
                        <p>Base interface for all LLM providers. All providers implement this interface.</p>
                        
                        <h4>generate()</h4>
                        <p>Generate text response from the LLM.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">def generate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[List[Callable]] = None,
    response_model: Optional[BaseModel] = None,
    retry_strategy: Optional[Retry] = None,
    stream: bool = False,
    **kwargs
) -> Union[GenerateResponse, Iterator[GenerateResponse]]</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>prompt</code> (str): Text prompt to generate from</li>
                                <li><code>messages</code> (List[Dict], optional): Conversation messages in OpenAI format</li>
                                <li><code>system_prompt</code> (str, optional): System prompt to set context</li>
                                <li><code>tools</code> (List[Callable], optional): Functions the LLM can call (using @tool decorator)</li>
                                <li><code>response_model</code> (BaseModel, optional): Pydantic model for structured output</li>
                                <li><code>retry_strategy</code> (Retry, optional): Custom retry strategy for structured output</li>
                                <li><code>stream</code> (bool): Enable streaming response</li>
                                <li><code>**kwargs</code>: Additional generation parameters</li>
                            </ul>
                        </div>

                        <h4>Returns</h4>
                        <ul>
                            <li>If <code>stream=False</code>: GenerateResponse</li>
                            <li>If <code>stream=True</code>: Iterator[GenerateResponse]</li>
                        </ul>

                        <h4>unload()</h4>
                        <p>Unload model from memory (local providers only).</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python"># Local providers support memory management
llm = create_llm("ollama", model="large-model")
response = llm.generate("Hello")
llm.unload()  # Explicitly free memory
del llm

# API providers (OpenAI, Anthropic) - unload() is safe but no-op
api_llm = create_llm("openai", model="gpt-4o-mini")
api_llm.unload()  # Safe to call, but has no effect</code></pre>
                        </div>
                    </div>

                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">GenerateResponse</h3>
                        <p>Response object from LLM generation.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">@dataclass
class GenerateResponse:
    content: Optional[str]          # Generated text content
    raw_response: Any              # Raw provider response
    model: Optional[str]           # Model used for generation
    finish_reason: Optional[str]   # Why generation stopped
    usage: Optional[Dict[str, int]] # Token usage information
    tool_calls: Optional[List[Dict]] # Tools called by the LLM
    metadata: Optional[Dict]       # Additional metadata</code></pre>
                        </div>

                        <h4>Attributes</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>content</code> (str): Generated text content</li>
                                <li><code>raw_response</code> (Any): Raw provider response</li>
                                <li><code>model</code> (str): Model used for generation</li>
                                <li><code>finish_reason</code> (str): Why generation stopped ("stop", "length", "tool_calls")</li>
                                <li><code>usage</code> (Dict): Token usage information</li>
                                <li><code>tool_calls</code> (List[Dict]): Tools called by the LLM</li>
                                <li><code>metadata</code> (Dict): Additional metadata</li>
                            </ul>
                        </div>
                    </div>

                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">BasicSession</h3>
                        <p>Manages conversation context and history.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">class BasicSession:
    def __init__(
        self,
        provider: AbstractLLMInterface,
        system_prompt: Optional[str] = None
    ):</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>provider</code> (AbstractLLMInterface): LLM provider instance</li>
                                <li><code>system_prompt</code> (str, optional): System prompt for the conversation</li>
                            </ul>
                        </div>

                        <h4>Methods</h4>
                        
                        <h5>generate()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def generate(self, prompt: str, **kwargs) -> GenerateResponse</code></pre>
                        </div>
                        <p>Generate response and add to conversation history.</p>

                        <h5>add_message()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def add_message(self, role: str, content: str, **metadata) -> Message</code></pre>
                        </div>
                        <p>Add message to conversation history with optional metadata.</p>

                        <h5>save()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def save(self, filepath: Path, summary: bool = False, assessment: bool = False, facts: bool = False) -> None</code></pre>
                        </div>
                        <p>Save session to JSON file with optional analytics.</p>

                        <h5>load()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">@classmethod
def load(cls, filepath: Path, provider: AbstractLLMInterface) -> "BasicSession"</code></pre>
                        </div>
                        <p>Load session from JSON file.</p>

                        <h4>Example</h4>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractllm import create_llm, BasicSession

llm = create_llm("openai", model="gpt-4o-mini")
session = BasicSession(
    provider=llm,
    system_prompt="You are a helpful coding tutor."
)

# Multi-turn conversation with memory
response1 = session.generate("My name is Alice and I'm learning Python.")
response2 = session.generate("What's my name and what am I learning?")
# Output: Your name is Alice and you're learning Python.

# Save with analytics
session.save(
    'tutoring_session.json',
    summary=True,      # Generate conversation summary
    assessment=True,   # Assess learning progress
    facts=True        # Extract key facts learned
)

# Load and continue later
loaded_session = BasicSession.load('tutoring_session.json', provider=llm)</code></pre>
                        </div>
                    </div>
                </section>

                <section id="event-system">
                    <h2>Event System</h2>
                    <p>Comprehensive observability and control through events.</p>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">Event Types</h3>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>GENERATION_STARTED</code>: Before LLM generation begins</li>
                                <li><code>GENERATION_COMPLETED</code>: After LLM generation completes</li>
                                <li><code>TOOL_STARTED</code>: Before tool execution (preventable)</li>
                                <li><code>TOOL_COMPLETED</code>: After tool execution completes</li>
                                <li><code>ERROR_OCCURRED</code>: When an error occurs</li>
                                <li><code>RETRY_ATTEMPTED</code>: When a retry is attempted</li>
                            </ul>
                        </div>

                        <h3 style="margin: 2rem 0 1rem 0;">Event Registration</h3>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractllm.events import EventType, on_global

# Cost monitoring
def monitor_costs(event):
    if event.cost_usd and event.cost_usd > 0.10:
        alert(f"High cost request: ${event.cost_usd}")

# Security control
def prevent_dangerous_tools(event):
    for call in event.data.get('tool_calls', []):
        if call.name in ['delete_file', 'system_command']:
            event.prevent()  # Stop tool execution

# Performance tracking
def track_performance(event):
    if event.duration_ms > 10000:
        log(f"Slow request: {event.duration_ms}ms")

# Register event handlers
on_global(EventType.GENERATION_COMPLETED, monitor_costs)
on_global(EventType.TOOL_STARTED, prevent_dangerous_tools)
on_global(EventType.GENERATION_COMPLETED, track_performance)</code></pre>
                        </div>
                    </div>
                </section>

                <section id="retry-configuration">
                    <h2>Retry Configuration</h2>
                    <p>Production-grade error handling with multiple layers.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractllm import create_llm
from abstractllm.core.retry import RetryConfig

config = RetryConfig(
    max_attempts=3,           # Try up to 3 times
    initial_delay=1.0,        # Start with 1 second delay
    max_delay=60.0,           # Cap at 1 minute
    use_jitter=True,          # Add randomness
    failure_threshold=5,      # Circuit breaker after 5 failures
    recovery_timeout=60.0     # Test recovery after 1 minute
)

llm = create_llm("openai", model="gpt-4o-mini", retry_config=config)</code></pre>
                    </div>
                </section>

                <section id="embeddings">
                    <h2>Embeddings</h2>
                    <p>Built-in support for semantic search and RAG applications.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractllm.embeddings import EmbeddingManager
from abstractllm import create_llm

# Initialize components
embedder = EmbeddingManager()
llm = create_llm("openai", model="gpt-4o-mini")

# Documents to search
documents = [
    "Python is great for data science and machine learning.",
    "JavaScript powers modern web applications.",
    "Rust ensures memory safety without garbage collection."
]

# Create embeddings
doc_embeddings = embedder.embed_batch(documents)

# User query
query = "Tell me about web development"
query_embedding = embedder.embed(query)

# Find most similar document
similarities = [
    embedder.compute_similarity(query_embedding, doc_emb)
    for doc_emb in doc_embeddings
]

best_doc_idx = similarities.index(max(similarities))
context = documents[best_doc_idx]

# Generate response with context
response = llm.generate(
    f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
)
print(response.content)</code></pre>
                    </div>
                </section>

                <section id="exceptions">
                    <h2>Exceptions</h2>
                    <p>Comprehensive exception hierarchy for robust applications.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractllm.exceptions import (
    ModelNotFoundError,
    ProviderAPIError,
    AuthenticationError,
    RateLimitError,
    TimeoutError,
    ToolExecutionError
)

try:
    response = llm.generate("Hello", tools=[some_tool])
except AuthenticationError:
    print("Invalid API key")
except RateLimitError:
    print("Rate limit exceeded, retrying...")
except ModelNotFoundError:
    print("Model not available")
except ToolExecutionError as e:
    print(f"Tool execution failed: {e.tool_name}")
except ProviderAPIError as e:
    print(f"Provider error: {e}")</code></pre>
                    </div>
                </section>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Related Documentation</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="getting-started.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Getting Started</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">5-minute quick start guide</p>
                        </a>
                        
                        <a href="tool-calling.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Tool Calling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Universal tool system guide</p>
                        </a>
                        
                        <a href="examples.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Examples</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Real-world usage examples</p>
                        </a>
                        
                        <a href="architecture.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Architecture</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">System design overview</p>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
