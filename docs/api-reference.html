<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Reference - AbstractCore</title>
    <meta name="description" content="Complete Python API reference for AbstractCore. All examples work across any provider.">
    <link rel="icon" type="image/svg+xml" href="../assets/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    
    <!-- Navbar Component -->
    <script src="../assets/js/navbar-component.js"></script>
</head>
<body>
    <!-- Navigation -->
    <div class="navbar-placeholder"></div>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            createNavbar({
                basePath: '',
                menuItems: [
                    { text: 'Features', href: '/docs/capabilities.html' },
                    { text: 'Quick Start', href: '/docs/getting-started.html' },
                    { text: 'Documentation', href: '/#docs' },
                    { text: 'Examples', href: '/docs/examples.html' },
                    {
                        text: 'GitHub',
                        href: 'https://github.com/lpalbou/abstractcore',
                        target: '_blank',
                        icon: 'github'
                    },
                    {
                        text: 'PyPI',
                        href: 'https://pypi.org/project/abstractcore/',
                        target: '_blank',
                        icon: 'pypi'
                    }
                ]
            });
        });
    </script>

    <!-- Main Content -->
    <main style="padding-top: 5rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="doc-header" style="margin-bottom: 3rem;">
                <h1 style="font-size: 2.5rem; margin-bottom: 1rem;">API Reference</h1>
                <p style="font-size: 1.25rem; color: var(--text-secondary);">
                    Complete reference for the AbstractCore API. All examples work across any provider.
                </p>
            </div>

            <!-- Table of Contents -->
            <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin-bottom: 3rem;">
                <h2 style="margin: 0 0 1rem 0;">Table of Contents</h2>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                    <div>
                        <a href="#core-functions" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Core Functions</a>
                        <a href="#classes" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Classes</a>
                        <a href="#event-system" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Event System</a>
                    </div>
                    <div>
                        <a href="#retry-configuration" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Retry Configuration</a>
                        <a href="#embeddings" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Embeddings</a>
                        <a href="#exceptions" style="display: block; padding: 0.5rem 0; color: var(--primary-color);">Exceptions</a>
                    </div>
                </div>
            </div>

            <div class="doc-content">
                <section id="core-functions">
                    <h2>Core Functions</h2>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">create_llm()</h3>
                        <p>Creates an LLM provider instance using the factory pattern.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">def create_llm(
    provider: str,
    model: Optional[str] = None,
    retry_config: Optional[RetryConfig] = None,
    **kwargs
) -> AbstractCoreInterface</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>provider</code> (str): Provider name ("openai", "anthropic", "openrouter", "ollama", "lmstudio", "mlx", "huggingface", "vllm", "openai-compatible")</li>
                                <li><code>model</code> (str, optional): Model name. If not provided, uses provider default</li>
                                <li><code>retry_config</code> (RetryConfig, optional): Custom retry configuration</li>
                                <li><code>tool_call_tags</code> (str or tuple, optional): Custom tool call format for agent CLI compatibility</li>
                                <li><code>**kwargs</code>: Provider-specific parameters</li>
                            </ul>
                        </div>

                        <h4>Unified Token Management Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>api_key</code> (str): API key for cloud providers</li>
                                <li><code>base_url</code> (str): Custom endpoint URL</li>
                                <li><code>temperature</code> (float): Sampling temperature (0-2)</li>
                                <li><code>seed</code> (int): Random seed for deterministic generation (supported by most providers)</li>
                                <li><code>max_tokens</code> (int): Context window (input + output) - unified across providers</li>
                                <li><code>max_output_tokens</code> (int): Maximum output tokens - unified parameter</li>
                                <li><code>max_input_tokens</code> (int): Maximum input tokens (auto-calculated if not set)</li>
                                <li><code>timeout</code> (int): Request timeout in seconds</li>
                                <li><code>top_p</code> (float): Nucleus sampling parameter</li>
                            </ul>
                        </div>

                        <h4>Returns</h4>
                        <p><code>AbstractCoreInterface</code> instance</p>

                        <h4>Example</h4>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractcore import create_llm

# Basic usage
llm = create_llm("openai", model="gpt-5-mini")

# With unified token management and deterministic generation
llm = create_llm(
    "anthropic",
    model="claude-haiku-4-5",
    temperature=0.7,
    seed=42,                    # For deterministic outputs (issues warning on Anthropic)
    max_tokens=32000,           # Context window (input + output)
    max_output_tokens=8000,     # Maximum output tokens
    max_input_tokens=24000,     # Maximum input tokens (auto-calculated if not set)
    timeout=30
)

# With tool syntax rewriting (keep tool-call markup in content)
llm = create_llm(
    "ollama",
    model="qwen3:4b-instruct",
    tool_call_tags="llama3"  # <function_call>...JSON...</function_call>
)

# Local provider
llm = create_llm("ollama", model="qwen3:4b-instruct", base_url="http://localhost:11434")</code></pre>
                        </div>
                    </div>
                </section>

                <section id="classes">
                    <h2>Classes</h2>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">AbstractCoreInterface</h3>
                        <p>Base interface for all LLM providers. All providers implement this interface.</p>
                        
                        <h4>generate()</h4>
                        <p>Generate text response from the LLM with optional media attachments.</p>

                        <div class="code-block">
                            <pre><code class="language-python">def generate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    media: Optional[List[str]] = None,
    tools: Optional[List[Callable]] = None,
    response_model: Optional[BaseModel] = None,
    retry_strategy: Optional[Retry] = None,
    stream: bool = False,
    **kwargs
) -> Union[GenerateResponse, Iterator[GenerateResponse]]</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>prompt</code> (str): Text prompt to generate from</li>
                                <li><code>messages</code> (List[Dict], optional): Conversation messages in OpenAI format</li>
                                <li><code>system_prompt</code> (str, optional): System prompt to set context</li>
                                <li><code>media</code> (List[str], optional): File paths to attach (images, PDFs, documents, data files)</li>
                                <li><code>tools</code> (List[Callable], optional): Functions the LLM can call (using @tool decorator)</li>
                                <li><code>response_model</code> (BaseModel, optional): Pydantic model for structured output</li>
                                <li><code>retry_strategy</code> (Retry, optional): Custom retry strategy for structured output</li>
                                <li><code>stream</code> (bool): Enable streaming response</li>
                                <li><code>**kwargs</code>: Additional generation parameters</li>
                            </ul>
                        </div>

                        <h4>Media Handling Example</h4>
                        <div class="code-block">
                            <pre><code class="language-python"># Attach images, documents, and data files
from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o")

# Single image
response = llm.generate(
    "What's in this image?",
    media=["photo.jpg"]
)

# Multiple files (images, PDFs, Office docs, data)
response = llm.generate(
    "Compare the chart with the data and summarize the document",
    media=["chart.png", "data.csv", "report.pdf"]
)

# Supported file types:
# Images: PNG, JPEG, GIF, WEBP, BMP, TIFF
# Documents: PDF, DOCX, XLSX, PPTX
# Data: CSV, TSV, TXT, MD, JSON</code></pre>
                        </div>

                        <h4>Returns</h4>
                        <ul>
                            <li>If <code>stream=False</code>: GenerateResponse</li>
                            <li>If <code>stream=True</code>: Iterator[GenerateResponse]</li>
                        </ul>

                        <h4>agenerate()</h4>
                        <p>Async version of <code>generate()</code> (supports async streaming when <code>stream=True</code>).</p>

                        <div class="code-block">
                            <pre><code class="language-python">async def agenerate(
    self,
    prompt: str,
    messages: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    media: Optional[List[str]] = None,
    tools: Optional[List[Callable]] = None,
    response_model: Optional[BaseModel] = None,
    retry_strategy: Optional[Retry] = None,
    stream: bool = False,
    **kwargs
) -> Union[GenerateResponse, AsyncIterator[GenerateResponse]]</code></pre>
                        </div>

                        <div class="code-block">
                            <pre><code class="language-python">import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm("openai", model="gpt-5-mini")
    response = await llm.agenerate("Explain async/await in one paragraph.")
    print(response.content)

asyncio.run(main())</code></pre>
                        </div>

                        <div style="background: #dbeafe; border-left: 4px solid #3b82f6; padding: 1rem; margin: 1rem 0;">
                            <p style="margin: 0; color: #1e40af;"><strong>See also:</strong> <a href="async-guide.html">Async Guide</a> for patterns like <code>asyncio.gather()</code> and async streaming loops.</p>
                        </div>

                        <h4>unload_model(model_name)</h4>
                        <p>Unload/cleanup resources for a specific model (best-effort).</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python"># Local providers support memory management
llm = create_llm("ollama", model="large-model")
response = llm.generate("Hello")
llm.unload_model(llm.model)  # Explicitly free memory
del llm

# API providers (OpenAI, Anthropic) - unload_model() is safe but usually no-op
api_llm = create_llm("openai", model="gpt-5-mini")
api_llm.unload_model(api_llm.model)  # Safe to call, but has no effect</code></pre>
                        </div>
                    </div>

                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">GenerateResponse</h3>
                        <p>Response object from LLM generation.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">@dataclass
class GenerateResponse:
    content: Optional[str]          # Generated text content
    raw_response: Any              # Raw provider response
    model: Optional[str]           # Model used for generation
    finish_reason: Optional[str]   # Why generation stopped
    usage: Optional[Dict[str, int]] # Token usage information
    tool_calls: Optional[List[Dict]] # Tools called by the LLM
    metadata: Optional[Dict]       # Additional metadata
    
    # NEW in v2.4.7: Consistent token access across ALL providers
    input_tokens: int              # Input tokens (always available)
    output_tokens: int             # Output tokens (always available)
    total_tokens: int              # Total tokens (always available)
    gen_time: float                # Generation time in ms (always available)</code></pre>
                        </div>

                        <h4>Attributes</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>content</code> (str): Generated text content</li>
                                <li><code>raw_response</code> (Any): Raw provider response</li>
                                <li><code>model</code> (str): Model used for generation</li>
                                <li><code>finish_reason</code> (str): Why generation stopped ("stop", "length", "tool_calls")</li>
                                <li><code>usage</code> (Dict): Token usage information (provider-specific format)</li>
                                <li><code>tool_calls</code> (List[Dict]): Tools called by the LLM</li>
                                <li><code>metadata</code> (Dict): Additional metadata</li>
                                <li><strong>NEW v2.4.7:</strong> <code>input_tokens</code> (int): Input tokens (consistent across all providers)</li>
                                <li><strong>NEW v2.4.7:</strong> <code>output_tokens</code> (int): Output tokens (consistent across all providers)</li>
                                <li><strong>NEW v2.4.7:</strong> <code>total_tokens</code> (int): Total tokens (consistent across all providers)</li>
                                <li><strong>NEW v2.4.7:</strong> <code>gen_time</code> (float): Generation time in milliseconds (consistent across all providers)</li>
                            </ul>
                        </div>

                        <h4>Token Count Sources (NEW in v2.4.7)</h4>
                        <div style="background: var(--background-secondary); padding: 1.5rem; border-radius: 0.75rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><strong>Provider APIs:</strong> OpenAI, Anthropic, LMStudio (native API token counts)</li>
                                <li><strong>AbstractCore Calculation:</strong> MLX, HuggingFace, Mock (using token_utils.py)</li>
                                <li><strong>Mixed Sources:</strong> Ollama (combination of provider and calculated tokens)</li>
                            </ul>
                        </div>

                        <h4>Methods</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>get_summary()</code> (str): Returns comprehensive summary with timing: "Model: gpt-5-mini | Tokens: 117 | Time: 1234.5ms"</li>
                            </ul>
                        </div>
                    </div>

                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">BasicSession</h3>
                        <p>Manages conversation context and history.</p>
                        
                        <div class="code-block">
                            <pre><code class="language-python">class BasicSession:
    def __init__(
        self,
        provider: AbstractCoreInterface,
        system_prompt: Optional[str] = None
    ):</code></pre>
                        </div>

                        <h4>Parameters</h4>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>provider</code> (AbstractCoreInterface): LLM provider instance</li>
                                <li><code>system_prompt</code> (str, optional): System prompt for the conversation</li>
                            </ul>
                        </div>

                        <h4>Methods</h4>
                        
                        <h5>generate()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def generate(self, prompt: str, **kwargs) -> GenerateResponse</code></pre>
                        </div>
                        <p>Generate response and add to conversation history.</p>

                        <h5>agenerate()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">async def agenerate(self, prompt: str, **kwargs) -> GenerateResponse</code></pre>
                        </div>
                        <p>Async version of <code>generate()</code> (use for concurrent sessions/calls).</p>

                        <h5>add_message()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def add_message(self, role: str, content: str, **metadata) -> Message</code></pre>
                        </div>
                        <p>Add message to conversation history with optional metadata.</p>

                        <h5>save()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">def save(self, filepath: Path, summary: bool = False, assessment: bool = False, facts: bool = False) -> None</code></pre>
                        </div>
                        <p>Save session to JSON file with optional analytics.</p>

                        <h5>load()</h5>
                        <div class="code-block">
                            <pre><code class="language-python">@classmethod
def load(cls, filepath: Path, provider: AbstractCoreInterface) -> "BasicSession"</code></pre>
                        </div>
                        <p>Load session from JSON file.</p>

                        <h4>Example</h4>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractcore import create_llm, BasicSession

llm = create_llm("openai", model="gpt-5-mini")
session = BasicSession(
    provider=llm,
    system_prompt="You are a helpful coding tutor."
)

# Multi-turn conversation with memory
response1 = session.generate("My name is Alice and I'm learning Python.")
response2 = session.generate("What's my name and what am I learning?")
# Output: Your name is Alice and you're learning Python.

# Save with analytics
session.save(
    'tutoring_session.json',
    summary=True,      # Generate conversation summary
    assessment=True,   # Assess learning progress
    facts=True        # Extract key facts learned
)

# Load and continue later
loaded_session = BasicSession.load('tutoring_session.json', provider=llm)</code></pre>
                        </div>
                    </div>
                </section>

                <section id="event-system">
                    <h2>Event System</h2>
                    <p>Comprehensive observability and control through events.</p>
                    
                    <div style="background: var(--background-secondary); padding: 2rem; border-radius: 0.75rem; margin: 2rem 0;">
                        <h3 style="margin: 0 0 1rem 0;">Event Types</h3>
                        <div style="background: var(--background); padding: 1.5rem; border-radius: 0.5rem; margin: 1rem 0;">
                            <ul style="margin: 0;">
                                <li><code>GENERATION_STARTED</code>: Before LLM generation begins</li>
                                <li><code>GENERATION_COMPLETED</code>: After LLM generation completes</li>
                                <li><code>TOOL_STARTED</code>: Before tool execution (preventable)</li>
                                <li><code>TOOL_COMPLETED</code>: After tool execution completes</li>
                                <li><code>ERROR_OCCURRED</code>: When an error occurs</li>
                                <li><code>RETRY_ATTEMPTED</code>: When a retry is attempted</li>
                            </ul>
                        </div>

                        <h3 style="margin: 2rem 0 1rem 0;">Event Registration</h3>
                        <div class="code-block">
                            <pre><code class="language-python">from abstractcore.events import EventType, on_global

# Cost monitoring
def monitor_costs(event):
    if event.cost_usd and event.cost_usd > 0.10:
        alert(f"High cost request: ${event.cost_usd}")

# Security control
def prevent_dangerous_tools(event):
    for call in event.data.get('tool_calls', []):
        if call.name in ['delete_file', 'system_command']:
            event.prevent()  # Stop tool execution

# Performance tracking
def track_performance(event):
    if event.duration_ms > 10000:
        log(f"Slow request: {event.duration_ms}ms")

# Register event handlers
on_global(EventType.GENERATION_COMPLETED, monitor_costs)
on_global(EventType.TOOL_STARTED, prevent_dangerous_tools)
on_global(EventType.GENERATION_COMPLETED, track_performance)</code></pre>
                        </div>
                    </div>
                </section>

                <section id="retry-configuration">
                    <h2>Retry Configuration</h2>
                    <p>Production-grade error handling with multiple layers.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

config = RetryConfig(
    max_attempts=3,           # Try up to 3 times
    initial_delay=1.0,        # Start with 1 second delay
    max_delay=60.0,           # Cap at 1 minute
    use_jitter=True,          # Add randomness
    failure_threshold=5,      # Circuit breaker after 5 failures
    recovery_timeout=60.0     # Test recovery after 1 minute
)

llm = create_llm("openai", model="gpt-5-mini", retry_config=config)</code></pre>
                    </div>
                </section>

                <section id="token-management">
                    <h2>Token Management</h2>
                    <p>Unified token parameters and budget validation across all providers.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm
from abstractcore.utils.token_utils import estimate_tokens

# Unified token parameters work across ALL providers
llm = create_llm(
    "anthropic",
    model="claude-haiku-4-5",
    max_tokens=32000,           # Context window (input + output)
    max_output_tokens=8000,     # Maximum output tokens
    max_input_tokens=24000      # Maximum input tokens (auto-calculated if not set)
)

# Token estimation and validation
text = "Your input text here..."
estimated = estimate_tokens(text, model="claude-haiku-4-5")
print(f"Estimated tokens: {estimated}")

# Budget validation with warnings
response = llm.generate("Write a detailed analysis...")
print(f"Input tokens: {response.usage.input_tokens}")
print(f"Output tokens: {response.usage.output_tokens}")
print(f"Cost estimate: ${response.usage.cost_usd:.4f}")</code></pre>
                    </div>
                </section>

                <section id="production-resilience">
                    <h2>Production Resilience</h2>
                    <p>Advanced retry logic, circuit breakers, and event-driven monitoring.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore import create_llm
from abstractcore.resilience import RetryManager, CircuitBreaker
from abstractcore.events import EventType, on_global

# Production resilience with retry and circuit breaker
llm = create_llm(
    "openai",
    model="gpt-5-mini",
    retry_manager=RetryManager(max_attempts=3, backoff_strategy="exponential"),
    circuit_breaker=CircuitBreaker(failure_threshold=5, timeout=60)
)

# Cost monitoring
def cost_monitor(event):
    if event.cost_usd and event.cost_usd > 0.10:
        alert(f"High cost request: ${event.cost_usd}")

on_global(EventType.GENERATION_COMPLETED, cost_monitor)

# Memory management for local models
local_llm = create_llm("ollama", model="large-model")
response = local_llm.generate("Hello")
local_llm.unload_model(local_llm.model)  # Free memory</code></pre>
                    </div>
                </section>

                <section id="embeddings">
                    <h2>Embeddings</h2>
                    <p>Built-in support for semantic search and RAG applications.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.embeddings import EmbeddingManager
from abstractcore import create_llm

# Initialize components
embedder = EmbeddingManager()
llm = create_llm("openai", model="gpt-5-mini")

# Documents to search
documents = [
    "Python is great for data science and machine learning.",
    "JavaScript powers modern web applications.",
    "Rust ensures memory safety without garbage collection."
]

# Create embeddings
doc_embeddings = embedder.embed_batch(documents)

# User query
query = "Tell me about web development"
query_embedding = embedder.embed(query)

# Find most similar document
similarities = [
    embedder.compute_similarity(query_embedding, doc_emb)
    for doc_emb in doc_embeddings
]

best_doc_idx = similarities.index(max(similarities))
context = documents[best_doc_idx]

# Generate response with context
response = llm.generate(
    f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
)
print(response.content)</code></pre>
                    </div>
                </section>

                <section id="exceptions">
                    <h2>Exceptions</h2>
                    <p>Comprehensive exception hierarchy for robust applications.</p>
                    
                    <div class="code-block">
                        <pre><code class="language-python">from abstractcore.exceptions import (
    ModelNotFoundError,
    ProviderAPIError,
    AuthenticationError,
    RateLimitError,
    TimeoutError,
    ToolExecutionError
)

try:
    response = llm.generate("Hello", tools=[some_tool])
except AuthenticationError:
    print("Invalid API key")
except RateLimitError:
    print("Rate limit exceeded, retrying...")
except ModelNotFoundError:
    print("Model not available")
except ToolExecutionError as e:
    print(f"Tool execution failed: {e.tool_name}")
except ProviderAPIError as e:
    print(f"Provider error: {e}")</code></pre>
                    </div>
                </section>

                <!-- Related Documentation -->
                <div style="margin-top: 4rem; padding: 2rem; background: var(--background-secondary); border-radius: 0.75rem;">
                    <h2 style="margin: 0 0 1.5rem 0;">Related Documentation</h2>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem;">
                        <a href="getting-started.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Getting Started</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">5-minute quick start guide</p>
                        </a>

                        <a href="centralized-config.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Centralized Configuration</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Global defaults and settings</p>
                        </a>

                        <a href="media-handling-system.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Media Handling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Universal file attachment</p>
                        </a>

                        <a href="vision-capabilities.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Vision Capabilities</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Image analysis with fallback</p>
                        </a>

                        <a href="tool-calling.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Tool Calling</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Universal tool system guide</p>
                        </a>

                        <a href="examples.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Examples</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">Real-world usage examples</p>
                        </a>

                        <a href="architecture.html" style="display: block; padding: 1rem; background: var(--background); border-radius: 0.5rem; text-decoration: none; border: 1px solid var(--border-light); transition: all 0.15s ease;">
                            <h4 style="margin: 0 0 0.5rem 0; color: var(--primary-color);">Architecture</h4>
                            <p style="margin: 0; color: var(--text-secondary); font-size: 0.875rem;">System design overview</p>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
