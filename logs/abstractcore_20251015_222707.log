ðŸš€ AbstractCore Server Starting | version=2.4.0, debug_mode=True
Request options: {'method': 'get', 'url': '/models', 'post_parser': <function SyncAPIClient._request_api_list.<locals>._parser at 0x1075116c0>, 'json_data': None}
Sending HTTP Request: GET https://api.openai.com/v1/models
connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111303470>
start_tls.started ssl_context=<ssl.SSLContext object at 0x11050ded0> server_hostname='api.openai.com' timeout=5.0
start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11039b0b0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.failed exception=ReadTimeout(TimeoutError('The read operation timed out'))
response_closed.started
response_closed.complete
Encountered httpx.TimeoutException
Traceback (most recent call last):
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py", line 126, in read
    with map_exceptions(exc_map):
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/Users/albou/projects/abstractllm_core/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out
2 retries left
Retrying request to /models in 0.397485 seconds
Request options: {'method': 'get', 'url': '/models', 'post_parser': <function SyncAPIClient._request_api_list.<locals>._parser at 0x1075116c0>, 'json_data': None}
Sending HTTP Request: GET https://api.openai.com/v1/models
connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1116b7a70>
start_tls.started ssl_context=<ssl.SSLContext object at 0x11050ded0> server_hostname='api.openai.com' timeout=5.0
start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1116b7980>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 15 Oct 2025 20:27:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-version', b'2020-10-01'), (b'x-request-id', b'278ac49c6703ec383bae44056d86bfa1'), (b'openai-processing-ms', b'348'), (b'x-envoy-upstream-service-time', b'352'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MbYyWS1Pulo7ufFvIWoUpQXsWpbYC_W9FCJBb2YFT38-1760560034-1.0.1.1-7TPlSxb3gDXH5_G3yOVijnxbLgCJhFzSHjeAesJjei04Bh7nf6F9luyTtqKu8ixgRzhPh7gyiKpHRjc6qB0mA2.QhXyc3rudvBOxqv2TYts; path=/; expires=Wed, 15-Oct-25 20:57:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=yoHz0SxqeSAbWY64EhYJMbpymGwIgdvYZJA4PPBQvfA-1760560034905-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98f20e57196a6fab-CDG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
HTTP Response: GET https://api.openai.com/v1/models "200 OK" Headers([('date', 'Wed, 15 Oct 2025 20:27:14 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-version', '2020-10-01'), ('x-request-id', '278ac49c6703ec383bae44056d86bfa1'), ('openai-processing-ms', '348'), ('x-envoy-upstream-service-time', '352'), ('x-openai-proxy-wasm', 'v0.1'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=MbYyWS1Pulo7ufFvIWoUpQXsWpbYC_W9FCJBb2YFT38-1760560034-1.0.1.1-7TPlSxb3gDXH5_G3yOVijnxbLgCJhFzSHjeAesJjei04Bh7nf6F9luyTtqKu8ixgRzhPh7gyiKpHRjc6qB0mA2.QhXyc3rudvBOxqv2TYts; path=/; expires=Wed, 15-Oct-25 20:57:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=yoHz0SxqeSAbWY64EhYJMbpymGwIgdvYZJA4PPBQvfA-1760560034905-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98f20e57196a6fab-CDG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
request_id: 278ac49c6703ec383bae44056d86bfa1
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_tokens 200000 from model capabilities for claude-3-haiku-20240307
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_output_tokens 4096 from model capabilities for claude-3-haiku-20240307
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_tokens 16384 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_output_tokens 4096 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Initialized tool handler for llama2: architecture=llama2, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111766bd0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Transfer-Encoding', b'chunked')])
HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_tokens 16384 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_output_tokens 4096 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Initialized tool handler for local-model: architecture=generic, native=False, prompted=True
connect_tcp.started host='localhost' port=1234 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111767620>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'2031'), (b'ETag', b'W/"7ef-l0yXIXblUzyYqINRHta1LIEi9so"'), (b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
Listed 105 models from all providers
Request options: {'method': 'get', 'url': '/models', 'post_parser': <function SyncAPIClient._request_api_list.<locals>._parser at 0x1116da3e0>, 'json_data': None}
Sending HTTP Request: GET https://api.openai.com/v1/models
connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111766990>
start_tls.started ssl_context=<ssl.SSLContext object at 0x1117902d0> server_hostname='api.openai.com' timeout=5.0
start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111766510>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-version', b'2020-10-01'), (b'x-request-id', b'be8c1e422f8ff5f1301f8609bcc85f0f'), (b'openai-processing-ms', b'172'), (b'x-envoy-upstream-service-time', b'173'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IDG1MJbYsb4JzqDUD27PeaTB_RAqBfdfiSAybAuHGpY-1760560035-1.0.1.1-zRUvaJaNjQ0ijYqTAb_au9RZXIR95VKDL_wj5Y1wrjAUOWT2KGblY10kir2XaENSNbYC8Y3JZoL08I54rB6mC5hCqhmdUkAyQtXjsAxWr1A; path=/; expires=Wed, 15-Oct-25 20:57:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=kE8aDSV36qq4hlkhXm81zpjtE5lg2qmQ9jOOKK1kafQ-1760560035392-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98f20e5b688422a0-CDG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
HTTP Response: GET https://api.openai.com/v1/models "200 OK" Headers([('date', 'Wed, 15 Oct 2025 20:27:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-version', '2020-10-01'), ('x-request-id', 'be8c1e422f8ff5f1301f8609bcc85f0f'), ('openai-processing-ms', '172'), ('x-envoy-upstream-service-time', '173'), ('x-openai-proxy-wasm', 'v0.1'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=IDG1MJbYsb4JzqDUD27PeaTB_RAqBfdfiSAybAuHGpY-1760560035-1.0.1.1-zRUvaJaNjQ0ijYqTAb_au9RZXIR95VKDL_wj5Y1wrjAUOWT2KGblY10kir2XaENSNbYC8Y3JZoL08I54rB6mC5hCqhmdUkAyQtXjsAxWr1A; path=/; expires=Wed, 15-Oct-25 20:57:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=kE8aDSV36qq4hlkhXm81zpjtE5lg2qmQ9jOOKK1kafQ-1760560035392-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98f20e5b688422a0-CDG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
request_id: be8c1e422f8ff5f1301f8609bcc85f0f
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_tokens 200000 from model capabilities for claude-3-haiku-20240307
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_output_tokens 4096 from model capabilities for claude-3-haiku-20240307
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_tokens 16384 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_output_tokens 4096 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Initialized tool handler for llama2: architecture=llama2, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117887a0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Transfer-Encoding', b'chunked')])
HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_tokens 16384 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_output_tokens 4096 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Initialized tool handler for local-model: architecture=generic, native=False, prompted=True
connect_tcp.started host='localhost' port=1234 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111789340>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'2031'), (b'ETag', b'W/"7ef-l0yXIXblUzyYqINRHta1LIEi9so"'), (b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
Listed 105 models from all providers
Request options: {'method': 'get', 'url': '/models', 'post_parser': <function SyncAPIClient._request_api_list.<locals>._parser at 0x1116d9260>, 'json_data': None}
Sending HTTP Request: GET https://api.openai.com/v1/models
connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11178a9c0>
start_tls.started ssl_context=<ssl.SSLContext object at 0x111756750> server_hostname='api.openai.com' timeout=5.0
start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11178a8a0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 15 Oct 2025 20:27:15 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-version', b'2020-10-01'), (b'x-request-id', b'6f89ae8d7747869fa1e83e74f7558c24'), (b'openai-processing-ms', b'274'), (b'x-envoy-upstream-service-time', b'277'), (b'x-openai-proxy-wasm', b'v0.1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=6YfKmX4rGwQU24lfeUGdB.lbVNJ1uBiI6yn3fScH4_w-1760560035-1.0.1.1-ho6U7Q7iRkstqAIRCbr53W6gEqjjLZOMhJLtIOD6FEqNLQscRiUW_sJKtMiTVvPmWirejCWMWrM7LvYy1ZM_BjcybufizC7w3a3b2BcWndM; path=/; expires=Wed, 15-Oct-25 20:57:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=L56yX.mfydy6TbK94eQr2c5JUD7m0P9g_VpAjm1Cx9w-1760560035992-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'98f20e5e2ea57037-CDG'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
HTTP Response: GET https://api.openai.com/v1/models "200 OK" Headers([('date', 'Wed, 15 Oct 2025 20:27:15 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-version', '2020-10-01'), ('x-request-id', '6f89ae8d7747869fa1e83e74f7558c24'), ('openai-processing-ms', '274'), ('x-envoy-upstream-service-time', '277'), ('x-openai-proxy-wasm', 'v0.1'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=6YfKmX4rGwQU24lfeUGdB.lbVNJ1uBiI6yn3fScH4_w-1760560035-1.0.1.1-ho6U7Q7iRkstqAIRCbr53W6gEqjjLZOMhJLtIOD6FEqNLQscRiUW_sJKtMiTVvPmWirejCWMWrM7LvYy1ZM_BjcybufizC7w3a3b2BcWndM; path=/; expires=Wed, 15-Oct-25 20:57:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=L56yX.mfydy6TbK94eQr2c5JUD7m0P9g_VpAjm1Cx9w-1760560035992-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '98f20e5e2ea57037-CDG'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
request_id: 6f89ae8d7747869fa1e83e74f7558c24
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_tokens 200000 from model capabilities for claude-3-haiku-20240307
Detected architecture 'claude' for model 'claude-3-haiku-20240307' (pattern: 'claude')
Using capabilities from 'claude-3-haiku' for 'claude-3-haiku-20240307'
Using max_output_tokens 4096 from model capabilities for claude-3-haiku-20240307
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_tokens 16384 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Using max_output_tokens 4096 from model capabilities for llama2
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Detected architecture 'llama2' for model 'llama2' (pattern: 'llama2')
Using default capabilities for 'llama2' (architecture: llama2)
Initialized tool handler for llama2: architecture=llama2, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11178bda0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:16 GMT'), (b'Transfer-Encoding', b'chunked')])
HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_tokens 16384 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Using max_output_tokens 4096 from model capabilities for local-model
No specific architecture detected for 'local-model', using generic
No specific architecture detected for 'local-model', using generic
Using default capabilities for 'local-model' (architecture: generic)
Initialized tool handler for local-model: architecture=generic, native=False, prompted=True
connect_tcp.started host='localhost' port=1234 local_address=None timeout=5.0 socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117892b0>
send_request_headers.started request=<Request [b'GET']>
send_request_headers.complete
send_request_body.started request=<Request [b'GET']>
send_request_body.complete
receive_response_headers.started request=<Request [b'GET']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'2031'), (b'ETag', b'W/"7ef-l0yXIXblUzyYqINRHta1LIEi9so"'), (b'Date', b'Wed, 15 Oct 2025 20:27:16 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'GET']>
receive_response_body.complete
response_closed.started
response_closed.complete
Listed 105 models from all providers
ðŸ“¥ Chat Completion Request | request_id=0ffd600a, provider=ollama, model=qwen3-coder:30b, messages=4, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=0ffd600a, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b0500>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:23 GMT'), (b'Content-Length', b'881')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 7300.25ms (tokens: 313)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
ðŸ“¥ Chat Completion Request | request_id=db41a80d, provider=ollama, model=qwen3-coder:30b, messages=4, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=db41a80d, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b1100>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:24 GMT'), (b'Content-Length', b'829')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 1344.04ms (tokens: 165)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
ðŸ“¥ Chat Completion Request | request_id=f4ba7503, provider=ollama, model=qwen3-coder:30b, messages=6, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=f4ba7503, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b1eb0>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:26 GMT'), (b'Content-Length', b'1119')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 1968.41ms (tokens: 271)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
ðŸ“¥ Chat Completion Request | request_id=ca89934a, provider=ollama, model=qwen3-coder:30b, messages=6, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=ca89934a, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b2a80>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:27 GMT'), (b'Content-Length', b'367')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 327.28ms (tokens: 97)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
ðŸ“¥ Chat Completion Request | request_id=e83478b8, provider=ollama, model=qwen3-coder:30b, messages=4, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=e83478b8, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b36b0>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:28 GMT'), (b'Content-Length', b'652')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 950.20ms (tokens: 118)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
ðŸ“¥ Chat Completion Request | request_id=a68d5084, provider=ollama, model=qwen3-coder:30b, messages=4, has_tools=False, stream=False
ðŸŽ¯ Target Format Detected | request_id=a68d5084, target_format=qwen3, user_agent=python-httpx/0.28.1
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_tokens 32768 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Using max_output_tokens 8192 from model capabilities for qwen3-coder:30b
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected architecture 'qwen3_moe' for model 'qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Using capabilities from 'qwen3' for 'qwen3-coder:30b'
Initialized tool handler for qwen3-coder:30b: architecture=qwen3_moe, native=False, prompted=True
connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1117b8230>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 15 Oct 2025 20:27:29 GMT'), (b'Content-Length', b'787')])
HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
Generation completed for qwen3-coder:30b: 1195.03ms (tokens: 143)
Detected architecture 'qwen3_moe' for model 'ollama/qwen3-coder:30b' (pattern: 'qwen3-coder:30b')
Detected 0 tool calls in content
