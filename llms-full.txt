# AbstractCore - Complete Documentation

This file contains the complete documentation for AbstractCore, a unified Python library providing a consistent interface to all major LLM providers.

## Table of Contents

1. [Project Overview](#project-overview)
2. [Installation](#installation)
3. [Getting Started](#getting-started)
4. [Tool Calling System](#tool-calling-system)
5. [Core Features](#core-features)
6. [Documentation Index](#documentation-index)

---

## Project Overview

AbstractCore is a Python library providing a unified interface to all major LLM providers (OpenAI, Anthropic, Ollama, MLX, LMStudio, HuggingFace) with production-grade features including tool calling, streaming, session management, and embeddings.

### Key Features

- **Universal Provider Support**: OpenAI, Anthropic, Ollama, LMStudio, MLX, HuggingFace
- **Tool Calling**: Universal @tool decorator works across ALL providers
- **Streaming**: Real-time responses for interactive applications
- **Session Management**: Persistent conversations with metadata
- **Structured Output**: Pydantic model validation with automatic retries
- **Embeddings**: SOTA embedding models for semantic search and RAG
- **Event System**: Monitor costs, performance, and tool executions
- **Built-in CLI Apps**: Production-ready terminal applications (`summarizer`, `extractor`, `judge`) with comprehensive parameter support
- **HTTP Server**: OpenAI-compatible REST API
- **Production Ready**: Robust error handling, retries, circuit breakers

### Supported Providers

| Provider | Status | Features |
|----------|--------|----------|
| OpenAI | Native tool calls, streaming, structured output |
| Anthropic | Native tool calls, streaming, structured output |
| Ollama | Prompted tool calls, streaming, local models |
| LMStudio | Prompted tool calls, streaming, local models |
| MLX | Prompted tool calls, streaming, Apple Silicon |
| HuggingFace | Prompted tool calls, streaming, open models |

---

## Installation

### Basic Installation

```bash
pip install abstractcore
```

### Provider-Specific Installation

```bash
# OpenAI
pip install abstractcore[openai]

# Anthropic
pip install abstractcore[anthropic]

# Ollama (local models)
pip install abstractcore[ollama]

# LMStudio (local models)
pip install abstractcore[lmstudio]

# MLX (Apple Silicon)
pip install abstractcore[mlx]

# HuggingFace
pip install abstractcore[huggingface]

# Server support
pip install abstractcore[server]

# Embeddings
pip install abstractcore[embeddings]

# Everything
pip install abstractcore[all]
```

---

## Getting Started

### Quick Start

```python
from abstractllm import create_llm

# Create any provider using the factory pattern
llm = create_llm("openai", model="gpt-4o-mini")

# Generate a response
response = llm.generate("What is the capital of France?")
print(response.content)
```

### Provider Examples

```python
# OpenAI
llm = create_llm("openai", model="gpt-4o-mini")

# Anthropic
llm = create_llm("anthropic", model="claude-3-5-haiku-latest")

# Ollama (local)
llm = create_llm("ollama", model="qwen3:4b-instruct-2507-q4_K_M")

# LMStudio (local)
llm = create_llm("lmstudio", model="qwen3-coder:30b")

# MLX (Apple Silicon)
llm = create_llm("mlx", model="mlx-community/Qwen2.5-Coder-7B-Instruct-4bit")
```

### Streaming

```python
# Stream responses in real-time
for chunk in llm.generate("Tell me a story", stream=True):
    print(chunk.content, end="", flush=True)
```

### Structured Output

```python
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int
    occupation: str

# Automatic validation and retry on failures
person = llm.generate(
    "Extract: John Doe is a 30-year-old software engineer",
    response_model=Person
)

print(f"{person.name}, {person.age}, {person.occupation}")
```

---

## Tool Calling System

AbstractCore provides a universal tool calling system that works across all LLM providers, even those without native tool support.

### The @tool Decorator

The `@tool` decorator is the primary way to create tools in AbstractCore:

```python
from abstractllm import create_llm, tool

@tool
def get_weather(city: str) -> str:
    """Get current weather for a specified city."""
    return f"The weather in {city} is sunny, 72°F"

@tool
def calculate(expression: str) -> float:
    """Perform a mathematical calculation."""
    try:
        result = eval(expression)  # Simplified for demo
        return result
    except Exception:
        return float('nan')

# Works with ANY provider
llm = create_llm("openai", model="gpt-4o-mini")
response = llm.generate(
    "What's the weather in Tokyo and what's 15 * 23?",
    tools=[get_weather, calculate]
)
```

### Enhanced Metadata

The `@tool` decorator supports rich metadata that gets automatically injected into system prompts:

```python
@tool(
    description="Search the database for records matching the query",
    tags=["database", "search", "query"],
    when_to_use="When the user asks for specific data from the database",
    examples=[
        {
            "description": "Find all users named John",
            "arguments": {
                "query": "name=John",
                "table": "users"
            }
        }
    ]
)
def search_database(query: str, table: str = "users") -> str:
    """Search the database for records matching the query."""
    return f"Searching {table} for: {query}"
```

### Universal Tool Support

AbstractCore's tool system works across all providers through two mechanisms:

1. **Native Tool Support**: For providers with native tool APIs (OpenAI, Anthropic)
2. **Intelligent Prompting**: For providers without native tool support (Ollama, MLX, LMStudio)

AbstractCore automatically:
- Detects the model architecture (Qwen3, LLaMA3, etc.)
- Formats tools with examples into system prompt
- Parses tool calls from response using appropriate format
- Executes tools locally and returns results

### Architecture-Aware Tool Call Detection

| Architecture | Format | Example |
|-------------|--------|---------|
| **Qwen3** | `<|tool_call|>...JSON...</|tool_call|>` | `<|tool_call|>{"name": "get_weather", "arguments": {"city": "Paris"}}</|tool_call|>` |
| **LLaMA3** | `<function_call>...JSON...</function_call>` | `<function_call>{"name": "get_weather", "arguments": {"city": "Paris"}}</function_call>` |
| **OpenAI/Anthropic** | Native API tool calls | Structured JSON in API response |
| **XML-based** | `<tool_call>...JSON...</tool_call>` | `<tool_call>{"name": "get_weather", "arguments": {"city": "Paris"}}</tool_call>` |

### Tool Chaining

Tools can call other tools or return data that triggers additional tool calls:

```python
@tool
def get_user_location(user_id: str) -> str:
    """Get the location of a user."""
    locations = {"user123": "Paris", "user456": "Tokyo"}
    return locations.get(user_id, "Unknown")

@tool
def get_weather(city: str) -> str:
    """Get weather for a city."""
    return f"Weather in {city}: 72°F, sunny"

# LLM can chain these tools:
response = llm.generate(
    "What's the weather like for user123?",
    tools=[get_user_location, get_weather]
)
# LLM will first call get_user_location, then get_weather with the result
```

### Tool Syntax Rewriting

AbstractCore provides comprehensive tool call format conversion for compatibility with different agentic CLIs:

```python
# Custom tag configuration
llm = create_llm(
    "ollama",
    model="qwen3-coder:30b",
    tool_call_tags="mytag"  # Becomes: <mytag>...JSON...</mytag>
)

# Predefined CLI formats
from abstractllm.tools.tag_rewriter import create_tag_rewriter

# Codex CLI
rewriter = create_tag_rewriter("codex")  # Qwen3 format

# Crush CLI  
rewriter = create_tag_rewriter("crush")  # LLaMA3 format

# Gemini CLI
rewriter = create_tag_rewriter("gemini")  # XML format
```

---

## Core Features

### Session Management

Persistent conversations with metadata and analytics:

```python
from abstractllm import BasicSession, create_llm

llm = create_llm("openai", model="gpt-4o-mini")
session = BasicSession(llm, system_prompt="You are a helpful assistant.")

response1 = session.generate("My name is Alice")
response2 = session.generate("What's my name?")  # Remembers context

# Save with analytics
session.save("conversation.json", summary=True, assessment=True, facts=True)

# Load session
loaded_session = BasicSession.load("conversation.json")
```

### Embeddings

SOTA embedding models for semantic search and RAG applications:

```python
from abstractllm.embeddings import EmbeddingManager

# HuggingFace (default)
embedder = EmbeddingManager(model="sentence-transformers/all-MiniLM-L6-v2")

# Ollama
embedder = EmbeddingManager(model="granite-embedding:278m", provider="ollama")

# LMStudio
embedder = EmbeddingManager(model="text-embedding-all-minilm-l6-v2-embedding", provider="lmstudio")

# Generate embeddings
embedding = embedder.embed("Hello world")
embeddings = embedder.embed_batch(["Hello", "World", "AI"])

# Similarity computation
similarity = embedder.compute_similarity("Hello", "Hi there")
```

### Built-in CLI Applications

AbstractCore includes three production-ready CLI applications that work directly from the terminal:

#### Direct Terminal Usage

```bash
# Document summarization with multiple styles and lengths
summarizer document.pdf --style=executive --length=brief --output=summary.txt
summarizer report.txt --focus="technical details" --chunk-size=15000 --verbose

# Knowledge graph extraction with various formats
extractor research_paper.pdf --format=json-ld --focus=technology --iterate=3
extractor article.txt --entity-types=person,organization,location --output=entities.jsonld

# Text evaluation with custom criteria and contexts
judge essay.txt --criteria=clarity,accuracy,coherence --context="academic writing"
judge code.py --context="code review" --format=plain --verbose
```

#### Alternative Usage Methods

```bash
# Method 1: Direct commands (recommended)
summarizer document.txt --style=executive
extractor report.pdf --format=triples
judge essay.md --criteria=soundness

# Method 2: Via Python module
python -m abstractllm.apps.summarizer document.txt --style=executive
python -m abstractllm.apps.extractor report.pdf --format=triples
python -m abstractllm.apps.judge essay.md --criteria=soundness
```

#### Python API (for programmatic use)

```python
from abstractllm.processing import BasicSummarizer, BasicExtractor, BasicJudge

# Intelligent summarization
summarizer = BasicSummarizer()
summary = summarizer.summarize(long_text, style="executive", length="brief")

# Knowledge graph extraction
extractor = BasicExtractor()
kg = extractor.extract(text, domain_focus="technology", output_format="jsonld")

# Text evaluation
judge = BasicJudge()
assessment = judge.evaluate(text, context="code review", criteria=["clarity", "soundness"])
```

### Event System

Monitor and control tool execution through events:

```python
from abstractllm.events import EventType, on_global

def cost_monitor(event):
    if event.cost_usd and event.cost_usd > 0.10:
        alert(f"High cost request: ${event.cost_usd}")

def prevent_dangerous_tools(event):
    for call in event.data.get('tool_calls', []):
        if call.name in ['delete_file', 'system_command']:
            event.prevent()  # Stop execution

on_global(EventType.GENERATION_COMPLETED, cost_monitor)
on_global(EventType.TOOL_STARTED, prevent_dangerous_tools)
```

### HTTP Server

OpenAI-compatible REST API server:

```bash
# Start server
python -m abstractllm.server.app

# Or with uvicorn
uvicorn abstractllm.server.app:app --host 0.0.0.0 --port 8000
```

```python
import openai

# Point to AbstractCore server
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"
)

# Works with any model via AbstractCore
response = client.chat.completions.create(
    model="ollama/qwen3-coder:30b",  # Non-OpenAI model
    messages=[{"role": "user", "content": "Hello!"}],
    tools=[...]
)
```

### Memory Management

For local providers, explicit memory management:

```python
# Explicit memory management for local models
llm = create_llm("ollama", model="large-model")
response = llm.generate("Hello")
llm.unload()  # Free memory
del llm
```

---

## Documentation Index

### Core Documentation

- **Getting Started**: Quick setup and first LLM call with any provider
- **API Reference**: Complete API documentation with examples for all methods
- **Tool Calling System**: Universal @tool decorator and tool system across all providers
- **Session Management**: Persistent conversations with metadata, analytics, and serialization
- **Embeddings System**: SOTA embedding models for semantic search and RAG applications
- **Architecture Overview**: System design and component interactions

### Built-in CLI Applications

- **Summarizer CLI**: Direct terminal document summarization (`summarizer document.pdf --style=executive`)
- **Extractor CLI**: Knowledge graph extraction (`extractor report.txt --format=json-ld --focus=technology`)
- **Judge CLI**: Text evaluation and scoring (`judge essay.txt --criteria=clarity,accuracy --context="academic writing"`)
- **CLI Usage Guide**: Complete parameter reference with all available options and usage examples

### Server & Integration

- **HTTP Server Guide**: OpenAI-compatible REST API server for multi-language access
- **Agentic CLI Integration**: Codex, Crush, Gemini CLI compatibility
- **Tool Syntax Rewriting**: Real-time format conversion for external CLI tools

### Advanced Features

- **Event System**: Monitor costs, performance, and tool executions with preventable events
- **Retry Configuration**: Circuit breakers, exponential backoff, jitter for production reliability
- **Error Handling**: Comprehensive exception hierarchy for robust applications
- **Token Management**: Centralized token counting, cost estimation, and management utilities (`abstractllm.utils.token_utils`)

### Examples and Patterns

- **Progressive Examples**: Step-by-step learning from basic to advanced usage
- **Complete RAG Example**: Full retrieval-augmented generation implementation
- **Embeddings Demos**: Semantic search and similarity computation
- **Streaming Examples**: Real-time vs batch processing patterns
- **Tool Usage Examples**: Complex tool calling scenarios
- **Production Patterns**: Enterprise-ready deployment examples

### Support and Troubleshooting

- **Troubleshooting Guide**: Common issues and solutions
- **Capabilities Reference**: What AbstractCore can and cannot do
- **Contributing Guidelines**: How to contribute to the project

---

## Repository Information

- **GitHub**: https://github.com/lpalbou/AbstractCore
- **Website**: https://lpalbou.github.io/AbstractCore/
- **License**: MIT
- **Python**: 3.9+
- **Status**: Production Ready (v2.3.5)
- **Package**: Available on PyPI as `abstractcore`

## Key Differentiators

- **Provider Agnostic**: Same code works with any LLM provider via factory pattern
- **Production Ready**: Robust error handling, retries, circuit breakers, event system
- **Type Safe**: Full Pydantic integration for structured outputs with automatic retries
- **Local & Cloud**: Support for both local models and cloud APIs with memory management
- **Unified Streaming**: Consistent streaming across all providers with tool support
- **Universal Tool Calling**: @tool decorator works across ALL providers with format conversion
- **Session Management**: Persistent conversations with analytics and complete serialization
- **Built-in Apps**: Ready-to-use summarizer, extractor, and judge applications
- **Embeddings**: Built-in SOTA embedding models for RAG applications
- **Server Mode**: Optional OpenAI-compatible HTTP API with agentic CLI support
- **Well Documented**: Comprehensive guides and working examples

---

This documentation provides a complete overview of AbstractCore's capabilities. For the latest updates and detailed examples, visit the GitHub repository and website.