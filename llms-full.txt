# AbstractCore - Complete Documentation

This file contains the complete documentation for AbstractCore, a unified Python library designed primarily for open source LLMs with complete offline capability. Provides a consistent interface to all major LLM providers.

## Table of Contents

1. [Project Overview](#project-overview) - What AbstractCore does and supported providers
2. [Installation](#installation) - Quick setup with provider-specific options
3. [Centralized Configuration](#centralized-configuration) - Global defaults and preferences management
4. [Media Handling System](#media-handling-system) - Universal file attachment and processing
5. [Vision Capabilities](#vision-capabilities) - Image analysis across all providers
6. [Getting Started](#getting-started) - Essential code examples and patterns
7. [Provider Health Monitoring](#provider-health-monitoring-v244) - Connectivity checks and fallback strategies (v2.4.4+)
8. [Token Counting and Usage Tracking](#token-counting-and-usage-tracking-v244) - Input/output token tracking across all providers (v2.4.4+)
9. [Provider Discovery](#provider-discovery) - Programmatic provider and model discovery
10. [Built-in CLI Applications](#built-in-cli-applications) - Ready-to-use terminal tools
11. [Debug & Advanced Features](#debug-capabilities-and-self-healing-json) - Troubleshooting and performance
12. [Complete CLI Parameters](#complete-cli-parameters-reference) - All available options
13. [Documentation Index](#documentation-index) - Links to detailed guides

---

## Project Overview

AbstractCore is a comprehensive Python library designed primarily for open source LLMs with complete offline capability. Provides a unified interface to 9 LLM providers (OpenAI, Anthropic, OpenRouter, Ollama, LMStudio, MLX, HuggingFace, vLLM, openai-compatible) with production-grade features including universal media handling, vision capabilities, tool calling, streaming, centralized configuration, and session management.

### Key Features

- **Offline-First Design**: Built primarily for open source LLMs with complete offline operation. Download once, run forever without internet access
- **Universal Provider Support**: OpenAI, Anthropic, OpenRouter, Ollama, LMStudio, MLX, HuggingFace, vLLM, openai-compatible with identical syntax
- **Async/Await Support** (v2.6.0): Native async support for concurrent requests with `asyncio.gather()` - works across providers for 3-10x faster batch operations ([docs/api-reference.md#agenerate](docs/api-reference.md#agenerate))
- **vLLM Provider** (v2.6.4): High-throughput GPU inference with guided decoding, Multi-LoRA, beam search for NVIDIA CUDA GPUs
- **OpenAI-Compatible Provider** (v2.6.4): Generic provider for any OpenAI-compatible endpoint (llama.cpp, LocalAI, text-generation-webui)
- **Dynamic Base URL** (v2.6.5): POST `base_url` parameter for runtime endpoint routing without environment variables
- **Enhanced Error Messages** (v2.6.0): Authentication and connection errors include fix commands and setup URLs
- **Interaction Tracing**: Complete LLM observability with programmatic access to prompts, responses, tokens, timing for debugging and compliance ([docs/interaction-tracing.md](docs/interaction-tracing.md))
- **Centralized Configuration**: Global defaults, app-specific preferences, API key management with `~/.abstractcore/config/abstractcore.json`
- **Media Handling System**: Universal file attachment with `@filename` CLI syntax and `media=[]` API parameter - supports images, PDFs, Office docs, CSV/TSV
- **Vision Capabilities**: Image analysis across all providers with automatic resolution optimization and vision fallback for text-only models
- **Tool Calling**: Universal @tool decorator works across ALL providers with intelligent format conversion
- **Streaming**: Real-time responses for interactive applications with tool call detection
- **Session Management**: Persistent conversations with metadata and automatic context management
- **Structured Output**: Native vs prompted strategies, schema validation, production deployment ([docs/structured-output.md](docs/structured-output.md))
- **Embeddings**: SOTA embedding models for semantic search and RAG applications
- **Event System**: Monitor costs, performance, tool executions, and media processing
- **Built-in CLI Apps**: Ready-to-use terminal applications (`summarizer`, `extractor`, `judge`, `intent`, `deepsearch`)
- **HTTP Server**: OpenAI-compatible REST API with provider discovery and unified access
- **MCP (Model Context Protocol)**: Discover tools from MCP servers (HTTP/stdio) and expose them as tool specs
- **Production Ready**: Robust error handling, retries, circuit breakers, and graceful degradation
- **Provider Health Checks**: `.health()` method for connectivity monitoring with timeout management (v2.4.4+)
- **Centralized Token Counting**: Consistent input/output token tracking across all providers with unified TokenUtils infrastructure (v2.4.4+)

### Supported Providers

| Provider | Features | Hardware |
|----------|----------|----------|
| **OpenAI** | Native tools, streaming, structured output, vision (GPT-4o) | Any |
| **Anthropic** | Native tools, streaming, structured output, vision (Claude 4.5) | Any |
| **OpenRouter** | OpenAI-compatible aggregator (multi-provider routing, unified billing) | Any |
| **Ollama** | Prompted tools, streaming, structured output, local models | Any |
| **LMStudio** | Prompted tools, streaming, structured output, local models | Any |
| **MLX** | Prompted tools, streaming, Apple Silicon optimized | Apple Silicon only |
| **HuggingFace** | Prompted tools, streaming, transformers/GGUF models | Any |
| **vLLM** | Guided decoding, Multi-LoRA, beam search, high-throughput | NVIDIA CUDA only |
| **openai-compatible** | Any OpenAI-compatible endpoint (llama.cpp, LocalAI) | Any |

---

## Installation

### Basic Installation

```bash
pip install abstractcore
```

### Provider-Specific Installation

```bash
# OpenAI
pip install abstractcore[openai]

# Anthropic
pip install abstractcore[anthropic]

# Ollama (local models)
pip install abstractcore[ollama]

# LMStudio (local models)
pip install abstractcore[lmstudio]

# MLX (Apple Silicon)
pip install abstractcore[mlx]

# HuggingFace
pip install abstractcore[huggingface]

# vLLM (NVIDIA CUDA GPUs only)
pip install abstractcore[vllm]

# OpenAI-compatible endpoints (llama.cpp, LocalAI, etc.)
# No extra dependencies required (uses core httpx).

# Media handling (images, basic documents)
pip install abstractcore[media]

# Server support
pip install abstractcore[server]

# Embeddings
pip install abstractcore[embeddings]

# Full installs (recommended; pick one)
pip install abstractcore[all-apple]    # macOS/Apple Silicon (includes MLX, excludes vLLM)
pip install abstractcore[all-non-mlx]  # Linux/Windows/Intel Mac (excludes MLX and vLLM)
pip install abstractcore[all-gpu]      # Linux NVIDIA GPU (includes vLLM, excludes MLX)

# Note: `abstractcore[all]` exists, but includes both MLX and vLLM and may not install cleanly on all platforms.
```

---

## Centralized Configuration

AbstractCore provides a unified configuration system that manages default models, cache directories, logging settings, and other package-wide preferences from a single location. This eliminates the need to specify providers and models repeatedly and provides consistent behavior across all applications.

### Configuration File Location

Configuration is stored in: `~/.abstractcore/config/abstractcore.json`

### Quick Configuration Setup

```bash
# Check current configuration status
abstractcore --status                         # Configuration CLI
abstractcore-config --status                  # Same thing (alias for clarity)

# Set global fallback model (used when no app-specific default is configured)
abstractcore --set-global-default ollama/llama3:8b

# Set app-specific defaults for optimal performance
abstractcore --set-app-default summarizer openai gpt-5-mini
abstractcore --set-app-default extractor ollama qwen3:4b-instruct
abstractcore --set-app-default judge anthropic claude-haiku-4-5
abstractcore --set-app-default cli huggingface unsloth/Qwen3-4B-Instruct-2507-GGUF

# Set API keys
abstractcore --set-api-key openai sk-your-key-here
abstractcore --set-api-key anthropic your-anthropic-key
abstractcore --set-api-key openrouter your-openrouter-key

# Configure logging
abstractcore --set-console-log-level WARNING  # Reduce console output
abstractcore --enable-file-logging            # Save logs to files

# Interactive setup
abstractcore --configure
```

### Interactive Chat CLI

For interactive REPL sessions with LLMs (different from configuration CLI):

```bash
# Start interactive REPL for chatting with LLMs
abstractcore-chat                                    # New convenient entry point (v2.4.5+)
abstractcore-chat --provider ollama --model llama2
abstractcore-chat --stream

# Alternative (backwards compatible)
python -m abstractcore.utils.cli --provider openai --model gpt-5-mini
python -m abstractcore.utils.cli --prompt "What is Python?"
```

**Note**: AbstractCore has two separate CLI tools:
- **Configuration CLI** (`abstractcore` or `abstractcore-config`): Manage settings, API keys, models
- **Interactive Chat CLI** (`abstractcore-chat`): REPL for conversing with LLMs

### Configuration Priority System

AbstractCore uses a clear priority hierarchy:

1. **Explicit Parameters** (highest priority)
   ```bash
   summarizer document.txt --provider openai --model gpt-5-mini
   ```

2. **App-Specific Configuration**
   ```bash
   abstractcore --set-app-default summarizer openai gpt-5-mini
   ```

3. **Global Configuration**
   ```bash
   abstractcore --set-global-default openai/gpt-5-mini
   ```

4. **Hardcoded Defaults** (lowest priority)
   - Current default: `huggingface/unsloth/Qwen3-4B-Instruct-2507-GGUF`

### Application Defaults

Set default providers and models for specific AbstractCore applications:

```bash
# Set defaults for individual apps
abstractcore --set-app-default summarizer openai gpt-5-mini
abstractcore --set-app-default cli anthropic claude-haiku-4-5
abstractcore --set-app-default extractor ollama qwen3:4b-instruct
abstractcore --set-app-default judge anthropic claude-haiku-4-5

# View current app defaults
abstractcore --status
```

### Logging Configuration

Control logging behavior across all AbstractCore components:

```bash
# Change console logging level
abstractcore --set-console-log-level DEBUG    # Show all messages
abstractcore --set-console-log-level INFO     # Show info and above
abstractcore --set-console-log-level WARNING  # Show warnings and errors only (default)
abstractcore --set-console-log-level ERROR    # Show only errors
abstractcore --set-console-log-level NONE     # Disable all console logging

# File logging controls
abstractcore --enable-file-logging      # Start saving logs to files
abstractcore --disable-file-logging     # Stop saving logs to files
abstractcore --set-log-base-dir ~/.abstractcore/logs

# Quick commands
abstractcore --enable-debug-logging     # Sets both console and file to DEBUG
abstractcore --disable-console-logging  # Keeps file logging if enabled
```

### Cache and Storage Configuration

```bash
# Set cache directories
abstractcore --set-default-cache-dir ~/.cache/abstractcore
abstractcore --set-huggingface-cache-dir ~/.cache/huggingface
abstractcore --set-local-models-cache-dir ~/.abstractcore/models
```

### Vision Fallback Configuration

Configure vision processing for text-only models:

```bash
# Download local vision model (recommended)
abstractcore --download-vision-model

# Use existing Ollama model
abstractcore --set-vision-provider ollama qwen2.5vl:7b

# Use cloud API
abstractcore --set-vision-provider openai gpt-4o

# Disable vision fallback
abstractcore --disable-vision
```

### Streaming Defaults

```bash
# Set default streaming behavior for CLI
abstractcore --stream on           # Enable streaming by default
abstractcore --stream off          # Disable streaming by default
```

### Configuration Status

View complete configuration with helpful change commands:

```bash
abstractcore --status
```

This displays a hierarchical dashboard showing:
- [CONFIG] Application Defaults (CLI, Summarizer, Extractor, Judge)
- ðŸŒ Global Fallback settings
- ðŸ‘ï¸ Media Processing configuration
- ðŸ”‘ Provider Access (API key status)
- ðŸ“ Logging configuration
- ðŸ’¾ Storage locations

### Common Configuration Workflows

**First-Time Setup:**
```bash
# Check what's available
abstractcore --status

# Configure for development (free local models)
abstractcore --set-global-default ollama/llama3:8b
abstractcore --set-console-log-level WARNING

# Add API keys when ready for cloud providers
abstractcore --set-api-key openai sk-your-key-here
abstractcore --set-api-key anthropic your-anthropic-key
abstractcore --set-api-key openrouter your-openrouter-key

# Verify everything works
abstractcore --status
```

**Development Environment:**
```bash
# Optimize for local development
abstractcore --set-global-default ollama/llama3:8b  # Free local models
abstractcore --enable-debug-logging                 # Detailed logs for debugging
abstractcore --set-app-default cli ollama qwen3:4b  # Fast model for CLI testing
```

**Production Environment:**
```bash
# Configure for production reliability and performance
abstractcore --set-global-default openai/gpt-5-mini  # Reliable cloud provider
abstractcore --set-console-log-level WARNING          # Reduce noise
abstractcore --enable-file-logging                    # Persistent logs
abstractcore --set-app-default summarizer openai gpt-5-mini  # Optimize for quality
```

**Multi-Environment Approach:**
```bash
# Use different providers for different applications
abstractcore --set-app-default cli ollama qwen3:4b           # Fast for development
abstractcore --set-app-default summarizer openai gpt-5-mini # Quality for documents
abstractcore --set-app-default judge anthropic claude-haiku-4-5 # Detailed analysis
```

For complete configuration reference, see: [Centralized Configuration Guide](docs/centralized-config.md)

### Environment Variables for Provider Base URLs (v2.6.1)

AbstractCore supports environment variables for configuring provider base URLs, enabling remote servers, Docker deployments, and non-standard ports. Provider discovery (`get_all_providers_with_models()`) automatically respects these environment variables.

**Supported Environment Variables:**

```bash
# Ollama (supports two variants)
export OLLAMA_BASE_URL="http://192.168.1.100:11434"  # Explicit base URL
export OLLAMA_HOST="http://192.168.1.100:11434"       # Alternative (official Ollama env var)

# LMStudio
export LMSTUDIO_BASE_URL="http://localhost:1235/v1"   # Non-standard port

# vLLM (OpenAI-compatible server)
export VLLM_BASE_URL="http://localhost:8000/v1"

# OpenAI-compatible (generic)
export OPENAI_COMPATIBLE_BASE_URL="http://localhost:1234/v1"

# OpenRouter
export OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"

# OpenAI (for proxies and gateways)
export OPENAI_BASE_URL="https://api.portkey.ai/v1"

# Anthropic (for proxies and gateways)
export ANTHROPIC_BASE_URL="https://api.portkey.ai/v1"
```

**Priority System:**
1. **Programmatic parameter** (highest): `create_llm("ollama", base_url="http://custom:11434")`
2. **Environment variable**: `OLLAMA_BASE_URL` or `OLLAMA_HOST`
3. **Default value** (lowest): `http://localhost:11434`

**Use Cases:**

```python
# Remote Ollama server
import os
from abstractcore import create_llm
from abstractcore.providers import get_all_providers_with_models

os.environ['OLLAMA_BASE_URL'] = 'http://192.168.1.100:11434'

# Provider discovery automatically checks the remote URL
providers = get_all_providers_with_models(include_models=False)
ollama = next((p for p in providers if p['name'] == 'ollama'), None)
# ollama['status'] reflects actual connectivity to remote server

# Create LLM with automatic env var usage
llm = create_llm('ollama', model='llama3:8b')  # Uses remote URL from env var
response = llm.generate("Hello from remote server!")
```

```python
# Docker deployment with LMStudio
os.environ['LMSTUDIO_BASE_URL'] = 'http://lmstudio-container:1234/v1'

# Both provider discovery and LLM creation use the custom URL
providers = get_all_providers_with_models(include_models=False)
llm = create_llm('lmstudio', model='qwen3-4b')
```

```python
# OpenAI-compatible proxy (Portkey, etc.)
os.environ['OPENAI_BASE_URL'] = 'https://api.portkey.ai/v1'

llm = create_llm('openai', model='gpt-5-mini')
# All requests route through Portkey for observability and caching
```

**Benefits:**
- Remote server support without code changes
- Docker-friendly networking configuration
- Non-standard port support for multi-instance deployments
- Accurate provider availability detection in `get_all_providers_with_models()`
- OpenAI-compatible proxy and gateway support

---

### Programmatic Provider Configuration (v2.6.2)

Configure provider settings at runtime without relying on environment variables. Perfect for web UIs, Docker startup scripts, testing, and multi-tenant deployments.

**Simple API:**

```python
from abstractcore.config import configure_provider, get_provider_config, clear_provider_config
from abstractcore import create_llm

# Set base URL programmatically
configure_provider('ollama', base_url='http://192.168.1.100:11434')

# All future create_llm() calls automatically use the configured URL
llm = create_llm('ollama', model='llama3:8b')  # Uses http://192.168.1.100:11434

# Query current configuration
config = get_provider_config('ollama')
print(config)  # {'base_url': 'http://192.168.1.100:11434'}

# Clear configuration (revert to env var / default)
configure_provider('ollama', base_url=None)

# Or clear all providers
clear_provider_config()
```

**Priority System:**
1. **Constructor parameter** (highest): `create_llm("ollama", base_url="...")`
2. **Runtime configuration**: `configure_provider('ollama', base_url="...")`
3. **Environment variable**: `OLLAMA_BASE_URL`
4. **Default value** (lowest): `http://localhost:11434`

**Use Cases:**

**Web UI Settings Page:**

```python
from fastapi import FastAPI
from abstractcore.config import configure_provider
from abstractcore.providers import get_all_providers_with_models

@app.post("/api/settings/update-provider")
async def update_provider(provider: str, base_url: str):
    # Configure at runtime (no env vars needed!)
    configure_provider(provider, base_url=base_url)

    # Provider discovery automatically uses new config
    providers = get_all_providers_with_models(include_models=False)
    result = next((p for p in providers if p['name'] == provider), None)

    return {
        "provider": provider,
        "base_url": base_url,
        "status": result['status']
    }
```

**Docker Startup Script:**

```python
import os
from abstractcore.config import configure_provider

def configure_from_env():
    """Configure providers from Docker environment on startup."""
    if url := os.getenv('OLLAMA_URL'):
        configure_provider('ollama', base_url=url)
    if url := os.getenv('LMSTUDIO_URL'):
        configure_provider('lmstudio', base_url=url)

# Call on application startup
configure_from_env()
```

**Integration Testing:**

```python
from abstractcore.config import configure_provider, clear_provider_config

def test_with_mock_server():
    try:
        configure_provider('ollama', base_url='http://mock-server:11434')
        # Test code here
    finally:
        clear_provider_config('ollama')
```

**Multi-tenant Deployment:**

```python
from abstractcore.config import configure_provider

def configure_for_tenant(tenant_id: str):
    """Configure provider URLs based on tenant."""
    tenant_config = load_tenant_config(tenant_id)
    configure_provider('ollama', base_url=tenant_config['ollama_url'])
    configure_provider('lmstudio', base_url=tenant_config['lmstudio_url'])
```

**Benefits:**
- **No environment variables required**: Configure providers programmatically
- **Runtime updates**: Change URLs without restarting application
- **Clean API**: Simple `configure_provider()` function
- **Case-insensitive**: Provider names like 'Ollama' or 'ollama' both work
- **Provider discovery**: `get_all_providers_with_models()` uses runtime config
- **Testing-friendly**: Easy to set and clear config in tests
- **Multi-value support**: Configure multiple settings (base_url, timeout, etc.)

---

## Media Handling System

AbstractCore provides a **production-ready unified media handling system** that enables seamless file attachment and processing across all LLM providers and models. The system automatically processes images, documents, and other media files using the same simple API, with intelligent provider-specific formatting and graceful fallback handling.

### Key Features

- **Universal API**: Same `media=[]` parameter works across all providers (OpenAI, Anthropic, Ollama, LMStudio, etc.)
- **CLI Integration**: Simple `@filename` syntax in CLI for instant file attachment
- **Intelligent Processing**: Automatic file type detection with specialized processors for each format
- **Provider Adaptation**: Automatic formatting for each provider's API requirements
- **Robust Fallback**: Graceful degradation when advanced processing fails
- **Cross-Format Support**: Images, PDFs, Office documents, CSV/TSV, text files all work seamlessly

### Quick Start

```python
from abstractcore import create_llm

# Works with any provider - just change the provider name
llm = create_llm("openai", model="gpt-4o", api_key="your-key", temperature=0.7, seed=42)
response = llm.generate(
    "What's in this image and document?",
    media=["photo.jpg", "report.pdf"]
)

# Same code works with any provider
llm = create_llm("anthropic", model="claude-haiku-4-5")
response = llm.generate(
    "Analyze these materials",
    media=["chart.png", "data.csv", "presentation.pptx"]
)
```

### Deterministic Generation

```python
from abstractcore import create_llm

# Best-effort deterministic sampling (when provider/model supports it)
llm = create_llm("openai", model="gpt-5-mini", seed=42, temperature=0.0)

response1 = llm.generate("Write exactly 3 words about coding")
response2 = llm.generate("Write exactly 3 words about coding")
print(response1.content)
print(response2.content)

# Notes:
# - `seed` is best-effort and may not be supported by every provider/model.
# - For maximum consistency, use `temperature=0.0`.
```

### CLI Integration

Use the simple `@filename` syntax to attach any file type:

```bash
# PDF Analysis
python -m abstractcore.utils.cli --prompt "What is this document about? @report.pdf"

# Office Documents
python -m abstractcore.utils.cli --prompt "Summarize this presentation @slides.pptx"
python -m abstractcore.utils.cli --prompt "What data is in @spreadsheet.xlsx"
python -m abstractcore.utils.cli --prompt "Analyze this document @contract.docx"

# Data Files
python -m abstractcore.utils.cli --prompt "What patterns are in @sales_data.csv"

# Images
python -m abstractcore.utils.cli --prompt "What's in this image? @screenshot.png"

# Mixed Media
python -m abstractcore.utils.cli --prompt "Compare @chart.png and @data.csv and explain trends"
```

### Supported File Types

**90+ File Extensions Supported** - The media system supports comprehensive file type coverage:

**Images (Vision Models):**
- **Formats**: PNG, JPEG, GIF, WEBP, BMP, TIFF, ICO (9 formats)
- **Features**: Automatic optimization, resizing, format conversion, EXIF handling

**Documents:**
- **Binary Documents**: PDF, DOCX, XLSX, PPTX, ODT, RTF (9 formats)
  - **PDF**: Full text extraction with PyMuPDF4LLM, preserves formatting and structure
  - **Office**: DOCX, XLSX, PPTX with complete content extraction using Unstructured library
    - **Word**: Full document analysis with structure preservation
    - **Excel**: Sheet-by-sheet extraction with data analysis
    - **PowerPoint**: Slide content extraction with comprehensive analysis

**Text Files (90+ extensions):**
- **Core Data Formats**: TXT, MD, Markdown, CSV, TSV, JSON, JSONL, XML, YAML, TOML, INI, CFG, CONF
- **Programming Languages**: Python (.py), JavaScript (.js), Java, C/C++ (.c, .cpp), Go, Rust (.rs), Ruby (.rb), PHP, R (.r, .R), SQL, Julia (.jl), Lua, Dart, Swift, Kotlin, Scala, and more
- **Notebooks**: Jupyter (.ipynb), R Markdown (.rmd, .Rmd), Quarto (.qmd)
- **Web & Markup**: HTML, CSS, SCSS, SASS, LESS, Vue, Svelte, JSX, TSX, RST, TeX, AsciiDoc
- **Build & Scripts**: Shell (.sh, .bash, .zsh), Dockerfile, CMake, Gradle, Makefile
- **Logs & Output**: .log, .out, .err
- **Unknown Text Files**: Automatically detected via content analysis (no extension required)

**Programmatic Access:**
```python
from abstractcore.media.types import get_all_supported_extensions, get_supported_extensions_by_type, MediaType

# Get all formats
all_formats = get_all_supported_extensions()
print(f"Text: {len(all_formats['text'])} extensions")  # 90+
print(f"Images: {len(all_formats['image'])} extensions")  # 9
print(f"Documents: {len(all_formats['document'])} extensions")  # 9

# Check specific extension
text_exts = get_supported_extensions_by_type(MediaType.TEXT)
print('r' in text_exts)  # True - R scripts supported
print('ipynb' in text_exts)  # True - Jupyter notebooks supported
```

**Content Detection Fallback**: Files with unknown extensions are analyzed for text content. If the file contains valid text (UTF-8, Latin-1, etc.), it's automatically processed as a text file. This means you can attach ANY text-based file, even with custom or missing extensions.

### How It Works Behind the Scenes

The media system uses a **four-layer architecture** that processes and adapts content for each provider:

#### **Layer 1: Input Acceptance (BaseProvider)**
```python
# Three input formats accepted by media= parameter:
media = [
    "file.pdf",                    # File path (string)
    MediaContent(...),             # MediaContent object
    {"type": "image", "url": "..."} # Dictionary (converted to MediaContent)
]
```

**Location**: `abstractcore/providers/base.py::_process_media_content()`
- Accepts file paths, `MediaContent` objects, or dicts
- Uses `AutoMediaHandler` to process file paths into `MediaContent` objects
- Returns standardized list of `MediaContent` objects to provider

#### **Layer 2: File Processing (AutoMediaHandler + Processors)**
```python
# AutoMediaHandler selects processor based on file type
# Location: abstractcore/media/auto_handler.py

file_path = "report.pdf"
â†’ AutoMediaHandler detects PDF
â†’ PDFProcessor extracts text with PyMuPDF4LLM
â†’ Returns MediaContent(media_type=DOCUMENT, content="text...", content_format=TEXT)

file_path = "photo.jpg"
â†’ AutoMediaHandler detects IMAGE
â†’ ImageProcessor loads, optimizes, converts to base64
â†’ Returns MediaContent(media_type=IMAGE, content="base64...", content_format=BASE64)
```

**Available Processors**:
- `ImageProcessor` (abstractcore/media/processors/image_processor.py): Resolution optimization, format conversion
- `PDFProcessor` (abstractcore/media/processors/pdf_processor.py): Full text extraction with PyMuPDF4LLM
- `OfficeProcessor` (abstractcore/media/processors/office_processor.py): DOCX/XLSX/PPTX with Unstructured library
- `TextProcessor` (abstractcore/media/processors/text_processor.py): Plain text, CSV, JSON, TSV

#### **Layer 3: Provider-Specific Formatting (Provider Handlers)**
```python
# Each provider has a specialized handler that formats MediaContent
# for its specific API requirements

# OpenAI (abstractcore/media/handlers/openai_handler.py)
OpenAIMediaHandler.create_multimodal_message(text, media_contents)
â†’ {
    "role": "user",
    "content": [
      {"type": "text", "text": "Analyze these files"},
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0..."}},
      {"type": "text", "text": "PDF Content: # Report Title\n\nExecutive Summary..."}
    ]
  }

# Anthropic (abstractcore/media/handlers/anthropic_handler.py)
AnthropicMediaHandler.create_multimodal_message(text, media_contents)
â†’ {
    "role": "user",
    "content": [
      {"type": "text", "text": "Analyze these files"},
      {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": "iVBORw0..."}},
      {"type": "text", "text": "PDF Content: # Report Title\n\nExecutive Summary..."}
    ]
  }

# Local Providers (abstractcore/media/handlers/local_handler.py)
LocalMediaHandler.create_multimodal_message(text, media_contents)
â†’ For vision models: Uses provider's native vision API (Ollama /api/chat with images)
â†’ For text models: Embeds extracted text directly into prompt
```

#### **Layer 4: API Call (Provider Implementation)**
```python
# Provider's _generate_internal() receives formatted message and calls API
# Location: abstractcore/providers/openai_provider.py, anthropic_provider.py, etc.

# OpenAI Provider (_generate_internal)
if media:
    from ..media.handlers import OpenAIMediaHandler
    media_handler = OpenAIMediaHandler(self.model_capabilities)
    multimodal_message = media_handler.create_multimodal_message(prompt, media)
    api_messages.append(multimodal_message)
# â†’ Calls OpenAI API with formatted message

# Anthropic Provider (_generate_internal)
if media:
    from ..media.handlers import AnthropicMediaHandler
    media_handler = AnthropicMediaHandler(self.model_capabilities)
    multimodal_message = media_handler.create_multimodal_message(prompt, media)
    api_messages.append(multimodal_message)
# â†’ Calls Anthropic API with formatted message
```

### **Complete Flow Example**

```python
from abstractcore import create_llm

llm = create_llm("openai", model="gpt-4o")
response = llm.generate("Analyze these", media=["chart.png", "report.pdf"])

# Flow:
# 1. BaseProvider._process_media_content(["chart.png", "report.pdf"])
#    â†’ Uses AutoMediaHandler to detect file types and select processors
#    â†’ ImageProcessor converts chart.png to base64 MediaContent
#    â†’ PDFProcessor extracts text from report.pdf to text MediaContent
#    â†’ Returns [MediaContent(IMAGE, base64...), MediaContent(DOCUMENT, text...)]
#
# 2. OpenAIProvider._generate_internal(media=[MediaContent, MediaContent])
#    â†’ Creates OpenAIMediaHandler
#    â†’ Formats MediaContent objects into OpenAI API format
#    â†’ Returns multimodal message with text + image_url + text content
#
# 3. OpenAI API call with formatted message
#    â†’ GPT-4o processes text, image, and PDF content together
```

For complete media handling documentation, see: [Media Handling System Guide](docs/media-handling-system.md)

---

## Vision Capabilities

AbstractCore provides comprehensive **vision capabilities** that enable seamless image analysis across multiple AI providers and models. The system automatically handles image optimization, provider-specific formatting, and intelligent fallback mechanisms.

### Overview

AbstractCore provides comprehensive vision capabilities across all major AI providers with automatic image optimization and intelligent fallback mechanisms. The same code works identically whether you're using cloud APIs (OpenAI, Anthropic) or local models (Ollama, LMStudio).

### Supported Providers and Models

**Cloud Providers:**
- **OpenAI**: GPT-4o, GPT-4 Turbo Vision (multiple images, up to 4096Ã—4096)
- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Haiku (up to 20 images, 1568Ã—1568)

**Local Providers:**
- **Ollama**: qwen2.5vl:7b, llama3.2-vision:11b, gemma3:4b
- **LMStudio**: qwen/qwen2.5-vl-7b, google/gemma-3n-e4b
- **HuggingFace**: Qwen2.5-VL variants, LLaVA models
- **MLX**: Vision models via MLX framework

**Image Formats**: PNG, JPEG, GIF, WEBP, BMP, TIFF with automatic optimization

### Basic Vision Analysis

```python
from abstractcore import create_llm

# Works with any vision-capable provider
llm = create_llm("openai", model="gpt-4o")

# Single image analysis
response = llm.generate(
    "What objects do you see in this image?",
    media=["photo.jpg"]
)

# Multiple images comparison
response = llm.generate(
    "Compare these architectural styles and identify differences",
    media=["building1.jpg", "building2.jpg", "building3.jpg"]
)
```

### Cross-Provider Consistency

```python
# Same code works across all providers
image_files = ["chart.png", "document.pdf"]
prompt = "Analyze the data in these files"

# All work identically
openai_response = create_llm("openai", model="gpt-4o").generate(prompt, media=image_files)
anthropic_response = create_llm("anthropic", model="claude-haiku-4-5").generate(prompt, media=image_files)
ollama_response = create_llm("ollama", model="qwen2.5vl:7b").generate(prompt, media=image_files)
```

### Vision Fallback System

The **Vision Fallback System** enables text-only models to process images through a transparent two-stage pipeline:

```bash
# Configure vision fallback (one-time setup)
abstractcore --download-vision-model              # Download local model (recommended)
# OR
abstractcore --set-vision-provider ollama qwen2.5vl:7b    # Use existing Ollama model
# OR
abstractcore --set-vision-provider openai gpt-4o  # Use cloud API
```

```python
# After configuration, text-only models can process images seamlessly
text_llm = create_llm("lmstudio", model="qwen/qwen3-4b-2507")  # No native vision

response = text_llm.generate(
    "What's happening in this image?",
    media=["complex_scene.jpg"]
)
# Works transparently: vision model analyzes image â†’ text model processes description
```

### Automatic Resolution Optimization

AbstractCore automatically optimizes images for each model's maximum capability:

```python
# Images automatically optimized per model
llm = create_llm("openai", model="gpt-4o")
response = llm.generate("Analyze this", media=["photo.jpg"])  # Auto-resized to 4096Ã—4096

llm = create_llm("ollama", model="qwen2.5vl:7b")
response = llm.generate("Analyze this", media=["photo.jpg"])  # Auto-resized to 3584Ã—3584
```

### Structured Vision Analysis

```python
# Get structured responses with specific requirements
llm = create_llm("openai", model="gpt-4o")

response = llm.generate("""
Analyze this image and provide:
- objects: list of objects detected
- colors: dominant colors
- setting: location/environment
- activities: what's happening

Format as JSON.
""", media=["scene.jpg"])

import json
analysis = json.loads(response.content)
```

For complete vision capabilities documentation, see: [Vision Capabilities Guide](docs/vision-capabilities.md)

---

## Getting Started

### Create LLM with Any Provider

AbstractCore supports 9 providers with **identical syntax**. Choose your provider:

```python
from abstractcore import create_llm

# OpenAI (requires OPENAI_API_KEY)
llm = create_llm("openai", model="gpt-5-mini")

# Anthropic (requires ANTHROPIC_API_KEY)
llm = create_llm("anthropic", model="claude-haiku-4-5")

# OpenRouter (requires OPENROUTER_API_KEY)
llm = create_llm("openrouter", model="openai/gpt-4o-mini")

# Ollama (local - ensure Ollama is running)
llm = create_llm("ollama", model="qwen3:4b-instruct")

# LMStudio (local - ensure LMStudio server is running)
llm = create_llm("lmstudio", model="qwen/qwen3-4b-2507")

# MLX (Apple Silicon only)
llm = create_llm("mlx", model="mlx-community/Qwen3-4B-4bit")

# HuggingFace (local or downloaded; token only needed for gated models)
llm = create_llm("huggingface", model="unsloth/Qwen3-4B-Instruct-2507-GGUF")

# vLLM (local server - set VLLM_BASE_URL if not default http://localhost:8000/v1)
llm = create_llm("vllm", model="Qwen/Qwen3-Coder-30B-A3B-Instruct")

# OpenAI-compatible (any compatible endpoint; set OPENAI_COMPATIBLE_BASE_URL if needed)
llm = create_llm("openai-compatible", model="default")

# List available models for any provider
available_models = llm.list_available_models()
print(f"Available models: {available_models}")

# Same interface for all providers
response = llm.generate("What is the capital of France?")
print(response.content)
```

### Essential Patterns

```python
from abstractcore import create_llm
from abstractcore.tools import tool
from pydantic import BaseModel

# 1. Basic Usage - Works with any provider
llm = create_llm("openai", model="gpt-5-mini")  # or any provider above
response = llm.generate("What is the capital of France?")
print(response.content)

# 2. Universal Tool Calling
@tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    return f"Weather in {city}: 72Â°F, Sunny"

response = llm.generate("What's the weather in Paris?", tools=[get_weather])

# 3. Structured Output
class Person(BaseModel):
    name: str
    age: int

person = llm.generate("Extract: John Doe is 25", response_model=Person)
print(f"{person.name}, age {person.age}")

# See comprehensive guide: docs/structured-output.md
# - Native server-side enforcement (Ollama, LMStudio, HuggingFace GGUF)
# - Prompted with validation (MLX, HuggingFace Transformers)
# - 100% validation success rate across 23 comprehensive tests
# - Schema design patterns and production deployment
```

### Common Development Patterns

```python
# Streaming responses for user interfaces
for chunk in llm.generate("Tell me a story", stream=True):
    print(chunk.content, end="", flush=True)

# Session management for chatbots
from abstractcore import BasicSession
session = BasicSession(llm, system_prompt="You are a helpful assistant.")
session.add_message('user', 'My name is Alice')
response = session.generate('What is my name?')  # Remembers context

# Error handling for production apps
try:
    response = llm.generate("Complex request")
except Exception as e:
    print(f"Error: {e}")
    # Handle gracefully

# Configuration for different environments
from abstractcore import create_llm

# Development (local models)
dev_llm = create_llm("ollama", model="qwen3:4b")

# Production (cloud APIs)
prod_llm = create_llm("openai", model="gpt-5-mini", api_key="your-key")

# Cost-sensitive applications (smaller models)
cost_llm = create_llm("anthropic", model="claude-haiku-4-5")
```

---

## Provider Health Monitoring (v2.4.4+)

Check provider connectivity and availability before making requests. All providers now include a `.health()` method that returns structured health information.

### Basic Health Check

```python
from abstractcore import create_llm

# Check if provider is online and available
provider = create_llm("ollama", model="llama2")
health = provider.health(timeout=3.0)  # 3-second timeout

if health["status"]:
    print(f"Supported: {health['provider']} is healthy")
    print(f"   Models available: {health['model_count']}")
    print(f"   Response time: {health['latency_ms']}ms")
    # Safe to proceed with generation
    response = provider.generate("Hello!")
else:
    print(f"Not Supported: {health['provider']} is offline")
    print(f"   Error: {health['error']}")
    # Implement fallback strategy
```

### Health Check Response Structure

```python
{
    "status": bool,              # True if provider is healthy/online
    "provider": str,             # Provider class name (e.g., "OllamaProvider")
    "models": List[str] | None,  # Available models if online, None if offline
    "model_count": int,          # Number of available models (0 if offline)
    "error": str | None,         # Error message if offline, None if healthy
    "latency_ms": float          # Health check duration in milliseconds
}
```

### Production Pattern: Fallback Chain

```python
from abstractcore import create_llm

# Try providers in order of preference with automatic fallback
providers_to_try = [
    ("ollama", {"model": "llama2"}),
    ("lmstudio", {"model": "local-model"}),
    ("openai", {"model": "gpt-5-mini", "api_key": "sk-..."})
]

provider = None
for provider_name, kwargs in providers_to_try:
    try:
        candidate = create_llm(provider_name, **kwargs)
        health = candidate.health(timeout=2.0)
        if health["status"]:
            provider = candidate
            print(f"Using {health['provider']} ({health['model_count']} models)")
            break
    except Exception as e:
        print(f"Skipping {provider_name}: {e}")
        continue

if provider is None:
    raise RuntimeError("No healthy providers available")

# Proceed with healthy provider
response = provider.generate("Hello, world!")
```

---

## Token Counting and Usage Tracking (v2.4.4+)

All providers now consistently track both input (context) and output (generated) tokens using a centralized token counting infrastructure.

### Token Usage Structure

Every response includes detailed token usage information:

```python
from abstractcore import create_llm

llm = create_llm("anthropic", model="claude-haiku-4-5")
response = llm.generate("Explain quantum computing in simple terms")

# All providers return consistent token usage
print(f"Input tokens (context): {response.usage['prompt_tokens']}")
print(f"Output tokens (generated): {response.usage['completion_tokens']}")
print(f"Total tokens (billing): {response.usage['total_tokens']}")
```

### Token Counting Methods

- **API Providers** (OpenAI, Anthropic, Ollama, LMStudio): Use exact API-provided token counts
- **Local Providers** (MLX, HuggingFace): Use centralized `TokenUtils` with content-type detection

```python
from abstractcore.utils.token_utils import TokenUtils

# Estimate tokens before making request
text = "Your input text here..."
estimated = TokenUtils.estimate_tokens(text, model="claude-haiku-4-5")
print(f"Estimated tokens: {estimated}")

# Content-type aware estimation
code_text = "def hello(): print('world')"
code_tokens = TokenUtils.estimate_tokens(code_text, model="gpt-4o")
# Automatically detects code and applies appropriate density factor

json_text = '{"key": "value", "array": [1, 2, 3]}'
json_tokens = TokenUtils.estimate_tokens(json_text, model="claude-haiku-4-5")
# Automatically detects JSON and accounts for structural tokens
```

### Token Budget Planning

```python
from abstractcore import create_llm

# Set token limits with unified parameters
llm = create_llm(
    "openai",
    model="gpt-4o",
    max_tokens=128000,          # Total context window
    max_output_tokens=16000,    # Maximum generated tokens
    max_input_tokens=112000     # Maximum input tokens (auto-calculated if not set)
)

# Track token usage across conversation
total_input = 0
total_output = 0

for i in range(5):
    response = llm.generate(f"Question {i+1}")
    total_input += response.usage['prompt_tokens']
    total_output += response.usage['completion_tokens']

print(f"Conversation used {total_input} input + {total_output} output tokens")
print(f"Remaining budget: {llm.max_tokens - (total_input + total_output)} tokens")
```

---

## Provider Discovery

Discover available providers and their capabilities programmatically. This is particularly useful for building applications that need to adapt to available providers or for administrative tooling.

### Basic Provider Discovery

```python
from abstractcore.providers import get_all_providers_with_models

# Get information about all available providers
providers = get_all_providers_with_models()

for provider in providers:
    print(f"Provider: {provider['display_name']}")
    print(f"Models available: {provider['model_count']}")
    print(f"Local provider: {provider['local_provider']}")
    print(f"Auth required: {provider['authentication_required']}")
    print(f"Features: {', '.join(provider['supported_features'])}")
    if provider.get('installation_extras'):
        print(f"Install: pip install abstractcore[{provider['installation_extras']}]")
    print("---")
```

### Provider Status and Model Discovery

```python
from abstractcore.providers import (
    list_available_providers,        # Get provider names
    get_provider_info,              # Detailed provider info
    is_provider_available,          # Check availability
    get_available_models_for_provider  # Get models for provider
)

# Check available providers
available = list_available_providers()
print(f"Available providers: {available}")

# Get models for a specific provider
if is_provider_available("ollama"):
    models = get_available_models_for_provider("ollama")
    print(f"Ollama models: {models[:5]}...")  # First 5 models

# Get detailed provider information
if is_provider_available("openai"):
    info = get_provider_info("openai")
    print(f"Default model: {info.default_model}")
    print(f"Features: {info.supported_features}")
```

### HTTP API Discovery

Access provider information through the HTTP API:

```bash
# Get all providers
curl http://localhost:8000/providers

# Include model lists (slower)
curl "http://localhost:8000/providers?include_models=true"

# Get models (optionally filtered by provider/capabilities)
curl "http://localhost:8000/v1/models?provider=ollama"
```

The response includes comprehensive metadata:
```json
{
  "providers": [
    {
      "name": "openai",
      "display_name": "OpenAI",
      "model_count": 15,
      "status": "available",
      "local_provider": false,
      "authentication_required": true,
      "supported_features": ["chat", "completion", "embeddings", "native_tools"],
      "models": ["gpt-5-mini", "gpt-5-nano-2025-08-07", "gpt-4o"]
    }
  ]
}
```

---

## Universal Tool Calling

Enable LLMs to call Python functions across **ALL** providers, including those without native tool support. This allows you to build interactive applications where the LLM can execute code, query databases, make API calls, or perform any Python operation.

```python
from abstractcore import create_llm, BasicSession
from abstractcore.tools import tool

@tool
def get_weather(city: str) -> str:
    """Get current weather for a specified city."""
    return f"Weather in {city}: 72Â°F, Sunny"

@tool
def calculate(expression: str) -> float:
    """Perform mathematical calculations."""
    return eval(expression)  # Simplified for demo

# Method 1: Direct provider usage
llm = create_llm("ollama", model="qwen3:4b-instruct")
response = llm.generate(
    "What's the weather in Tokyo and what's 15 * 23?",
    tools=[get_weather, calculate]  # Pass tools directly
)

# Method 2: Session with registered tools (recommended for conversations)
session = BasicSession(llm, tools=[get_weather, calculate])
response = session.generate("What's the weather in Tokyo?")  # Uses registered tools

# Method 3: Session with per-call tools (overrides registered tools)
response = session.generate("Calculate 15 * 23", tools=[calculate])  # Only use calculate
```

### Enhanced Metadata

The `@tool` decorator supports rich metadata that gets automatically injected into system prompts:

```python
@tool(
    description="Search the database for records matching the query",
    tags=["database", "search", "query"],
    when_to_use="When the user asks for specific data from the database",
    examples=[
        {
            "description": "Find all users named John",
            "arguments": {
                "query": "name=John",
                "table": "users"
            }
        }
    ]
)
def search_database(query: str, table: str = "users") -> str:
    """Search the database for records matching the query."""
    return f"Searching {table} for: {query}"
```

### Built-in Tools

AbstractCore includes a comprehensive set of ready-to-use tools in `abstractcore.tools.common_tools`:

```python
from abstractcore.tools.common_tools import fetch_url, search_files, read_file, list_files

# Intelligent web content fetching with automatic parsing
result = fetch_url("https://api.github.com/repos/python/cpython")
# Automatically detects JSON, HTML, images, PDFs, etc. and provides structured analysis

# File system operations
files = search_files("def.*fetch", ".", file_pattern="*.py")  # Find function definitions
content = read_file("config.json")  # Read file contents
directory_listing = list_files(".", pattern="*.py", recursive=True)

# Use with any LLM
llm = create_llm("anthropic", model="claude-haiku-4-5")
response = llm.generate(
    "Analyze this API response and summarize the key information",
    tools=[fetch_url]
)
```

**Available Built-in Tools:**
- `fetch_url` - Intelligent web content fetching with automatic content type detection and parsing
- `search_files` - Search for text patterns inside files using regex
- `list_files` - Find and list files by names/paths using glob patterns
- `read_file` - Read file contents with optional line range selection
- `write_file` - Write content to files with directory creation
- `edit_file` - Edit files using pattern matching and replacement
- `web_search` - Search the web using DuckDuckGo
- `execute_command` - Execute shell commands safely with security controls

### Universal Tool Support

AbstractCore's tool system works across all providers through two mechanisms:

1. **Native Tool Support**: For providers with native tool APIs (OpenAI, Anthropic)
2. **Intelligent Prompting**: For providers without native tool support (Ollama, MLX, LMStudio)

AbstractCore automatically:
- Detects the model architecture (Qwen3, LLaMA3, etc.)
- Formats tools with examples into the system prompt
- Parses tool calls into structured `GenerateResponse.tool_calls`
- (Optional/legacy) can auto-execute tools with `execute_tools=True` (deprecated; prefer host/runtime execution)

### Architecture-Aware Tool Call Detection

| Architecture | Format | Example |
|-------------|--------|---------|
| **Qwen3** | `<|tool_call|>...JSON...</|tool_call|>` | `<|tool_call|>{"name": "get_weather", "arguments": {"city": "Paris"}}</|tool_call|>` |
| **LLaMA3** | `<function_call>...JSON...</function_call>` | `<function_call>{"name": "get_weather", "arguments": {"city": "Paris"}}</function_call>` |
| **OpenAI/Anthropic** | Native API tool calls | Structured JSON in API response |
| **XML-based** | `<tool_call>...JSON...</tool_call>` | `<tool_call>{"name": "get_weather", "arguments": {"city": "Paris"}}</tool_call>` |

### Tool Chaining

Tools can call other tools or return data that triggers additional tool calls:

```python
@tool
def get_user_location(user_id: str) -> str:
    """Get the location of a user."""
    locations = {"user123": "Paris", "user456": "Tokyo"}
    return locations.get(user_id, "Unknown")

@tool
def get_weather(city: str) -> str:
    """Get weather for a city."""
    return f"Weather in {city}: 72Â°F, sunny"

# LLM can chain these tools:
response = llm.generate(
    "What's the weather like for user123?",
    tools=[get_user_location, get_weather]
)
# LLM will first call get_user_location, then get_weather with the result
```

### Tool Syntax Rewriting

AbstractCore can optionally preserve and rewrite *tool-call markup inside assistant content* so downstream runtimes can parse tool calls from plain text consistently.

There are two related controls:

- Python API: `tool_call_tags` (per-call)
- HTTP server: `agent_format` (per-request)

#### Python API: `tool_call_tags` (per-call)

Default (`tool_call_tags is None`):
- `response.tool_calls` is populated when tool calls are detected (native tools or prompted tags).
- Tool-call markup is stripped from `response.content` for clean UX/history.

When set:
- Tool-call markup is kept in `content` and rewritten to the requested format.
- `response.tool_calls` remains the canonical machine-readable representation for execution.

Supported values:
- Predefined formats: `qwen3`, `llama3`, `xml`, `gemma`
- Custom tags: `"START,END"` (exact strings) or `"MYTAG"` (auto `<MYTAG>...</MYTAG>`)

```python
llm = create_llm("ollama", model="qwen3:4b-instruct")
response = llm.generate(
    "What's the weather in Paris?",
    tools=[get_weather],
    tool_call_tags="llama3",
)
print(response.content)     # contains <function_call>...</function_call>
print(response.tool_calls)  # structured list of {name, arguments, call_id}
```

Streaming works the same:

```python
for chunk in llm.generate(
    "What's the weather in Paris?",
    tools=[get_weather],
    stream=True,
    tool_call_tags="llama3",
):
    print(chunk.content, end="", flush=True)
    if chunk.tool_calls:
        print(chunk.tool_calls)
```

Notes:
- Tool arguments are parsed using a tolerant JSON-ish loader (single quotes / Python literals) to handle common OSS outputs.
- Tool execution is not automatic by default (`execute_tools=False`); execute `response.tool_calls` in your app/agent runtime.

---

## Model Capability Filtering

AbstractCore provides a clean, type-safe system for filtering models based on their input and output capabilities. This allows you to programmatically discover models that can handle specific types of data.

### Key Concepts

**Input Capabilities**: What types of data can the model accept and analyze?
- `TEXT`: All models support text input
- `IMAGE`: Models that can analyze images (vision models)
- `AUDIO`: Models that can process audio input
- `VIDEO`: Models that can analyze video content

**Output Capabilities**: What types of data can the model generate?
- `TEXT`: Models that generate text responses (most common)
- `EMBEDDINGS`: Models that generate vector embeddings

### Python API

```python
from abstractcore.providers import OllamaProvider, ModelInputCapability, ModelOutputCapability

# Vision models
vision_models = OllamaProvider.list_available_models(
    input_capabilities=[ModelInputCapability.IMAGE]
)

# Embedding models
embedding_models = OllamaProvider.list_available_models(
    output_capabilities=[ModelOutputCapability.EMBEDDINGS]
)

# Text-only models
text_models = OllamaProvider.list_available_models(
    input_capabilities=[ModelInputCapability.TEXT],
    output_capabilities=[ModelOutputCapability.TEXT]
)
```

### HTTP API Filtering

The server provides RESTful endpoints for model discovery with capability filtering:

```bash
# Vision models
curl http://localhost:8000/v1/models?input_type=image

# Embedding models
curl http://localhost:8000/v1/models?output_type=embeddings

# Provider-specific filtering
curl http://localhost:8000/v1/models?provider=ollama&input_type=image

# Combined filters
curl http://localhost:8000/v1/models?provider=openai&input_type=image
```

### Programmatic Model Discovery

```python
from abstractcore.providers.model_capabilities import (
    get_model_input_capabilities,
    get_model_output_capabilities,
    get_capability_summary
)

# Get detailed capabilities for a specific model
input_caps = get_model_input_capabilities("gpt-4o")
print(f"Input capabilities: {[cap.value for cap in input_caps]}")
# Output: ['text', 'image', 'audio']

output_caps = get_model_output_capabilities("gpt-4o")
print(f"Output capabilities: {[cap.value for cap in output_caps]}")
# Output: ['text']

# Get comprehensive summary
summary = get_capability_summary("gpt-4-vision-preview")
print(summary)
# Output: {
#     'model_name': 'gpt-4-vision-preview',
#     'input_capabilities': ['text', 'image'],
#     'output_capabilities': ['text'],
#     'is_multimodal': True,
#     'is_embedding_model': False
# }
```

### Cross-Provider Compatibility

The capability filtering system works identically across all providers:

```python
from abstractcore.providers import (
    OpenAIProvider, AnthropicProvider, OllamaProvider,
    ModelInputCapability, ModelOutputCapability
)

# Same API across all providers
providers = [OpenAIProvider, AnthropicProvider, OllamaProvider]

for provider in providers:
    vision_models = provider.list_available_models(
        input_capabilities=[ModelInputCapability.IMAGE]
    )
    print(f"{provider.__name__}: {len(vision_models)} vision models")
```

### Benefits

- **Type-Safe**: Enum-based filtering prevents typos and provides IDE autocomplete
- **Consistent**: Same API across all providers (OpenAI, Anthropic, Ollama, etc.)
- **Flexible**: Support for multiple input capabilities (e.g., text + image)
- **Future-Proof**: Easy to extend with new capabilities (video, audio generation)
- **Backward Compatible**: Legacy parameters still supported
- **Server Integrated**: HTTP API filtering for web applications

---

## Core Features

### Session Management

Comprehensive conversation management with message history, optional analytics, and optional auto-compaction:

```python
from abstractcore import BasicSession, create_llm

llm = create_llm("openai", model="gpt-5-mini")
session = BasicSession(
    llm,
    system_prompt="You are a helpful assistant.",
    auto_compact=True,
    auto_compact_threshold=6000,
)

session.generate("My name is Alice")
response = session.generate("What's my name?")
print(response.content)

# Optional analytics (stored on the session object and included in `session.save(...)`)
session.generate_summary(preserve_recent=6, focus="key facts")
session.generate_assessment()
session.extract_facts(output_format="triples")

# Force compaction (in-place) or create a compacted copy
session.force_compact(preserve_recent=8, focus="technical decisions")
# compacted = session.compact(preserve_recent=8, focus="technical decisions")

# Timeouts
session.set_timeout(30)
session.set_tool_timeout(60)
session.set_recovery_timeout(60)

# Save / load (provider and tools are not serialized; pass them back in on load)
session.save("conversation.json")
loaded = BasicSession.load("conversation.json", provider=llm)
```

#### Session Analytics

Extract insights from conversation history:

```python
summary = session.generate_summary(preserve_recent=6, focus="decisions")
assessment = session.generate_assessment(criteria={"clarity": True, "coherence": True})
facts = session.extract_facts(output_format="jsonld")
```

#### Auto-Compaction System

Intelligent conversation history compression that preserves context:

```python
session = BasicSession(llm, auto_compact=True, auto_compact_threshold=6000)
# When the session grows beyond the threshold, `session.generate(...)` auto-compacts before sending the next request.
```

### Async/Await for Concurrent Requests

Execute multiple LLM requests concurrently for 3-10x faster batch operations:

```python
import asyncio
from abstractcore import create_llm
from abstractcore.core.session import BasicSession

# Basic concurrent execution
async def main():
    llm = create_llm("openai", model="gpt-5-mini")

    # Execute 3 requests concurrently
    tasks = [
        llm.agenerate("Summarize Python in one sentence"),
        llm.agenerate("Summarize JavaScript in one sentence"),
        llm.agenerate("Summarize Rust in one sentence")
    ]
    responses = await asyncio.gather(*tasks)

    for i, response in enumerate(responses, 1):
        print(f"{i}. {response.content}")

asyncio.run(main())
```

#### Async Streaming

Real-time token generation with async iterators:

```python
async def stream_example():
    llm = create_llm("anthropic", model="claude-haiku-4-5")

    async for chunk in llm.agenerate("Tell me a story", stream=True):
        print(chunk.content, end='', flush=True)

asyncio.run(stream_example())
```

#### Session Async

Async conversations with full history management:

```python
async def session_example():
    llm = create_llm("ollama", model="qwen3:4b")
    session = BasicSession(provider=llm)

    # Multi-turn async conversation
    response1 = await session.agenerate("My name is Alice")
    response2 = await session.agenerate("What's my name?")

    print(response2.content)  # References Alice from history

asyncio.run(session_example())
```

#### Multi-Provider Concurrent Comparison

Query multiple providers simultaneously:

```python
async def compare_providers():
    openai = create_llm("openai", model="gpt-5-mini")
    claude = create_llm("anthropic", model="claude-haiku-4-5")
    ollama = create_llm("ollama", model="qwen3:4b")

    # Get responses from 3 providers concurrently
    responses = await asyncio.gather(
        openai.agenerate("What is 2+2?"),
        claude.agenerate("What is 2+2?"),
        ollama.agenerate("What is 2+2?")
    )

    print(f"OpenAI: {responses[0].content}")
    print(f"Claude: {responses[1].content}")
    print(f"Ollama: {responses[2].content}")

asyncio.run(compare_providers())
```

**Key Features:**
- Works with all 9 providers (OpenAI, Anthropic, OpenRouter, Ollama, LMStudio, MLX, HuggingFace, vLLM, openai-compatible)
- Full streaming support with AsyncIterator
- Session async maintains conversation history
- Zero breaking changes to sync API
- Compatible with FastAPI and async web frameworks
- 3-10x speedup for batch operations

**Use Cases:**
- Batch document processing
- Multi-provider consensus/comparison
- Non-blocking web applications
- Parallel data extraction tasks
- High-throughput API endpoints

### Enhanced Error Messages (v2.6.0)

Authentication and connection errors include fix commands and setup URLs:

```python
from abstractcore import create_llm

# Example: Invalid OpenAI API key
try:
    llm = create_llm("openai", model="gpt-5-mini", api_key="invalid-key")
    llm.generate("test")
except Exception as e:
    print(e)
    # Output:
    # OPENAI authentication failed: Invalid API key
    # Fix: abstractcore --set-api-key openai YOUR_KEY
    # Get key: https://platform.openai.com/api-keys
```

**What You Get:**
- Problem: Clear error description
- Fix: Command to set API key
- URL: Direct link to get API key

**Supported Providers:**
- OpenAI: Links to https://platform.openai.com/api-keys
- Anthropic: Links to https://console.anthropic.com/settings/keys
- Ollama: Installation and start instructions
- LMStudio: Setup instructions

**Apply the Fix:**
```bash
# Set API key using CLI
abstractcore --set-api-key openai sk-your-actual-key-here

# Or use environment variable
export OPENAI_API_KEY=sk-your-actual-key-here
```

### Custom Base URLs (v2.6.0)

Configure custom API endpoints for OpenAI and Anthropic providers to enable proxies and enterprise gateways:

```python
from abstractcore import create_llm

# OpenAI-compatible proxy (e.g., Portkey)
llm = create_llm(
    "openai",
    model="gpt-5-mini",
    base_url="https://api.portkey.ai/v1",
    api_key="your-portkey-key"
)

# Local OpenAI-compatible server
llm = create_llm(
    "openai",
    model="local-model",
    base_url="http://localhost:8080/v1",
    api_key="not-needed"  # Some local servers don't validate
)

# OpenAI-compatible proxy for Anthropic
llm = create_llm(
    "anthropic",
    model="claude-haiku-4-5",
    base_url="https://custom-proxy.example.com/v1",
    api_key="your-proxy-key"
)
```

**Environment Variables:**
```bash
# Set custom base URLs
export OPENAI_BASE_URL="https://api.portkey.ai/v1"
export ANTHROPIC_BASE_URL="https://api.portkey.ai/v1"

# Now create_llm uses custom URLs automatically
python your_script.py
```

**Use Cases:**
- OpenAI-compatible proxies (Portkey, etc.) for observability, caching, and cost management
- Local OpenAI-compatible servers (e.g., text-generation-webui with OpenAI API)
- Enterprise gateways for security and compliance
- Custom endpoints for testing and development

**Note**: Azure OpenAI is NOT supported via base_url (it requires different SDK class). Use OpenAI-compatible proxies only.

---

### Model Downloads (v2.6.0)

Download models programmatically with async progress reporting for Ollama, HuggingFace, and MLX providers:

```python
from abstractcore import download_model

# Download Ollama model with progress
async for progress in download_model("ollama", "llama3:8b"):
    print(f"{progress.status.value}: {progress.message}")
    if progress.percent:
        print(f"  Progress: {progress.percent:.1f}%")

# Download HuggingFace model
async for progress in download_model("huggingface", "meta-llama/Llama-2-7b"):
    print(progress.message)

# Download HuggingFace gated model with token
async for progress in download_model(
    "huggingface",
    "meta-llama/Llama-2-7b",
    token="hf_..."
):
    print(progress.message)

# Download MLX model (same as HuggingFace)
async for progress in download_model("mlx", "mlx-community/Qwen3-4B-4bit"):
    print(progress.message)

# Custom Ollama server
async for progress in download_model(
    "ollama",
    "gemma3:1b",
    base_url="http://custom-server:11434"
):
    print(progress.message)
```

**DownloadProgress Fields:**
- `status`: DownloadStatus enum (STARTING, DOWNLOADING, VERIFYING, COMPLETE, ERROR)
- `message`: Human-readable status message
- `percent`: Optional progress percentage (0-100)
- `downloaded_bytes`: Optional bytes downloaded
- `total_bytes`: Optional total size

**Provider Support:**
| Provider | Support | Method |
|----------|---------|--------|
| Ollama | âœ… | `/api/pull` with streaming NDJSON |
| HuggingFace | âœ… | `huggingface_hub.snapshot_download` |
| MLX | âœ… | Same as HuggingFace (uses HF Hub) |
| LMStudio | âŒ | No download API (CLI/GUI only) |
| OpenAI/Anthropic | âŒ | Cloud-only |

**Use Cases:**
- Docker deployments: Download models through web UI
- Automated setup: Pre-download models in scripts
- User-friendly UIs: Stream progress to frontend via SSE
- Batch downloads: Prepare multiple models in advance


**Supported Providers:**
- OpenAI: `base_url` parameter + `OPENAI_BASE_URL` environment variable
- Anthropic: `base_url` parameter + `ANTHROPIC_BASE_URL` environment variable
- Ollama: Already supported via `base_url="http://localhost:11434"`
- LMStudio: Already supported via `base_url="http://localhost:1234/v1"`

---

### vLLM Provider (v2.6.4)

High-throughput GPU inference with advanced features for NVIDIA CUDA GPUs.

**Requirements:**
- NVIDIA GPU with CUDA support (NOT compatible with Apple Silicon, AMD, Intel)
- vLLM server running: `vllm serve Qwen/Qwen2.5-Coder-7B-Instruct --host 0.0.0.0 --port 8000`

**Basic Usage:**
```python
from abstractcore import create_llm

llm = create_llm("vllm", model="Qwen/Qwen2.5-Coder-7B-Instruct")
response = llm.generate("Write a Python function to sort a list")
print(response.content)
```

**Guided Decoding (100% format-compliant output):**
```python
# Regex-constrained generation
response = llm.generate(
    "Generate a valid US phone number",
    guided_regex=r"\d{3}-\d{3}-\d{4}"
)
print(response.content)  # Always matches format, e.g. "555-123-4567"

# JSON schema-constrained generation
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int
    email: str

response = llm.generate(
    "Extract: John Doe is 30 years old, john@example.com",
    guided_json=Person.model_json_schema()
)

# Grammar-constrained generation (EBNF)
response = llm.generate(
    "Generate SQL query",
    guided_grammar="..."  # EBNF grammar string
)
```

**Multi-LoRA (Adapter Management):**

Requires vLLM started with `--enable-lora --max-loras 4` flag.

```python
# Load specialized LoRA adapter into vLLM server
llm.load_adapter("sql-expert", "/path/to/sql-lora-adapter")

# List loaded adapters
adapters = llm.list_adapters()
print(adapters)  # ["sql-expert", ...]

# Unload when done
llm.unload_adapter("sql-expert")

# To generate with a loaded adapter, create a new provider instance:
sql_llm = create_llm("vllm", model="sql-expert")
response = sql_llm.generate("Write SQL for user table")
```

**Beam Search (Higher Accuracy):**
```python
response = llm.generate(
    "Solve this complex problem step by step...",
    use_beam_search=True,
    best_of=5
)
```

**Environment Variables:**
```bash
VLLM_BASE_URL=http://gpu-server:8000/v1  # Default: http://localhost:8000/v1
VLLM_API_KEY=your-key                     # Optional, for authenticated servers
```

**Multi-GPU Setup:**
```bash
# 4x GPU with tensor parallelism
vllm serve Qwen/Qwen2.5-Coder-7B-Instruct \
    --host 0.0.0.0 --port 8000 \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --max-num-seqs 128
```

---

### OpenAI-Compatible Provider (v2.6.4)

Generic provider for any OpenAI-compatible API endpoint.

**Supported Servers:**
- llama.cpp (`./server`)
- text-generation-webui (OpenAI extension)
- LocalAI
- FastChat
- Aphrodite
- SGLang
- Custom proxies

**Basic Usage:**
```python
from abstractcore import create_llm

# llama.cpp server
llm = create_llm("openai-compatible",
                 base_url="http://localhost:8080/v1",
                 model="llama-3.1-8b")
response = llm.generate("Hello!")

# text-generation-webui
llm = create_llm("openai-compatible",
                 base_url="http://localhost:5000/v1",
                 model="mistral-7b")

# LocalAI
llm = create_llm("openai-compatible",
                 base_url="http://localhost:8080/v1",
                 model="gpt4all-j")

# With API key (optional for most local servers)
llm = create_llm("openai-compatible",
                 base_url="https://proxy.example.com/v1",
                 api_key="my-key",
                 model="custom-model")
```

**Environment Variables:**
```bash
OPENAI_COMPATIBLE_BASE_URL=http://localhost:8080/v1  # Required
OPENAI_COMPATIBLE_API_KEY=your-key                    # Optional
```

**Via AbstractCore Server (Dynamic Routing):**
```bash
# Route to any endpoint per-request using base_url parameter
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai-compatible/my-model",
    "base_url": "http://gpu-server:8080/v1",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

**Priority Chain:**
1. POST `base_url` parameter (highest) - for dynamic routing
2. Environment variable (`OPENAI_COMPATIBLE_BASE_URL`)
3. Constructor `base_url` parameter
4. Default: `http://localhost:1234/v1`

---

### Token Management

AbstractCore provides **consistent token terminology** and **transparent token counting** across all providers:

```python
from abstractcore import create_llm
from abstractcore.utils.token_utils import estimate_tokens

# Unified token parameters
llm = create_llm(
    "openai",
    model="gpt-5-mini",
    max_tokens=32000,           # Context window (input + output)
    max_output_tokens=8000,     # Maximum output tokens
    max_input_tokens=24000      # Maximum input tokens (auto-calculated if not set)
)

response = llm.generate("Explain quantum computing")

# GenerateResponse object provides consistent structure across all providers
print(f"Generated text: {response.content}")         # The actual LLM response
print(f"Model used: {response.model}")               # Model that generated the response
print(f"Finish reason: {response.finish_reason}")   # Why generation stopped

# Consistent token access across ALL providers (NEW in v2.4.7)
print(f"Input tokens: {response.input_tokens}")     # Always available
print(f"Output tokens: {response.output_tokens}")   # Always available  
print(f"Total tokens: {response.total_tokens}")     # Always available

# Generation time tracking (NEW in v2.4.7)
print(f"Generation time: {response.gen_time}ms")    # Always available (rounded to 1 decimal)

# Advanced properties (when applicable)
print(f"Tool calls: {response.tool_calls}")         # Tools executed (if any)
print(f"Raw usage: {response.usage}")               # Provider-specific token data
print(f"Metadata: {response.metadata}")             # Additional context

# Comprehensive summary with timing
print(f"Summary: {response.get_summary()}")         # "Model: gpt-5-mini | Tokens: 117 | Time: 1234.5ms"
```

**Token Count Sources:**
- **Provider APIs**: OpenAI, Anthropic, LMStudio (native API token counts)
- **AbstractCore Calculation**: MLX, HuggingFace (using `token_utils.py`)
- **Mixed Sources**: Ollama (combination of provider and calculated tokens)

**Backward Compatibility**: Legacy `prompt_tokens` and `completion_tokens` keys remain available in `response.usage` dictionary.

# Token estimation and validation
text = "Your input text here..."
estimated = estimate_tokens(text, model="gpt-5-mini")
print(f"Estimated tokens: {estimated}")

# Estimate required max_tokens for a specific input + desired output
recommended_max_tokens, warnings = llm.calculate_token_budget(
    input_text=text,
    desired_output_tokens=8000,
    safety_margin=0.1,
)
print(f"Recommended max_tokens: {recommended_max_tokens}")
for w in warnings:
    print(f"Warning: {w}")
```

#### Provider-Specific Parameter Mapping

AbstractCore automatically maps unified parameters to provider-specific formats:

```python
# Unified parameters work across all providers
providers = ["openai", "anthropic", "openrouter", "ollama", "lmstudio", "mlx", "huggingface", "vllm", "openai-compatible"]

for provider in providers:
    llm = create_llm(
        provider,
        model="default",
        max_tokens=16000,        # Maps to provider-specific parameter
        max_output_tokens=4000,  # Maps to max_tokens, num_predict, etc.
        temperature=0.7
    )

    # Same interface, different internal mappings:
    # OpenAI: max_tokens, max_completion_tokens
    # Anthropic: max_tokens
    # Ollama: num_ctx, num_predict
    # LMStudio: max_tokens, max_tokens_to_sample
    # MLX: max_tokens
    # HuggingFace: max_length, max_new_tokens
```

#### Token Configuration Checks

```python
# Validate configured limits (returns human-readable warnings)
warnings = llm.validate_token_constraints()
for w in warnings:
    print(w)

# Response-level token access (works across providers)
response = llm.generate("Hello")
print(f"Input: {response.input_tokens} | Output: {response.output_tokens} | Total: {response.total_tokens}")
```

Chunking very large inputs is application-specific; use `estimate_tokens(...)` to split inputs before calling the model.

### Embeddings

SOTA embedding models for semantic search and RAG applications:

```python
from abstractcore.embeddings import EmbeddingManager

# HuggingFace (default)
embedder = EmbeddingManager(model="sentence-transformers/all-MiniLM-L6-v2")

# Ollama
embedder = EmbeddingManager(model="granite-embedding:278m", provider="ollama")

# LMStudio
embedder = EmbeddingManager(model="text-embedding-all-minilm-l6-v2-embedding", provider="lmstudio")

# Generate embeddings
embedding = embedder.embed("Hello world")
embeddings = embedder.embed_batch(["Hello", "World", "AI"])

# Similarity computation
similarity = embedder.compute_similarity("Hello", "Hi there")
```

### Event System

Comprehensive event-driven architecture for monitoring, control, and observability across all operations:

#### Event Types

```python
from abstractcore.events import EventType

# Core generation events
EventType.GENERATION_STARTED      # LLM generation begins
EventType.GENERATION_COMPLETED    # LLM generation finishes

# Tool execution events
EventType.TOOL_STARTED            # Tool execution begins
EventType.TOOL_COMPLETED          # Tool execution finishes

# Error and retry events
EventType.ERROR                   # Any error occurred
EventType.RETRY_ATTEMPTED         # Retry attempt made
EventType.RETRY_EXHAUSTED         # All retries failed

# Validation events
EventType.VALIDATION_FAILED       # Input/output validation failed

# Session events
EventType.SESSION_CREATED         # New session created
EventType.SESSION_CLEARED         # Session history cleared

# Compaction events
EventType.COMPACTION_STARTED      # Session compaction begins
EventType.COMPACTION_COMPLETED    # Session compaction finishes
```

#### Global Event Bus

```python
from abstractcore.events import GlobalEventBus, on_global, emit_global

# Register global event handlers
def cost_monitor(event):
    if event.cost_usd and event.cost_usd > 0.10:
        print(f"High cost request: ${event.cost_usd}")

def performance_monitor(event):
    if event.duration_ms and event.duration_ms > 10000:
        print(f"Slow request: {event.duration_ms}ms")

# Register handlers
on_global(EventType.GENERATION_COMPLETED, cost_monitor)
on_global(EventType.GENERATION_COMPLETED, performance_monitor)

# Access global event bus directly
bus = GlobalEventBus()
bus.on(EventType.TOOL_STARTED, tool_security_check)
bus.off(EventType.TOOL_STARTED, tool_security_check)  # Unregister
bus.clear()  # Clear all handlers

# Emit custom events
emit_global(EventType.ERROR, {
    "error_type": "custom_error",
    "message": "Custom error occurred",
    "metadata": {"component": "custom"}
})
```

#### Event Prevention and Control

```python
def prevent_dangerous_tools(event):
    """Prevent execution of dangerous tools."""
    for call in event.data.get('tool_calls', []):
        if call.name in ['delete_file', 'system_command', 'execute_shell']:
            event.prevent()  # Stop execution immediately
            print(f"Blocked dangerous tool: {call.name}")

def limit_tool_execution_time(event):
    """Prevent tools that might run too long."""
    tool_name = event.data.get('tool_name')
    if tool_name in ['database_scan', 'file_search'] and not event.data.get('timeout'):
        event.prevent()
        print(f"Tool {tool_name} requires timeout parameter")

# Register prevention handlers (execute before tool runs)
on_global(EventType.TOOL_STARTED, prevent_dangerous_tools)
on_global(EventType.TOOL_STARTED, limit_tool_execution_time)
```

#### Production Observability

```python
# Comprehensive monitoring setup
def setup_production_monitoring():
    """Setup production-grade event monitoring."""

    # Cost tracking
    def track_costs(event):
        if event.cost_usd:
            log_cost_metric(event.cost_usd, event.provider, event.model)

    # Performance monitoring
    def track_performance(event):
        metrics = {
            'duration_ms': event.duration_ms,
            'tokens_used': event.tokens_used,
            'provider': event.provider,
            'model': event.model
        }
        send_to_metrics_system(metrics)

    # Error tracking
    def track_errors(event):
        error_data = {
            'error_type': event.error_type,
            'message': event.message,
            'provider': event.provider,
            'stack_trace': event.stack_trace
        }
        send_to_error_tracking(error_data)

    # Tool usage analytics
    def track_tool_usage(event):
        tool_metrics = {
            'tool_name': event.data.get('tool_name'),
            'execution_time': event.duration_ms,
            'success': event.data.get('success', True)
        }
        log_tool_analytics(tool_metrics)

    # Register all monitors
    on_global(EventType.GENERATION_COMPLETED, track_costs)
    on_global(EventType.GENERATION_COMPLETED, track_performance)
    on_global(EventType.ERROR, track_errors)
    on_global(EventType.TOOL_COMPLETED, track_tool_usage)

setup_production_monitoring()
```

### Memory Management

For local providers, explicit memory management:

```python
# Explicit memory management for local models
llm = create_llm("ollama", model="large-model")
response = llm.generate("Hello")
llm.unload()  # Free memory
del llm
```

## Built-in CLI Applications

Five production-ready terminal tools - no Python code required:

### Common Use Cases

```bash
# Generate executive summaries for business documents
summarizer quarterly_report.pdf --style executive --length brief --output executive_summary.txt

# Extract entities and relationships for research
extractor research_paper.pdf --format json-ld --entity-types person,organization,technology --output entities.json

# Evaluate document quality for different contexts
judge technical_spec.md --criteria clarity,accuracy,completeness --context "technical documentation"

# Process large documents with chunking
summarizer large_document.pdf --chunk-size 15000 --style structured --verbose

# Fast entity extraction for content processing
extractor articles/ --mode fast --format json --entity-types person,organization --output results/

# Code review and quality assessment
judge src/main.py --context "code review" --focus "error handling,documentation" --format plain

# Psychological intent analysis with deception detection
intent conversation.txt --focus-participant user --depth comprehensive --verbose

# Autonomous research with web search and reflexive refinement
deepsearch "What are the latest advances in quantum computing?" --depth comprehensive --reflexive
```

### Alternative Methods

```bash
# Method 1: Direct commands (recommended)
summarizer document.txt --style executive
extractor report.pdf --format triples
judge essay.md --criteria soundness
intent conversation.txt --depth comprehensive
deepsearch "your research query" --depth comprehensive

# Method 2: Via Python module
python -m abstractcore.apps.summarizer document.txt --style executive
python -m abstractcore.apps.extractor report.pdf --format triples
python -m abstractcore.apps.judge essay.md --criteria soundness
python -m abstractcore.apps.intent conversation.txt --depth comprehensive
python -m abstractcore.apps.deepsearch "your research query" --depth comprehensive
```

### Python API

```python
from abstractcore.processing import BasicSummarizer, BasicExtractor, BasicJudge, BasicIntentAnalyzer, BasicDeepSearch

# Use programmatically
summarizer = BasicSummarizer()
summary = summarizer.summarize(text, style="executive", length="brief")

extractor = BasicExtractor()
kg = extractor.extract(text, output_format="jsonld")

judge = BasicJudge()
assessment = judge.evaluate(text, context="code review", focus="error handling")

intent_analyzer = BasicIntentAnalyzer()
intent = intent_analyzer.analyze(text, focus_participant="user", depth="comprehensive")

deep_search = BasicDeepSearch()
research = deep_search.research("your research query", depth="comprehensive")
```

---

## HTTP Server

Comprehensive OpenAI-compatible REST API with advanced routing and agent CLI integration:

### Quick Start

```bash
# Start server with default configuration
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000

# Start with custom configuration
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000 --workers 4 --reload

# Production deployment
gunicorn abstractcore.server.app:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

### Complete Endpoint Reference

#### Core Endpoints

```bash
# Chat completions (OpenAI-compatible)
POST /v1/chat/completions

# Provider-specific chat completions (explicit routing)
POST /{provider}/v1/chat/completions

# Embeddings
POST /v1/embeddings

# OpenAI Responses API (100% Compatible)
POST /v1/responses

# Models (OpenAI-compatible list)
GET /v1/models

# Provider discovery
GET /providers

# Health
GET /health
```

#### Provider Discovery API

```bash
# Get all available providers with comprehensive metadata
curl http://localhost:8000/providers

# Include model lists (slower)
curl "http://localhost:8000/providers?include_models=true"

# List models (optionally filter by provider and capabilities)
curl "http://localhost:8000/v1/models?provider=ollama"
curl "http://localhost:8000/v1/models?provider=ollama&input_type=image"

# Health check
curl http://localhost:8000/health
```

### OpenAI-Compatible Usage

Drop-in replacement for OpenAI API with any provider:

```python
import openai

# Standard OpenAI client configuration
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="unused"  # API key not required for local providers
)

# Route to any provider using model format: provider/model
response = client.chat.completions.create(
    model="ollama/qwen3-coder:30b",      # Ollama provider
    messages=[{"role": "user", "content": "Hello!"}]
)

# Alternative: Use anthropic provider
response = client.chat.completions.create(
    model="anthropic/claude-haiku-4-5",    # Anthropic provider
    messages=[{"role": "user", "content": "Write code"}],
    max_tokens=8000,
    temperature=0.7
)

# Embeddings with any provider
embeddings = client.embeddings.create(
    model="ollama/granite-embedding:278m",
    input=["Hello", "World"]
)
```

### Multimodal Requests (Files, Images, Documents)

AbstractCore server provides comprehensive support for multimodal requests through multiple compatible formats:

#### 1. AbstractCore @filename Syntax

Convenient syntax that works across all providers:

```python
import openai

client = openai.OpenAI(base_url="http://localhost:8000/v1", api_key="unused")

# Simple file attachment
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Analyze @report.pdf and @chart.png"}]
)

# Works with any provider
response = client.chat.completions.create(
    model="anthropic/claude-haiku-4-5",
    messages=[{"role": "user", "content": "Summarize @document.docx"}]
)
```

#### 2. OpenAI Vision API Format (with NEW type="file" Support)

Standard OpenAI multimodal format with enhanced file support:

```python
# Image with URL
response = client.chat.completions.create(
    model="ollama/qwen2.5vl:7b",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What is in this image?"},
            {
                "type": "image_url",
                "image_url": {"url": "https://example.com/image.jpg"}
            }
        ]
    }]
)

# NEW: Explicit file type for documents (PDF, DOCX, XLSX, CSV, etc.)
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Analyze this document"},
            {
                "type": "file",
                "file_url": {"url": "https://example.com/report.pdf"}
            }
        ]
    }]
)

# Mixed media (images, documents, data files)
response = client.chat.completions.create(
    model="anthropic/claude-haiku-4-5",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Compare this chart with the data"},
            {"type": "image_url", "image_url": {"url": "https://example.com/chart.png"}},
            {"type": "file", "file_url": {"url": "https://example.com/data.csv"}}
        ]
    }]
)

# Document with base64 data (PDF, DOCX, CSV, etc.)
with open("report.pdf", "rb") as f:
    pdf_data = base64.b64encode(f.read()).decode()

response = client.chat.completions.create(
    model="lmstudio/qwen/qwen3-4b-2507",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Summarize this document"},
            {"type": "file", "file_url": {"url": f"data:application/pdf;base64,{pdf_data}"}}
        ]
    }]
)

# Image with base64 data
import base64

with open("image.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this image"},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
            }
        ]
    }]
)
```

#### 3. OpenAI File Format (Forward-Compatible)

AbstractCore supports OpenAI's planned file format with simplified structure (consistent with image_url):

```python
# HTTP/HTTPS URLs
response = client.chat.completions.create(
    model="anthropic/claude-haiku-4-5",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What are the key findings in this report?"},
            {
                "type": "file",
                "file_url": {
                    "url": "https://example.com/documents/report.pdf"
                }
            }
        ]
    }]
)

# Local file paths
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Analyze this local spreadsheet"},
            {
                "type": "file",
                "file_url": {
                    "url": "/Users/username/documents/spreadsheet.xlsx"
                }
            }
        ]
    }]
)

# Base64 data URLs
import base64

with open("report.pdf", "rb") as f:
    file_data = base64.b64encode(f.read()).decode()

response = client.chat.completions.create(
    model="ollama/qwen3:4b",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this document?"},
            {
                "type": "file",
                "file_url": {
                    "url": f"data:application/pdf;base64,{file_data}"
                }
            }
        ]
    }]
)
```

**Consistent Behavior with image_url:**
```python
# Images and files use identical URL pattern
content = [
    {"type": "text", "text": "Compare these"},
    {
        "type": "image_url",
        "image_url": {"url": "https://example.com/chart.png"}
    },
    {
        "type": "file",
        "file_url": {"url": "https://example.com/data.xlsx"}
    }
]
```

#### Supported File Types

- **Images**: PNG, JPEG, GIF, WEBP, BMP, TIFF
- **Documents**: PDF, DOCX, XLSX, PPTX
- **Data/Text**: CSV, TSV, TXT, MD, JSON, XML
- **Size Limits**: 10MB per file, 32MB total per request

### OpenAI Responses API (/v1/responses)

AbstractCore 2.5.0 introduces 100% OpenAI-compatible `/v1/responses` endpoint with native `input_file` support:

#### Why Use /v1/responses?

- **OpenAI Compatible**: Drop-in replacement for OpenAI's Responses API
- **Native File Support**: `input_file` type designed specifically for document attachments
- **Cleaner API**: Explicit separation between text (`input_text`) and files (`input_file`)
- **Backward Compatible**: Existing `messages` format still works alongside new `input` format
- **Optional Streaming**: Streaming opt-in with `"stream": true` (defaults to `false`)

#### OpenAI Responses API Format

```python
import requests

# Standard OpenAI Responses API format
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "gpt-4o",
        "input": [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": "Analyze this document"},
                    {"type": "input_file", "file_url": "https://example.com/report.pdf"}
                ]
            }
        ],
        "stream": False  # Optional streaming (defaults to False)
    }
)

# Works with any provider
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "lmstudio/qwen/qwen3-4b-2507",
        "input": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "Analyze the letter and provide a summary of the key points."
                    },
                    {
                        "type": "input_file",
                        "file_url": "https://www.berkshirehathaway.com/letters/2024ltr.pdf"
                    }
                ]
            }
        ]
    }
)

# Streaming mode (opt-in)
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "anthropic/claude-haiku-4-5",
        "input": [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": "Summarize this report"},
                    {"type": "input_file", "file_url": "https://example.com/report.pdf"}
                ]
            }
        ],
        "stream": True  # Enable real-time streaming
    },
    stream=True  # Important for streaming responses
)

for line in response.iter_lines():
    if line:
        print(line.decode())
```

#### Legacy Format (Still Supported)

The endpoint automatically detects and supports the legacy `messages` format:

```python
# Legacy format (backward compatible)
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "openai/gpt-4",
        "messages": [
            {"role": "user", "content": "Tell me a story"}
        ],
        "stream": False
    }
)
```

#### Automatic Format Detection

The server automatically detects which format you're using:
- **OpenAI Format**: Presence of `input` field â†’ converts to internal format
- **Legacy Format**: Presence of `messages` field â†’ processes directly
- **Error**: Missing both `input` and `messages` â†’ returns 400 error with clear message

#### Supported Media Types in input_file

All file types supported via URL, local path, or base64:

```python
# PDF from URL
{"type": "input_file", "file_url": "https://example.com/report.pdf"}

# Excel from local path
{"type": "input_file", "file_url": "/path/to/spreadsheet.xlsx"}

# CSV from base64
{"type": "input_file", "file_url": "data:text/csv;base64,RGF0ZSxQcm9kdW..."}

# PowerPoint from URL
{"type": "input_file", "file_url": "https://example.com/presentation.pptx"}
```

#### Complete Example with Multiple Files

```python
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "openai/gpt-4o",
        "input": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "Compare the financial data in the spreadsheet with the PDF report and the chart image"
                    },
                    {"type": "input_file", "file_url": "https://example.com/financial_data.xlsx"},
                    {"type": "input_file", "file_url": "https://example.com/annual_report.pdf"},
                    {"type": "input_file", "file_url": "https://example.com/quarterly_chart.png"}
                ]
            }
        ],
        "max_tokens": 2000,
        "temperature": 0.7,
        "stream": False
    }
)

print(response.json()["choices"][0]["message"]["content"])

#### Mixed Content Example

Combine multiple file types in a single request:

```python
# Multiple files with different formats
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Compare the chart with the spreadsheet data and summarize the document"},
            {
                "type": "image_url",
                "image_url": {"url": "data:image/png;base64,iVBORw0KGgoA..."}
            },
            {
                "type": "file",
                "file_url": {
                    "url": "https://example.com/data/spreadsheet.xlsx"
                }
            },
            {
                "type": "file",
                "file_url": {
                    "url": "/Users/username/documents/summary.pdf"
                }
            }
        ]
    }]
)
```

### Advanced Server Features

#### Request ID Tracking and Structured Logging

```python
import requests
import json

# All requests include tracking
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "ollama/qwen3-coder:30b",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": False
    },
    headers={"X-Request-ID": "my-custom-id"}  # Optional custom ID
)

# Response includes request tracking
data = response.json()
print(f"Request ID: {data.get('request_id')}")
print(f"Provider: {data.get('provider')}")
print(f"Model: {data.get('model')}")
print(f"Duration: {data.get('duration_ms')}ms")
```

#### Agent CLI Integration Through Syntax Conversion

The server automatically handles tool call format conversion for different agent CLIs:

```python
# Request with tool calls for Codex CLI
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "ollama/qwen3-coder:30b",
        "messages": [{"role": "user", "content": "Use the calculator"}],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "calculate",
                    "description": "Perform calculations",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "expression": {"type": "string"}
                        }
                    }
                }
            }
        ],
        "tool_format": "codex"  # Server converts to Qwen3 format
    }
)

# Tool calls are automatically converted to requested format
```

#### Streaming Support

```python
import requests

# Streaming chat completions
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "ollama/qwen3-coder:30b",
        "messages": [{"role": "user", "content": "Write a story"}],
        "stream": True
    },
    stream=True
)

# Process streaming response
for line in response.iter_lines():
    if line:
        data = json.loads(line.decode('utf-8').replace('data: ', ''))
        if data.get('choices'):
            content = data['choices'][0]['delta'].get('content', '')
            print(content, end='', flush=True)
```

### Server Configuration

#### Environment Variables

```bash
# Enable verbose HTTP logging (server-side)
export ABSTRACTCORE_DEBUG=true

# Provider credentials (same env vars as Python usage)
export OPENAI_API_KEY=your_openai_key
export ANTHROPIC_API_KEY=your_anthropic_key
export OPENROUTER_API_KEY=your_openrouter_key
export HUGGINGFACE_API_TOKEN=your_hf_token

# Optional provider endpoints (when using local/self-hosted providers)
export OLLAMA_BASE_URL="http://localhost:11434"
export LMSTUDIO_BASE_URL="http://localhost:1234/v1"
export VLLM_BASE_URL="http://localhost:8000/v1"
export OPENAI_COMPATIBLE_BASE_URL="http://localhost:1234/v1"
```

#### Customization

The server is a FastAPI app. Configure host/port/workers via your ASGI runner (uvicorn/gunicorn); there is no separate `ServerConfig` wrapper.

### Production Deployment

Use standard ASGI runners (uvicorn/gunicorn) and pass host/port/workers via CLI flags:

```bash
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000 --workers 4
```

### Monitoring

```bash
curl http://localhost:8000/health
curl http://localhost:8000/providers
curl "http://localhost:8000/v1/models?provider=ollama"
```

### Benefits

- **Drop-in OpenAI Replacement**: Use any provider through familiar OpenAI API
- **Agent CLI Compatible**: Automatic tool call format conversion
- **Production Ready**: Health endpoint + provider discovery
- **Multi-Provider Routing**: Route to any provider using model prefix
- **Comprehensive Logging**: Structured logs with request tracing
- **Scalable**: Supports horizontal scaling with standard ASGI runners

## Production Resilience

Enterprise-grade error handling, retries, and circuit breaker patterns for robust applications:

### Retries and Circuit Breaker

```python
from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

llm = create_llm(
    "openai",
    model="gpt-5-mini",
    retry_config=RetryConfig(
        max_attempts=3,
        initial_delay=1.0,
        max_delay=30.0,
        use_jitter=True,
        failure_threshold=5,
        recovery_timeout=60.0,
    ),
)

response = llm.generate("What is the capital of France?")
```

### Recovery Timeout Management

```python
from abstractcore import BasicSession, create_llm

llm = create_llm("anthropic", model="claude-haiku-4-5", timeout=45.0, tool_timeout=120.0)
llm.set_recovery_timeout(60.0)  # Circuit breaker recovery timeout (seconds)

session = BasicSession(llm)
session.set_timeout(45.0)
session.set_tool_timeout(120.0)
session.set_recovery_timeout(60.0)

print(llm.get_timeout(), llm.get_tool_timeout(), llm.get_recovery_timeout())
```

### Automatic Backoff Strategies

```python
from abstractcore.core.retry import RetryConfig

# Backoff is configured via RetryConfig (exponential backoff + optional jitter).
retry_config = RetryConfig(
    max_attempts=5,
    initial_delay=0.5,
    max_delay=20.0,
    exponential_base=2.0,
    use_jitter=True,
)
```

### Exception Hierarchy and Error Handling

```python
from abstractcore.exceptions import (
    AbstractCoreError,
    ProviderAPIError,
    ModelNotFoundError,
    AuthenticationError,
    RateLimitError,
    InvalidRequestError,
    UnsupportedFeatureError,
)

try:
    response = llm.generate("Complex request")
except AuthenticationError as e:
    print(f"Auth error: {e}")
except ModelNotFoundError as e:
    print(f"Model error: {e}")
except RateLimitError as e:
    print(f"Rate limit: {e}")
except InvalidRequestError as e:
    print(f"Invalid request: {e}")
except UnsupportedFeatureError as e:
    print(f"Unsupported feature: {e}")
except ProviderAPIError as e:
    print(f"Provider API error (includes circuit-breaker open): {e}")
except AbstractCoreError as e:
    print(f"AbstractCore error: {e}")
```

### Health Checks and Monitoring

```python
from abstractcore import create_llm

primary = create_llm("openai", model="gpt-5-mini")
health = primary.health(timeout=2.0)
if not health["status"]:
    primary = create_llm("anthropic", model="claude-haiku-4-5")
```

### Production Configuration Patterns

```python
from abstractcore import create_llm
from abstractcore.core.retry import RetryConfig

llm = create_llm(
    "openai",
    model="gpt-5-mini",
    timeout=30.0,
    tool_timeout=120.0,
    retry_config=RetryConfig(max_attempts=3),
)
```

### Graceful Degradation

```python
from abstractcore import create_llm

fallback_chain = [
    ("openai", {"model": "gpt-5-mini"}),
    ("anthropic", {"model": "claude-haiku-4-5"}),
    ("ollama", {"model": "qwen3:4b-instruct"}),
]

llm = None
for provider_name, kwargs in fallback_chain:
    try:
        candidate = create_llm(provider_name, **kwargs)
        if candidate.health(timeout=2.0)["status"]:
            llm = candidate
            break
    except Exception:
        continue

if llm is None:
    raise RuntimeError("No healthy providers available")
```

### Benefits

- **Retries + Circuit Breaker**: Built-in via `RetryConfig` (exponential backoff + jitter)
- **Timeout Controls**: `timeout`, `tool_timeout`, and `set_recovery_timeout()`
- **Health Checks**: `llm.health(timeout=...)` for proactive routing/fallback
- **Composable Fallback**: Implement provider chains in your orchestrator/runtime

## Debug Capabilities and Self-Healing JSON

Robust debugging and error recovery features:

```bash
# Debug raw LLM responses for troubleshooting
judge document.txt --debug --provider lmstudio --model qwen/qwen3-4b-2507

# Automatic JSON self-repair handles truncated/malformed responses
# Uses self_fixes.py for intelligent error recovery
# Increased token limits prevent truncation: max_tokens=32k, max_output_tokens=8k

# Focus areas for targeted evaluation (new --focus parameter)
judge README.md --focus "architectural diagrams, technical comparisons" --debug
```

**Key Features:**
- **Self-Healing JSON**: Automatically repairs truncated or malformed JSON responses
- **Debug Mode**: `--debug` flag shows raw LLM responses for troubleshooting
- **Focus Areas**: `--focus` parameter for targeted evaluation (replaces `--custom-criteria`)
- **Increased Token Limits**: Default `max_tokens=32000`, `max_output_tokens=8000` prevent truncation
- **Consistent CLI Syntax**: All apps use space syntax (`--param value`) for consistency

### Complete CLI Parameters Reference

#### Extractor Parameters
```bash
# Core parameters
--focus FOCUS                    # Specific focus area (e.g., "technology", "business")
--format {json-ld,triples,json,yaml}  # Output format
--entity-types TYPES             # Comma-separated types (person,organization,location,etc.)
--output OUTPUT                  # Output file path

# Performance & Quality
--mode {fast,balanced,thorough}  # Extraction mode (balanced=default)
--iterate N                      # Refinement iterations (default: 1)
--similarity-threshold 0.0-1.0   # Entity deduplication threshold (default: 0.85)
--no-embeddings                  # Disable semantic deduplication
--minified                       # Compact JSON output

# LLM Configuration
--provider PROVIDER --model MODEL  # Custom LLM provider/model
--max-tokens 32000               # Context window (default: 32000)
--max-output-tokens 8000         # Output tokens (default: 8000)
--timeout 300                    # HTTP timeout seconds (default: 300)
--chunk-size 8000                # Chunk size for large files
```

#### Judge Parameters
```bash
# Evaluation Configuration
--criteria CRITERIA              # Standard criteria (clarity,soundness,etc.)
--focus FOCUS                    # Primary focus areas for evaluation
--context CONTEXT                # Evaluation context description
--reference FILE_OR_TEXT         # Reference content for comparison

# Output & Debug
--format {json,plain,yaml}       # Output format (default: json)
--debug                          # Show raw LLM responses
--include-criteria               # Include detailed criteria explanations
--exclude-global                 # Skip global assessment for multiple files

# LLM Configuration  
--temperature 0.1                # Evaluation consistency (default: 0.1)
--max-tokens 32000               # Context window (default: 32000)
--max-output-tokens 8000         # Output tokens (default: 8000)
--timeout 300                    # HTTP timeout seconds
```

#### Summarizer Parameters
```bash
# Content Configuration
--style {structured,narrative,objective,analytical,executive,conversational}
--length {brief,standard,detailed,comprehensive}
--focus FOCUS                    # Specific focus area
--chunk-size 8000                # Chunk size for large files (max: 32000)

# Output & Performance
--output OUTPUT                  # Output file path
--max-tokens 32000               # Context window (default: 32000)
--max-output-tokens 8000         # Output tokens (default: 8000)
--verbose                        # Show detailed progress
```

---

## Documentation Index

### Essential Guides
- **[Getting Started](docs/getting-started.md)**: 5-minute setup and first LLM call
- **[API Reference](docs/api-reference.md)**: Complete Python API with examples
- **[Centralized Configuration](docs/centralized-config.md)**: Global defaults, app preferences, and configuration management
- **[Media Handling System](docs/media-handling-system.md)**: Universal file attachment and processing across all providers
- **[Vision Capabilities](docs/vision-capabilities.md)**: Image analysis across 7 providers with automatic optimization
- **[Tool Calling](docs/tool-calling.md)**: Universal @tool decorator system
- **[Session Management](docs/session.md)**: Persistent conversations with analytics

### CLI Applications
- **[Summarizer](docs/apps/basic-summarizer.md)**: Document summarization (`summarizer doc.pdf`)
- **[Extractor](docs/apps/basic-extractor.md)**: Knowledge graph extraction (`extractor report.txt`)
- **[Judge](docs/apps/basic-judge.md)**: Text evaluation (`judge essay.txt`)
- **[Intent](docs/apps/basic-intent.md)**: Intent analysis & deception detection (`intent conversation.txt`)
- **[DeepSearch](docs/apps/basic-deepsearch.md)**: Multi-stage web-assisted research (`deepsearch "query"`)

### Advanced Topics
- **[Server Guide](docs/server.md)**: OpenAI-compatible HTTP API
- **[Tool Syntax Rewriting](docs/tool-syntax-rewriting.md)**: `tool_call_tags` (Python) and `agent_format` (server)
- **[Interaction Tracing](docs/interaction-tracing.md)**: Programmatic tracing for prompts/responses/timing/usage
- **[MCP](docs/mcp.md)**: MCP tool servers (HTTP/stdio) as tool sources
- **[Embeddings](docs/embeddings.md)**: Semantic search and RAG
- **[Architecture](docs/architecture.md)**: System design overview with media processing
- **Error Handling**: Comprehensive exception hierarchy for robust applications
- **Token Management**: Centralized token counting, cost estimation, and management utilities (`abstractcore.utils.token_utils`)

### Examples & Support
- **[Examples](examples/)**: Progressive tutorials and real-world patterns
- **[Troubleshooting](docs/troubleshooting.md)**: Common issues and solutions
- **[GitHub Issues](https://github.com/lpalbou/AbstractCore/issues)**: Report bugs and get help

---

## Quick Links

- **GitHub**: https://github.com/lpalbou/AbstractCore
- **PyPI**: `pip install abstractcore` (or `abstractcore[all-apple]` / `all-non-mlx` / `all-gpu`)
- **License**: MIT | **Python**: 3.9+ | **Status**: Production Ready

**For Developers and Architects:**
- **Provider Agnostic**: Switch between OpenAI, Anthropic, Ollama, and others without code changes
- **Production Ready**: Enterprise-grade error handling, retries, and resilience patterns
- **Universal Tool Calling**: Enable function calling on any provider, even those without native support
- **Media Processing**: Attach any file type (PDFs, images, Office docs) with simple `@filename` syntax
- **Built-in Apps**: Skip boilerplate with ready-to-use CLI tools for common document processing tasks
