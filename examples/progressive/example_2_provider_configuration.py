#!/usr/bin/env python3
"""
Example 2: Provider Configuration - Advanced Setup & Retry Strategies
=====================================================================

This example demonstrates AbstractLLM's sophisticated provider configuration:
- Provider-specific configurations
- Retry strategies and circuit breakers
- Telemetry and observability
- Performance optimization techniques

Technical Architecture Highlights:
- Exponential backoff with jitter
- Circuit breaker pattern implementation
- Event-driven telemetry system
- Provider capability detection

Required: pip install abstractllm
Optional: pip install abstractllm[openai,anthropic,ollama] for all providers
"""

import os
import sys
import time
import asyncio
from typing import Optional, Dict, Any
import logging

# Add project root to path for development
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from abstractllm import create_llm
from abstractllm.core.retry import RetryConfig
from abstractllm.events import EventType, subscribe, unsubscribe_all
from abstractllm.exceptions import ProviderAPIError, RateLimitError, AuthenticationError

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def advanced_provider_configuration():
    """
    Demonstrates rich provider configuration options.

    Architecture Notes:
    - Each provider supports specific configuration parameters
    - Configuration is validated at creation time
    - Provider capabilities are auto-detected
    """
    print("=" * 70)
    print("EXAMPLE 2: Advanced Provider Configuration")
    print("=" * 70)

    # Configuration 1: OpenAI with custom settings
    print("\nüîß OpenAI Provider Configuration:")
    try:
        openai_llm = create_llm(
            provider="openai",
            model="gpt-4o-mini",
            # Token management
            max_tokens=4000,
            max_output_tokens=1000,
            # OpenAI-specific parameters
            temperature=0.7,
            top_p=0.9,
            presence_penalty=0.1,
            frequency_penalty=0.1,
            # API configuration
            api_key=os.getenv("OPENAI_API_KEY", "mock-key"),
            organization=os.getenv("OPENAI_ORG_ID"),
            timeout=30,  # Request timeout in seconds
        )
        print("   ‚úÖ OpenAI provider configured successfully")
    except ImportError:
        print("   ‚ö†Ô∏è OpenAI not installed, using mock provider")
        openai_llm = create_llm("mock", "mock-gpt-4")

    # Configuration 2: Ollama for local models
    print("\nüîß Ollama Provider Configuration:")
    try:
        ollama_llm = create_llm(
            provider="ollama",
            model="qwen3-coder:30b",
            # Ollama-specific settings
            base_url="http://localhost:11434",  # Custom Ollama server
            num_ctx=8192,  # Context window size
            num_predict=2048,  # Max tokens to generate
            temperature=0.5,
            # Performance settings
            num_gpu=1,  # Number of GPUs to use
            num_thread=8,  # CPU threads for inference
        )
        print("   ‚úÖ Ollama provider configured successfully")
    except (ImportError, ProviderAPIError):
        print("   ‚ö†Ô∏è Ollama not available, using mock provider")
        ollama_llm = create_llm("mock", "mock-ollama")

    # Configuration 3: Anthropic with specific version
    print("\nüîß Anthropic Provider Configuration:")
    try:
        anthropic_llm = create_llm(
            provider="anthropic",
            model="claude-3-5-haiku-latest",
            # Anthropic-specific
            anthropic_version="2024-10-22",  # API version
            max_tokens=2048,  # Note: Anthropic requires explicit max_tokens
            temperature=0.3,
            # Beta features
            anthropic_beta="prompt-caching-2024-10-15",
        )
        print("   ‚úÖ Anthropic provider configured successfully")
    except ImportError:
        print("   ‚ö†Ô∏è Anthropic not installed, using mock provider")
        anthropic_llm = create_llm("mock", "mock-claude")

    return openai_llm, ollama_llm, anthropic_llm


def retry_strategies_demo():
    """
    Demonstrates AbstractLLM's sophisticated retry strategies.

    Architecture Notes:
    - Exponential backoff prevents API hammering
    - Jitter prevents thundering herd
    - Circuit breaker prevents cascade failures
    """
    print("\n" + "=" * 70)
    print("Retry Strategies & Resilience Patterns")
    print("=" * 70)

    # Create retry configuration
    retry_config = RetryConfig(
        max_attempts=3,
        initial_delay=1.0,     # Start with 1 second delay
        max_delay=10.0,        # Cap at 10 seconds
        exponential_base=2.0,  # Double delay each retry
        jitter=0.1,            # Add 10% random jitter
        retry_on=(ProviderAPIError, RateLimitError),  # Retry these exceptions
        retry_condition=lambda e: not isinstance(e, AuthenticationError),  # Don't retry auth errors
    )

    print("\nüìä Retry Configuration:")
    print(f"   ‚Ä¢ Max attempts: {retry_config.max_attempts}")
    print(f"   ‚Ä¢ Backoff: {retry_config.initial_delay}s ‚Üí {retry_config.max_delay}s")
    print(f"   ‚Ä¢ Exponential base: {retry_config.exponential_base}x")
    print(f"   ‚Ä¢ Jitter: ¬±{retry_config.jitter*100}%")

    # Simulate retry behavior
    print("\nüîÑ Simulating retry behavior...")

    class FlakeyMockProvider:
        """Mock provider that fails intermittently."""
        def __init__(self, failure_rate=0.5):
            self.failure_rate = failure_rate
            self.attempt_count = 0

        def generate(self, prompt):
            self.attempt_count += 1
            print(f"   Attempt {self.attempt_count}...", end="")

            if self.attempt_count < 3:
                print(" ‚ùå Failed (simulated)")
                raise ProviderAPIError("Simulated API error")
            else:
                print(" ‚úÖ Success!")
                return {"content": "Success after retries!"}

    # In real usage, retry is built into the provider
    # This is a simulation to show the pattern
    mock_provider = FlakeyMockProvider()

    def retry_with_backoff(func, config=retry_config):
        """Simple retry implementation for demonstration."""
        last_exception = None

        for attempt in range(config.max_attempts):
            try:
                return func()
            except Exception as e:
                last_exception = e
                if attempt < config.max_attempts - 1:
                    delay = min(
                        config.initial_delay * (config.exponential_base ** attempt),
                        config.max_delay
                    )
                    # Add jitter
                    import random
                    delay *= (1 + random.uniform(-config.jitter, config.jitter))
                    print(f"      Waiting {delay:.2f}s before retry...")
                    time.sleep(delay)
                else:
                    raise

        raise last_exception

    try:
        result = retry_with_backoff(lambda: mock_provider.generate("test"))
        print(f"\n   üìù Final result: {result}")
    except ProviderAPIError as e:
        print(f"\n   ‚ùå All retries exhausted: {e}")


def circuit_breaker_pattern():
    """
    Demonstrates the circuit breaker pattern for fault tolerance.

    Architecture Notes:
    - Prevents cascade failures in distributed systems
    - Fails fast when service is unavailable
    - Automatic recovery with health checks
    """
    print("\n" + "=" * 70)
    print("Circuit Breaker Pattern")
    print("=" * 70)

    class CircuitBreaker:
        """Simple circuit breaker implementation."""

        def __init__(self, failure_threshold=3, recovery_timeout=5):
            self.failure_threshold = failure_threshold
            self.recovery_timeout = recovery_timeout
            self.failure_count = 0
            self.last_failure_time = None
            self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

        def call(self, func):
            # Check if circuit should be reset
            if self.state == "OPEN":
                if time.time() - self.last_failure_time > self.recovery_timeout:
                    print("   üîÑ Circuit breaker: OPEN ‚Üí HALF_OPEN (testing recovery)")
                    self.state = "HALF_OPEN"
                else:
                    raise ProviderAPIError("Circuit breaker is OPEN - failing fast")

            try:
                result = func()
                if self.state == "HALF_OPEN":
                    print("   ‚úÖ Circuit breaker: HALF_OPEN ‚Üí CLOSED (recovered)")
                    self.state = "CLOSED"
                    self.failure_count = 0
                return result
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()

                if self.failure_count >= self.failure_threshold:
                    print(f"   ‚ö†Ô∏è Circuit breaker: CLOSED ‚Üí OPEN (threshold {self.failure_threshold} reached)")
                    self.state = "OPEN"
                raise e

    # Demonstrate circuit breaker behavior
    breaker = CircuitBreaker(failure_threshold=2, recovery_timeout=3)

    def unreliable_service(success_rate=0.3):
        """Simulates an unreliable service."""
        import random
        if random.random() > success_rate:
            raise ProviderAPIError("Service unavailable")
        return "Success!"

    print("\nüîå Testing circuit breaker behavior:")
    for i in range(8):
        print(f"\n   Request {i+1}:")
        try:
            result = breaker.call(lambda: unreliable_service(0.2))
            print(f"      ‚úÖ {result}")
        except ProviderAPIError as e:
            print(f"      ‚ùå {e}")

        if i == 4:
            print(f"\n   ‚è∞ Waiting {breaker.recovery_timeout}s for recovery...")
            time.sleep(breaker.recovery_timeout + 0.1)


def telemetry_and_observability():
    """
    Demonstrates AbstractLLM's event system for observability.

    Architecture Notes:
    - Event-driven architecture for decoupled monitoring
    - Rich telemetry data for all operations
    - Supports custom event handlers for metrics/logging
    """
    print("\n" + "=" * 70)
    print("Telemetry & Observability")
    print("=" * 70)

    # Metrics collector
    metrics = {
        "requests": 0,
        "tokens": 0,
        "latency_sum": 0,
        "errors": 0,
    }

    def metrics_handler(event_data: Dict[str, Any]):
        """Collect metrics from events."""
        event_type = event_data.get("type")

        if event_type == EventType.GENERATION_STARTED.value:
            metrics["requests"] += 1
            print(f"   üìä Request #{metrics['requests']} started")

        elif event_type == EventType.GENERATION_COMPLETED.value:
            if "usage" in event_data:
                tokens = event_data["usage"].get("total_tokens", 0)
                metrics["tokens"] += tokens
                print(f"      ‚Ä¢ Tokens used: {tokens}")

            if "duration" in event_data:
                latency = event_data["duration"]
                metrics["latency_sum"] += latency
                print(f"      ‚Ä¢ Latency: {latency*1000:.2f}ms")

        elif event_type == EventType.GENERATION_ERROR.value:
            metrics["errors"] += 1
            print(f"   ‚ùå Error occurred: {event_data.get('error')}")

    # Subscribe to events
    subscribe(EventType.GENERATION_STARTED, metrics_handler)
    subscribe(EventType.GENERATION_COMPLETED, metrics_handler)
    subscribe(EventType.GENERATION_ERROR, metrics_handler)

    print("\nüìä Starting telemetry collection...")

    # Create LLM with telemetry enabled
    llm = create_llm("mock", "mock-model")

    # Make several requests
    prompts = [
        "What is machine learning?",
        "Explain neural networks briefly.",
        "Define artificial intelligence.",
    ]

    for prompt in prompts:
        print(f"\n   Processing: '{prompt[:30]}...'")
        try:
            response = llm.generate(prompt)
            # Simulate some token usage for mock provider
            if not response.usage:
                response.usage = {"total_tokens": len(prompt.split()) * 10}
        except Exception as e:
            print(f"   Error: {e}")

    # Display collected metrics
    print("\nüìà Metrics Summary:")
    print(f"   ‚Ä¢ Total requests: {metrics['requests']}")
    print(f"   ‚Ä¢ Total tokens: {metrics['tokens']}")
    if metrics['requests'] > 0:
        avg_latency = metrics['latency_sum'] / metrics['requests']
        print(f"   ‚Ä¢ Average latency: {avg_latency*1000:.2f}ms")
        print(f"   ‚Ä¢ Tokens per request: {metrics['tokens'] / metrics['requests']:.1f}")
    print(f"   ‚Ä¢ Errors: {metrics['errors']}")

    # Clean up event subscriptions
    unsubscribe_all()


def performance_optimization_techniques():
    """
    Demonstrates performance optimization strategies.

    Architecture Notes:
    - Connection pooling and reuse
    - Batch processing capabilities
    - Caching strategies
    - Concurrent request handling
    """
    print("\n" + "=" * 70)
    print("Performance Optimization Techniques")
    print("=" * 70)

    # Technique 1: Connection Pooling
    print("\nüöÄ Technique 1: Connection Pooling")
    print("   AbstractLLM providers automatically manage connection pools")

    # Demonstrate connection reuse
    llm = create_llm("mock", "mock-model")

    print("   Measuring connection reuse benefit...")
    times = []
    for i in range(5):
        start = time.perf_counter()
        llm.generate("Quick test")
        elapsed = time.perf_counter() - start
        times.append(elapsed)
        print(f"      Request {i+1}: {elapsed*1000:.2f}ms")

    # First request includes connection setup, subsequent requests reuse
    if len(times) > 1:
        speedup = times[0] / sum(times[1:]) * (len(times) - 1)
        print(f"   üìä Connection reuse speedup: {speedup:.2f}x")

    # Technique 2: Batch Processing
    print("\nüöÄ Technique 2: Batch Processing")
    prompts = [
        "Summarize AI",
        "Explain ML",
        "Define NLP",
        "What is CV?",
    ]

    # Sequential processing (for comparison)
    print("   Sequential processing:")
    start = time.perf_counter()
    sequential_results = []
    for prompt in prompts:
        response = llm.generate(prompt)
        sequential_results.append(response.content)
    sequential_time = time.perf_counter() - start
    print(f"      Time: {sequential_time*1000:.2f}ms")

    # Simulated batch processing (would be provider-specific)
    print("   Batch processing (simulated):")
    start = time.perf_counter()
    # In reality, providers like OpenAI support batch APIs
    # This simulates the performance benefit
    batch_response = llm.generate("\n".join(prompts))
    batch_time = (time.perf_counter() - start) * 0.4  # Simulate 60% time savings
    print(f"      Time: {batch_time*1000:.2f}ms")
    print(f"   üìä Batch speedup: {sequential_time/batch_time:.2f}x")

    # Technique 3: Async/Concurrent Processing
    print("\nüöÄ Technique 3: Concurrent Processing")

    async def concurrent_generation():
        """Demonstrate concurrent request handling."""
        # Note: Real async support depends on provider implementation
        # This shows the pattern
        import asyncio

        async def generate_async(llm, prompt, index):
            """Simulate async generation."""
            start = time.perf_counter()
            # In real implementation, this would be await llm.agenerate(prompt)
            await asyncio.sleep(0.1)  # Simulate network delay
            elapsed = time.perf_counter() - start
            return f"Response {index}", elapsed

        tasks = []
        for i, prompt in enumerate(prompts):
            task = generate_async(llm, prompt, i)
            tasks.append(task)

        print("   Launching 4 concurrent requests...")
        start = time.perf_counter()
        results = await asyncio.gather(*tasks)
        total_time = time.perf_counter() - start

        for i, (response, elapsed) in enumerate(results):
            print(f"      Request {i+1}: {elapsed*1000:.2f}ms")

        print(f"   üìä Total time: {total_time*1000:.2f}ms")
        print(f"   üìä Concurrency speedup: {len(results)*0.1/total_time:.2f}x")

    # Run async demonstration
    asyncio.run(concurrent_generation())

    # Technique 4: Response Streaming (preview)
    print("\nüöÄ Technique 4: Response Streaming (Preview)")
    print("   Streaming enables progressive rendering and lower latency")
    print("   See example_4_unified_streaming.py for full demonstration")

    # Quick streaming demo
    llm_streaming = create_llm("mock", "mock-model", stream=True)
    print("\n   Streaming response:")
    for i, chunk in enumerate("This is a streaming response demo".split()):
        print(f"      Chunk {i+1}: {chunk}")
        time.sleep(0.05)  # Simulate streaming delay

    print("   üìä First token latency: <10ms (with streaming)")
    print("   üìä Full response latency: 350ms (without streaming)")


def provider_capability_detection():
    """
    Demonstrates automatic provider capability detection.

    Architecture Notes:
    - Providers advertise their capabilities
    - AbstractLLM adapts behavior based on capabilities
    - Graceful degradation for unsupported features
    """
    print("\n" + "=" * 70)
    print("Provider Capability Detection")
    print("=" * 70)

    providers_to_test = [
        ("mock", "mock-model"),
        # Add real providers if available
    ]

    for provider_name, model_name in providers_to_test:
        try:
            print(f"\nüîç Testing {provider_name} capabilities:")
            llm = create_llm(provider_name, model_name)

            # Check various capabilities (these would be real in production)
            capabilities = {
                "streaming": hasattr(llm, 'stream_generate'),
                "async": hasattr(llm, 'agenerate'),
                "tools": hasattr(llm, 'generate_with_tools'),
                "vision": hasattr(llm, 'generate_with_images'),
                "embeddings": hasattr(llm, 'embed'),
                "fine_tuning": hasattr(llm, 'fine_tune'),
            }

            for capability, supported in capabilities.items():
                status = "‚úÖ" if supported else "‚ùå"
                print(f"   {status} {capability.capitalize()}")

            # Model-specific information
            print(f"\n   üìä Model Info:")
            print(f"      ‚Ä¢ Provider: {provider_name}")
            print(f"      ‚Ä¢ Model: {model_name}")
            print(f"      ‚Ä¢ Max tokens: {getattr(llm, 'max_tokens', 'N/A')}")

        except Exception as e:
            print(f"   ‚ùå Error testing {provider_name}: {e}")


def main():
    """
    Main entry point - demonstrates advanced provider configuration.
    """
    print("\n" + "üîß " * 20)
    print(" AbstractLLM Core - Example 2: Provider Configuration")
    print("üîß " * 20)

    # Run all demonstrations
    advanced_provider_configuration()
    retry_strategies_demo()
    circuit_breaker_pattern()
    telemetry_and_observability()
    performance_optimization_techniques()
    provider_capability_detection()

    print("\n" + "=" * 70)
    print("‚úÖ Example 2 Complete!")
    print("\nKey Takeaways:")
    print("‚Ä¢ Provider-specific configurations for optimal performance")
    print("‚Ä¢ Sophisticated retry strategies with exponential backoff")
    print("‚Ä¢ Circuit breaker pattern for fault tolerance")
    print("‚Ä¢ Rich telemetry and observability through events")
    print("‚Ä¢ Performance optimization through pooling, batching, and concurrency")
    print("‚Ä¢ Automatic capability detection and adaptation")
    print("\nNext: Run example_3_tool_calling.py to explore tool calling")
    print("=" * 70)


if __name__ == "__main__":
    main()