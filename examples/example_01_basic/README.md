# AbstractCore Core: Basic Introduction

## What You'll Learn

- üåü Initialize AbstractCore Core
- üìù Configure basic text generation
- üîç Understand core library concepts

### Learning Objectives

1. Import AbstractCore library
2. Select and configure a basic model
3. Generate simple text responses
4. Explore basic configuration options

### Prerequisites

- Python 3.9+
- Basic Python programming knowledge
- Ollama or other supported provider installed

### Example Walkthrough

This example demonstrates the simplest way to use AbstractCore Core. You'll learn how to:
- Set up a basic text generation environment
- Make your first API call
- Handle basic configurations

### Installation

```bash
# Ensure you have the required dependencies
pip install abstractcore ollama

# Verify Ollama is running
ollama serve
```

### Code Example

```python
from abstractcore import create_llm

# Initialize a basic model
llm = create_llm(provider='ollama', model='llama3')

# Generate a simple response
response = llm.generate("Tell me a short joke.")
print(response)
```

### Expected Output

```
A typical AI-generated humorous response about a programmer walking into a bar.
```

### Configuration Options

```python
# Advanced configuration example
llm = create_llm(
    provider='ollama',
    model='llama3',
    max_tokens=100,      # Limit response length
    temperature=0.7,     # Control randomness
    stream=True          # Enable streaming
)
```

### Troubleshooting

#### Common Issues
- **Model Not Found**: Ensure the model is installed via `ollama pull llama3`
- **Connection Error**: Verify Ollama is running (`ollama serve`)
- **Dependency Problems**: Use `pip install -U abstractcore ollama`

#### Debugging Tips
- Check Ollama logs: `ollama logs`
- Validate model availability: `ollama list`
- Use verbose mode for detailed errors

### Best Practices

- Always specify a max token limit
- Use temperature to control response creativity
- Handle potential network or model errors

### Performance Considerations

- First call might be slower due to model loading
- Subsequent calls will be faster
- Consider model size and your hardware capabilities

### Reference Documentation

- [AbstractCore Core Documentation](/docs/getting-started.md)
- [Provider Configuration](/docs/prerequisites.md)
- [Troubleshooting Guide](/docs/troubleshooting.md)

### Next Steps

After completing this example, move to `example_02_providers` to explore more advanced provider configurations.

### Contribute & Feedback

- Found an issue? [Open an Issue](https://github.com/your-org/abstractcore/issues)
- Want to improve the example? Submit a pull request!

---

**Learning Time**: ~15 minutes
**Complexity**: Beginner
**Last Updated**: 2025-10-11