# AbstractCore - Unified LLM Provider Interface

**Write once, run everywhere.** Production-ready Python library designed primarily for open source LLMs with complete offline capability. Supports all major providers (OpenAI, Anthropic, Ollama, MLX, LMStudio, HuggingFace) with universal tool calling, streaming, session management, and built-in CLI apps.

## Quick Start

```bash
pip install abstractcore[all]
```

- **[5-Minute Setup](docs/getting-started.md)**: Basic usage with any provider
- **[Prerequisites](docs/prerequisites.md)**: API keys and local provider setup
- **[Installation Options](#installation-options)**: Provider-specific dependencies

## Core Features

- **[API Reference](docs/api-reference.md)**: Complete Python API with examples
- **[Async/Await Support](docs/api-reference.md#agenerate)** (v2.6.0): Concurrent requests for 3-10x faster batch operations
- **Enhanced Error Messages** (v2.6.0): Authentication and connection errors include fix commands and setup URLs
- **Custom Base URLs** (v2.6.0): Configure OpenAI-compatible proxies and enterprise gateways for OpenAI and Anthropic providers
- **Model Downloads** (v2.6.0): Async download API with progress reporting for Ollama, HuggingFace, and MLX models
- **[Interaction Tracing](docs/interaction-tracing.md)**: Complete LLM observability with programmatic access to prompts, responses, tokens, timing for debugging and compliance
- **[Centralized Configuration](docs/centralized-config.md)**: Global defaults, app preferences, API key management
- **[Media Handling System](docs/media-handling-system.md)**: Universal file attachment (images, PDFs, Office docs, CSV) across all providers
- **[Vision Capabilities](docs/vision-capabilities.md)**: Image analysis across 6 providers with automatic optimization
- **[Universal Tool Calling](docs/tool-calling.md)**: @tool decorator works across ALL providers
- **[Provider Discovery](docs/getting-started.md)**: Centralized registry to programmatically list ALL providers and models
- **[Session Management](docs/session.md)**: Persistent conversations with analytics and auto-compaction
- **[Streaming](docs/api-reference.md)**: Real-time responses across all providers
- **[Structured Output](docs/structured-output.md)**: Native vs prompted strategies, schema validation, production deployment
- **[Embeddings](docs/embeddings.md)**: SOTA models for semantic search and RAG
- **[Token Management](docs/token-management.md)**: Unified parameter vocabulary with budget validation
- **[Tool Syntax Rewriting](docs/tool-calling.md)**: Real-time format conversion for agent CLI compatibility

## Built-in CLI Applications

Ready-to-use terminal tools - no Python code required:

- **[Summarizer](docs/apps/basic-summarizer.md)**: `summarizer document.pdf --style executive`
- **[Extractor](docs/apps/basic-extractor.md)**: `extractor report.txt --format json-ld --focus technology`
- **[Judge](docs/apps/basic-judge.md)**: `judge essay.txt --criteria clarity,accuracy --context "academic writing"`
- **[Intent Analyzer](docs/apps/basic-intent.md)**: `intent conversation.txt --focus-participant user --depth comprehensive`
- **[DeepSearch](docs/apps/basic-deepsearch.md)**: `deepsearch "research query" --depth comprehensive --reflexive`

## Optional Components

- **[HTTP Server](docs/server.md)**: OpenAI-compatible REST API for multi-language access
- **[Architecture](docs/architecture.md)**: System design and component interactions
- **[Examples](examples/)**: Progressive tutorials from basic to production patterns

## Documentation & Support

- **[Troubleshooting](docs/troubleshooting.md)**: Common issues and solutions
- **[Full Documentation](docs/README.md)**: Complete navigation guide
- **[GitHub Issues](https://github.com/lpalbou/AbstractCore/issues)**: Report bugs and get help

## Installation Options

```bash
# Minimal core
pip install abstractcore

# With specific providers
pip install abstractcore[openai]
pip install abstractcore[anthropic]
pip install abstractcore[ollama]
pip install abstractcore[lmstudio]
pip install abstractcore[mlx]
pip install abstractcore[huggingface]

# With media handling (images, PDFs, Office docs)
pip install abstractcore[media]

# With server support
pip install abstractcore[server]

# With embeddings
pip install abstractcore[embeddings]

# Everything
pip install abstractcore[all]
```

## Why AbstractCore?

- **Offline-first design**: Built primarily for open source LLMs with complete offline operation. Download once, run forever without internet
- **Write once, run everywhere**: Same code works with any LLM provider
- **Production ready**: Built-in error handling, retries, and monitoring
- **Universal tool calling**: @tool decorator works across ALL providers
- **Universal media handling**: Simple `@filename` syntax and `media=[]` API for images, PDFs, Office docs
- **Vision capabilities**: Image analysis across all providers with automatic optimization
- **Centralized configuration**: Global defaults and app-specific preferences eliminate repetitive setup
- **Built-in apps**: Ready-to-use CLI tools (summarizer, extractor, judge)
- **Debug capabilities**: Self-healing JSON and `--debug` mode for troubleshooting

## Essential Code Examples

### Basic Usage with Factory Pattern
```python
from abstractcore import create_llm

# Factory pattern creates appropriate provider
llm = create_llm("anthropic", model="claude-3-5-haiku-latest")
response = llm.generate("What is the capital of France?")
print(response.content)
```

### Deterministic Generation
```python
from abstractcore import create_llm

# Deterministic outputs with seed + temperature=0
llm = create_llm("openai", model="gpt-3.5-turbo", seed=42, temperature=0.0)

# These will produce identical outputs
response1 = llm.generate("Write exactly 3 words about coding")
response2 = llm.generate("Write exactly 3 words about coding")
print(f"Response 1: {response1.content}")  # "Innovative, challenging, rewarding."
print(f"Response 2: {response2.content}")  # "Innovative, challenging, rewarding."

# Verified deterministic across providers (except Anthropic)
# Verified: OpenAI, Ollama, MLX, HuggingFace, LMStudio: Same seed → Identical outputs
# ⚠️ Anthropic: temperature=0 → Consistent outputs (issues warning for seed)
```

### Provider Discovery (Centralized Registry)
```python
from abstractcore.providers import get_all_providers_with_models

# Get comprehensive information about all providers with available models
providers = get_all_providers_with_models()
for provider in providers:
    print(f"{provider['display_name']}: {provider['model_count']} models")
    print(f"Features: {', '.join(provider['supported_features'])}")
    print(f"Local: {provider['local_provider']}")
    print(f"Auth Required: {provider['authentication_required']}")
```

### Centralized Configuration
```bash
# Check current configuration status
abstractcore --status                # Configuration CLI
abstractcore-config --status         # Same thing (alias for clarity)

# Set global fallback model
abstractcore --set-global-default ollama/llama3:8b

# Set app-specific defaults for optimal performance
abstractcore --set-app-default summarizer openai gpt-4o-mini
abstractcore --set-app-default extractor ollama llama3.2:3b
abstractcore --set-app-default judge anthropic claude-3-5-haiku-latest

# Set API keys
abstractcore --set-api-key openai sk-your-key-here
abstractcore --set-api-key anthropic your-anthropic-key
```

### Interactive Chat CLI
```bash
# Start interactive REPL for chatting with LLMs
abstractcore-chat                                    # New convenient entry point
abstractcore-chat --provider ollama --model llama2
abstractcore-chat --stream

# Alternative (backwards compatible)
python -m abstractcore.utils.cli --provider openai --model gpt-4o
```

### Media Handling System
```python
from abstractcore import create_llm

# Works with any provider - just change the provider name
llm = create_llm("openai", model="gpt-4o", api_key="your-key")

# Universal media parameter works across all providers
response = llm.generate(
    "What's in this image and document?",
    media=["photo.jpg", "report.pdf"]
)

# CLI integration with @filename syntax
# python -m abstractcore.utils.cli --prompt "Analyze @report.pdf and @chart.png"

# Supports 90+ file extensions:
# - Images: PNG, JPEG, GIF, WEBP, BMP, TIFF
# - Documents: PDF, DOCX, XLSX, PPTX, ODT, RTF
# - Text: TXT, MD, CSV, TSV, JSON, YAML, TOML, XML, HTML
# - Programming: .py, .js, .java, .c, .cpp, .go, .rs, .rb, .php, .r, .R, .sql, .jl, etc.
# - Notebooks: .ipynb, .rmd, .Rmd, .qmd
# - And any text-based file (auto-detected via content analysis)
response = llm.generate(
    "Compare the data",
    media=["chart.png", "data.csv", "presentation.pptx"]
)

# Query supported formats programmatically
from abstractcore.media.types import get_all_supported_extensions
formats = get_all_supported_extensions()
print(f"Text formats: {len(formats['text'])} extensions")  # 90+
```

### Vision Capabilities
```python
from abstractcore import create_llm

# Works with any vision-capable provider
llm = create_llm("openai", model="gpt-4o")

# Single image analysis
response = llm.generate(
    "What objects do you see in this image?",
    media=["photo.jpg"]
)

# Multiple images comparison
response = llm.generate(
    "Compare these architectural styles and identify differences",
    media=["building1.jpg", "building2.jpg", "building3.jpg"]
)

# Cross-provider consistency - same code works everywhere
openai_response = create_llm("openai", model="gpt-4o").generate("Analyze", media=["chart.png"])
anthropic_response = create_llm("anthropic", model="claude-3-5-sonnet-latest").generate("Analyze", media=["chart.png"])
ollama_response = create_llm("ollama", model="qwen2.5-vl:7b").generate("Analyze", media=["chart.png"])

# Vision fallback for text-only models (one-time setup)
# abstractcore --download-vision-model  # Downloads local vision model
# Now text-only models can process images transparently
```

### Universal Tool Calling with @tool Decorator
```python
from abstractcore import create_llm
from abstractcore.tools import tool

@tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    return f"Weather in {city}: 72°F, Sunny"

# Works with ANY provider - OpenAI, Anthropic, Ollama, etc.
llm = create_llm("ollama", model="llama3.2:3b")
response = llm.generate(
    "What's the weather in Paris?",
    tools=[get_weather]  # Automatic format detection and conversion
)

# Built-in tools available in abstractcore.tools.common_tools
from abstractcore.tools.common_tools import fetch_url, search_files, read_file

# Intelligent web content fetching with automatic parsing
result = fetch_url("https://api.github.com/repos/python/cpython")
# Automatically detects JSON, HTML, images, PDFs, etc.

# Available: fetch_url, search_files, list_files, read_file, write_file, 
#           edit_file, web_search, execute_command

# Enhanced metadata for complex tools
@tool(
    description="Search database for records",
    tags=["database", "search"],
    when_to_use="When user asks for specific data",
    examples=[{"description": "Find users named John", "arguments": {"query": "name=John"}}]
)
def search_database(query: str, table: str = "users") -> str:
    return f"Searching {table} for: {query}"

# Tool chaining - LLM automatically calls multiple tools in sequence
@tool
def get_user_location(user_id: str) -> str:
    return {"user123": "Paris", "user456": "Tokyo"}.get(user_id, "Unknown")

# LLM will call get_user_location first, then get_weather with the result
response = llm.generate("What's the weather for user123?", tools=[get_user_location, get_weather])
```

### Session Management with Analytics
```python
from abstractcore import BasicSession, create_llm

llm = create_llm("openai", model="gpt-4o-mini")
session = BasicSession(llm, system_prompt="You are a helpful assistant.")

response1 = session.generate("My name is Alice")
response2 = session.generate("What's my name?")  # Remembers context

# Auto-compaction with SOTA 2025 algorithm
session.compact(target_tokens=8000)  # Compresses while preserving context

# Advanced analytics
summary = session.generate_summary(style="executive", length="brief")
assessment = session.generate_assessment(criteria=["clarity", "helpfulness"])
facts = session.extract_facts(entity_types=["person", "organization", "date"])

# Save with analytics
session.save("conversation.json", summary=True, assessment=True, facts=True)
```

### Async/Await for Concurrent Requests
```python
import asyncio
from abstractcore import create_llm

async def main():
    llm = create_llm("openai", model="gpt-4o-mini")

    # Execute 3 requests concurrently for 3-10x speedup
    tasks = [
        llm.agenerate("Summarize Python"),
        llm.agenerate("Summarize JavaScript"),
        llm.agenerate("Summarize Rust")
    ]
    responses = await asyncio.gather(*tasks)

    for response in responses:
        print(response.content)

asyncio.run(main())

# Async streaming
async def stream_example():
    async for chunk in llm.agenerate("Tell a story", stream=True):
        print(chunk.content, end='', flush=True)

# Session async
async def session_example():
    session = BasicSession(provider=llm)
    response = await session.agenerate("What is Python?")
    print(response.content)

# Works with all 6 providers
# Use cases: batch operations, multi-provider comparisons, FastAPI integration
```

### Generation Parameters (Unified Interface)
```python
from abstractcore import create_llm
from abstractcore.utils.token_utils import estimate_tokens

# Unified parameters work across ALL providers
llm = create_llm(
    "anthropic",
    model="claude-3-5-haiku-latest",
    max_tokens=32000,           # Context window (input + output)
    max_output_tokens=8000,     # Maximum output tokens
    max_input_tokens=24000,     # Maximum input tokens (auto-calculated if not set)
    temperature=0.7,            # Creativity (0.0-1.0)
    seed=42                     # Deterministic outputs (Supported: OpenAI, Ollama, MLX, HuggingFace, LMStudio; ⚠️ Anthropic warns)
)

# Token estimation and validation
text = "Your input text here..."
estimated = estimate_tokens(text, model="claude-3-5-haiku-latest")
print(f"Estimated tokens: {estimated}")

# Budget validation with warnings - ALL providers track input + output separately
response = llm.generate("Write a detailed analysis...")

# Every generate() call returns a GenerateResponse object with consistent structure
print(f"Generated text: {response.content}")         # The actual LLM response
print(f"Model used: {response.model}")               # Model that generated the response
print(f"Finish reason: {response.finish_reason}")   # Why generation stopped

# Consistent token access across ALL providers (NEW in v2.4.7)
print(f"Input tokens: {response.input_tokens}")     # Always available
print(f"Output tokens: {response.output_tokens}")   # Always available  
print(f"Total tokens: {response.total_tokens}")     # Always available
print(f"Generation time: {response.gen_time}ms")    # Always available (rounded to 1 decimal)

# Comprehensive summary
print(f"Summary: {response.get_summary()}")         # "Model: gpt-4o-mini | Tokens: 117 | Time: 1234.5ms"

# Legacy access still works (backward compatibility)
print(f"Legacy input: {response.usage.get('prompt_tokens')}")
print(f"Legacy output: {response.usage.get('completion_tokens')}")
```

### Provider Health Monitoring
```python
from abstractcore import create_llm

# Check provider connectivity before use
provider = create_llm("ollama", model="llama2")
health = provider.health(timeout=3.0)

if health["status"]:
    print(f"Supported: {health['provider']} online: {health['model_count']} models available")
    print(f"Response time: {health['latency_ms']}ms")
    # Proceed with generation
    response = provider.generate("Hello!")
else:
    print(f"Not Supported: {health['provider']} offline: {health['error']}")
    # Fallback to different provider
    provider = create_llm("openai", model="gpt-4o-mini")
```

### Tool Syntax Rewriting (Real-time Format Conversion)
```python
from abstractcore import create_llm
from abstractcore.tools.tag_rewriter import ToolCallTagRewriter

# Custom tag configuration for any agent framework
llm = create_llm(
    "ollama",
    model="llama3.2:3b",
    tool_call_tags="function_call"  # Converts to: <function_call>...JSON...</function_call>
)

# Multiple custom formats
llm = create_llm(
    "mlx",
    model="mlx-community/Qwen2.5-Coder-7B-Instruct-4bit",
    tool_call_tags=("[[tool_use]]", "[[/tool_use]]")  # Start and end tags
)

# Architecture-aware format detection
# Qwen3: <|tool_call|>{...}</|tool_call|>
# LLaMA3: <function_call>{...}</function_call>
# XML-based: <tool_call>{...}</tool_call>
# Claude Code: [[tool_use]]{...}[[/tool_use]]
# OpenAI/Anthropic: Native JSON API calls

# Real-time streaming with format conversion
for chunk in llm.generate("Use the calculator tool", tools=[calculate], stream=True):
    if chunk.tool_calls:
        print(f"Tool detected: {chunk.tool_calls[0].name}")
    print(chunk.content, end="", flush=True)
```

### Structured Output with Native Support
```python
from pydantic import BaseModel
from abstractcore import create_llm

class Person(BaseModel):
    name: str
    age: int

# Ollama and LMStudio use native server-side schema enforcement
# OpenAI and Anthropic also support native structured outputs
llm = create_llm("ollama", model="qwen3:4b")
person = llm.generate(
    "Extract: John Doe is 25 years old",
    response_model=Person
)
print(f"{person.name}, age {person.age}")  # Schema validation with retry

# See comprehensive guide: docs/structured-output.md
# - Native vs prompted strategies across all providers
# - Schema design best practices
# - Performance benchmarks (100% validation success rate)
# - Production deployment guidelines
```

### CLI Applications (Terminal & Python)
```bash
# Terminal usage - no Python code needed
summarizer document.pdf --style executive --output summary.txt
extractor report.txt --format json-ld --focus technology
judge essay.txt --criteria clarity,accuracy --context "academic writing"
```

```python
# Python API usage
from abstractcore.processing import BasicSummarizer, BasicExtractor, BasicJudge

summarizer = BasicSummarizer()
summary = summarizer.summarize(text, style="executive", length="brief")

extractor = BasicExtractor()
kg = extractor.extract(text, output_format="jsonld")

judge = BasicJudge()
assessment = judge.evaluate(text, context="code review", focus="error handling")
```

### HTTP Server (OpenAI-Compatible)
```bash
# Start server
uvicorn abstractcore.server.app:app --host 0.0.0.0 --port 8000
```

```python
# OpenAI-compatible client usage
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="unused"  # Not required for local providers
)

# Route to any provider using model format: provider/model
response = client.chat.completions.create(
    model="ollama/qwen3-coder:30b",      # Ollama provider
    messages=[{"role": "user", "content": "Hello!"}]
)

# File attachments with @filename syntax
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Analyze @report.pdf and @chart.png"}]
)

# OpenAI Responses API (/v1/responses) with native input_file support
import requests
response = requests.post(
    "http://localhost:8000/v1/responses",
    json={
        "model": "gpt-4o",
        "input": [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": "Analyze this document"},
                    {"type": "input_file", "file_url": "https://example.com/report.pdf"}
                ]
            }
        ]
    }
)

# Provider discovery via HTTP API
# GET /providers - Complete provider metadata
# GET /providers/ollama/models - Models for specific provider
```

### Advanced Features
```python
# Architecture-aware tool calling - automatic format detection
# Event system for production monitoring
from abstractcore.events import EventType, on_global

def cost_monitor(event):
    if event.cost_usd and event.cost_usd > 0.10:
        alert(f"High cost request: ${event.cost_usd}")

on_global(EventType.GENERATION_COMPLETED, cost_monitor)

# Memory management for local models
llm = create_llm("ollama", model="large-model")
response = llm.generate("Hello")
llm.unload()  # Free memory

# Production resilience with retry and circuit breaker
from abstractcore.resilience import RetryManager, CircuitBreaker

llm = create_llm(
    "openai",
    model="gpt-4o-mini",
    retry_manager=RetryManager(max_attempts=3, backoff_strategy="exponential"),
    circuit_breaker=CircuitBreaker(failure_threshold=5, timeout=60)
)
```

### Debug & Performance Features
```bash
# Debug mode shows raw LLM responses
judge document.txt --debug --provider lmstudio --model qwen/qwen3-next-80b

# Advanced extractor modes and iterations
extractor document.pdf --mode thorough --iterate 3 --similarity-threshold 0.9
extractor large_file.txt --mode fast --no-embeddings --minified

# Focus areas for targeted evaluation
judge README.md --focus "architectural diagrams, technical comparisons" --debug
judge code.py --focus "error handling, performance" --temperature 0.05

# Self-healing JSON handles truncated responses automatically
# Token limits: max_tokens=32k, max_output_tokens=8k prevent truncation
```

## API Summary for AI Systems

### Core Functions
```python
# Factory
from abstractcore import create_llm, BasicSession

# Provider Discovery
from abstractcore.providers import (
    get_all_providers_with_models,
    list_available_providers,
    get_provider_info,
    is_provider_available
)

# Tools
from abstractcore.tools import tool, UniversalToolHandler

# Token Management
from abstractcore.utils.token_utils import estimate_tokens, calculate_token_budget

# Processing
from abstractcore.processing import BasicSummarizer, BasicExtractor, BasicJudge

# Events
from abstractcore.events import EventType, on_global, emit_global

# Resilience
from abstractcore.resilience import RetryManager, CircuitBreaker
```

### Supported Providers
| Provider | Features | SEED Support | Default Model |
|----------|----------|-------------|---------------|
| **OpenAI** | Native tools, streaming, native structured output, vision (GPT-4o), media processing | Supported: Native | gpt-4o-mini |
| **Anthropic** | Native tools, streaming, native structured output, vision (Claude 3.5), media processing | ⚠️ Warning* | claude-3-5-haiku-latest |
| **Ollama** | Prompted tools, streaming, native structured output, local models, vision (qwen2.5vl:7b), media processing | Supported: Native | llama3.2:3b |
| **LMStudio** | Prompted tools, streaming, native structured output, local models, vision (qwen2.5-vl), media processing | Supported: Native | llama-3.2-8b-instruct |
| **MLX** | Prompted tools, streaming, Apple Silicon, vision models, media processing | Supported: Native | mlx-community/Qwen2.5-Coder-7B-Instruct-4bit |
| **HuggingFace** | Prompted tools, streaming, open models, vision models, media processing | Supported: Native | unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit |

*Anthropic doesn't support seed parameters but issues a warning when provided. Use `temperature=0.0` for more consistent outputs.

### Key Capabilities for AI
- **130+ models** across 6 providers via centralized registry
- **Async/await support** for concurrent requests with 3-10x faster batch operations
- **Universal media handling** with `media=[]` API and `@filename` CLI syntax
- **Vision capabilities** across all providers with automatic optimization and fallback
- **Centralized configuration** with global defaults and app-specific preferences
- **Real-time tool syntax conversion** for agent CLI compatibility
- **SOTA 2025 session compaction** with context preservation
- **Unified token parameters** across all providers
- **Event-driven architecture** for monitoring and control
- **Production resilience** with retries and circuit breakers
- **Self-healing JSON** for robust structured output
- **OpenAI-compatible server** with /v1/responses endpoint for multi-language access
- **Provider health checks** with `.health()` method for connectivity monitoring
- **Centralized token counting** with consistent input/output token tracking across all providers

---

### Model Capability Filtering
```python
from abstractcore.providers import OllamaProvider, ModelInputCapability, ModelOutputCapability

# Vision models
vision_models = OllamaProvider.list_available_models(
    input_capabilities=[ModelInputCapability.IMAGE]
)

# Embedding models
embedding_models = OllamaProvider.list_available_models(
    output_capabilities=[ModelOutputCapability.EMBEDDINGS]
)
```

**HTTP API:**
```bash
# Vision models
curl http://localhost:8000/v1/models?input_type=image

# Embedding models
curl http://localhost:8000/v1/models?output_type=embeddings

# Combined filtering
curl http://localhost:8000/v1/models?provider=ollama&input_type=image
```

## Quick Links

- **GitHub**: https://github.com/lpalbou/AbstractCore
- **PyPI**: `pip install abstractcore[all]`
- **License**: MIT | **Python**: 3.9+ | **Status**: Production Ready

**Complete Documentation**: [llms-full.txt](https://lpalbou.github.io/AbstractCore/llms-full.txt)