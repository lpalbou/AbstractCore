# Deep Search Optimization Proposals

**Date**: 2025-10-26
**Status**: Query Intent System Complete - Optimization Phase
**Priority**: Performance & Advanced Features
**Estimated Total Time**: 8-12 hours

---

## Executive Summary

With the Query Intent System now 100% complete (Phases 1-4), BasicDeepResearcherC is production-ready. This document outlines **actionable optimization proposals** to improve performance, scalability, and capabilities.

**Current State**:
- ‚úÖ Functional and production-ready
- ‚úÖ 100% backward compatible
- ‚úÖ Conservative grounding maintained
- ‚ö†Ô∏è Sequential operations (room for parallelization)
- ‚ö†Ô∏è No caching (repeated queries inefficient)

**Optimization Goals**:
1. **30-40% faster** execution through parallelization
2. **Near-instant** results for cached queries
3. **10-15% better quality** through deduplication
4. **Enhanced capabilities** through advanced features

---

## Priority 1: Performance Optimizations

### **1.1 Parallel Evidence Extraction** üöÄ

**Current State**: Sequential URL fetching
```python
for url in urls:
    evidence = self._analyze_source_relevance(url)
    if evidence:
        self.evidence.append(evidence)
```

**Problem**:
- Fetches URLs one at a time
- Network I/O bound (waiting for responses)
- 10 URLs √ó 3 seconds each = 30 seconds total

**Proposal**: Parallel fetching with ThreadPoolExecutor

**Implementation**:
```python
# In basic_deepresearcherC.py
from concurrent.futures import ThreadPoolExecutor, as_completed

def _execute_react_action_parallel(self, action: ReActAction) -> None:
    """
    Execute ReAct action with parallel URL fetching

    Performance: 30-40% faster for 10+ URLs
    """
    if action.action_type == "search":
        # Get URLs from search (unchanged)
        search_results = self._perform_search(action.search_query)
        urls_to_probe = [r["url"] for r in search_results[:self.max_urls_to_probe]]

        # PARALLEL FETCHING (NEW)
        def fetch_and_analyze(url):
            try:
                return self._analyze_source_relevance(url)
            except Exception as e:
                logger.warning(f"Failed to fetch {url}: {e}")
                return None

        with ThreadPoolExecutor(max_workers=5) as executor:
            # Submit all URLs for parallel processing
            future_to_url = {
                executor.submit(fetch_and_analyze, url): url
                for url in urls_to_probe
                if url not in self.seen_urls
            }

            # Collect results as they complete
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                self.seen_urls.add(url)

                evidence = future.result()
                if evidence:
                    self.evidence.append(evidence)

                    # Track finding-to-source mapping
                    for fact in evidence.key_facts:
                        self.finding_to_source[fact] = evidence.url

                    if self.debug:
                        logger.info(f"‚úÖ Processed: {url}")
```

**Configuration**:
```python
def __init__(self, ..., max_parallel_fetches: int = 5):
    """
    Args:
        max_parallel_fetches: Number of concurrent URL fetches (default: 5)
    """
    self.max_parallel_fetches = max_parallel_fetches
```

**Expected Impact**:
- **Speed**: 30-40% faster for 10+ URLs
- **Throughput**: 5√ó parallel requests vs sequential
- **Time Saved**: 10 URLs: 30s ‚Üí 10s (20s savings)

**Risks & Mitigation**:
- **Risk**: Rate limiting from search engines
  - **Mitigation**: Configurable max_parallel_fetches (default: 5)
  - **Mitigation**: Exponential backoff on 429 errors
- **Risk**: Thread safety issues
  - **Mitigation**: Use thread-safe data structures
  - **Mitigation**: Lock for shared state (self.evidence.append)

**Testing**:
```python
# Test parallel vs sequential
import time

# Sequential (baseline)
start = time.time()
researcher_seq = BasicDeepResearcherC(llm, max_parallel_fetches=1)
result_seq = researcher_seq.research("How to write a LangGraph agent")
seq_time = time.time() - start

# Parallel
start = time.time()
researcher_par = BasicDeepResearcherC(llm, max_parallel_fetches=5)
result_par = researcher_par.research("How to write a LangGraph agent")
par_time = time.time() - start

print(f"Sequential: {seq_time:.1f}s")
print(f"Parallel: {par_time:.1f}s")
print(f"Speedup: {(seq_time/par_time):.1f}x")
```

**Priority**: HIGH
**Estimated Time**: 3-4 hours
**Complexity**: Medium
**Impact**: High (30-40% faster)

---

### **1.2 Synthesis Caching** ‚ö°

**Current State**: Re-synthesize on every call
```python
final_report = self._synthesize_with_grounding()  # Always generates new
```

**Problem**:
- Repeated queries re-synthesize from scratch
- LLM synthesis is expensive (5-10 seconds)
- No benefit from previous identical queries

**Proposal**: Cache synthesis results by evidence fingerprint

**Implementation**:
```python
import hashlib
import json
from functools import lru_cache

class BasicDeepResearcherC:
    def __init__(self, ..., enable_cache: bool = True):
        self.enable_cache = enable_cache
        self.synthesis_cache = {}  # evidence_hash -> SynthesisModel

    def _evidence_fingerprint(self) -> str:
        """
        Generate unique fingerprint for current evidence set

        Returns:
            str: MD5 hash of evidence URLs + key facts
        """
        evidence_data = [
            {
                "url": ev.url,
                "facts": sorted(ev.key_facts),  # Sorted for consistency
                "relevance": ev.relevance_score
            }
            for ev in sorted(self.evidence, key=lambda e: e.url)  # Sorted
        ]

        fingerprint = json.dumps(evidence_data, sort_keys=True)
        return hashlib.md5(fingerprint.encode()).hexdigest()

    def _synthesize_with_grounding(self) -> SynthesisModel:
        """
        Synthesize with caching support
        """
        # Check cache if enabled
        if self.enable_cache:
            cache_key = self._evidence_fingerprint()

            if cache_key in self.synthesis_cache:
                if self.debug:
                    logger.info("‚úÖ Using cached synthesis")
                return self.synthesis_cache[cache_key]

        # Adaptive routing (existing logic)
        if self.query_intent:
            # ... existing code ...

        # Generate synthesis (existing code)
        # ... existing synthesis code ...

        # Cache result
        if self.enable_cache:
            self.synthesis_cache[cache_key] = response
            if self.debug:
                logger.info(f"üíæ Cached synthesis (key: {cache_key[:8]}...)")

        return response
```

**Advanced: Persistent Cache**:
```python
import pickle
from pathlib import Path

class BasicDeepResearcherC:
    def __init__(self, ..., cache_dir: Optional[str] = None):
        """
        Args:
            cache_dir: Directory for persistent cache (None = memory only)
        """
        self.cache_dir = Path(cache_dir) if cache_dir else None
        if self.cache_dir:
            self.cache_dir.mkdir(exist_ok=True)

    def _load_from_cache(self, cache_key: str) -> Optional[SynthesisModel]:
        """Load synthesis from disk cache"""
        if not self.cache_dir:
            return None

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def _save_to_cache(self, cache_key: str, synthesis: SynthesisModel):
        """Save synthesis to disk cache"""
        if not self.cache_dir:
            return

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(synthesis, f)
```

**Expected Impact**:
- **Speed**: Near-instant (< 1s) for cached queries
- **Cost**: No LLM calls for repeated queries
- **Cache Hit Rate**: 20-30% for common queries

**Risks & Mitigation**:
- **Risk**: Stale cache (query intent changes)
  - **Mitigation**: Include query_intent in fingerprint
  - **Mitigation**: TTL (time-to-live) for cache entries
- **Risk**: Memory usage for large caches
  - **Mitigation**: LRU eviction policy (keep last 100)
  - **Mitigation**: Persistent disk cache option

**Testing**:
```python
# Test cache hit
researcher = BasicDeepResearcherC(llm, enable_cache=True)

# First query (cache miss)
start = time.time()
result1 = researcher.research("What is quantum computing?")
time1 = time.time() - start

# Second identical query (cache hit)
start = time.time()
result2 = researcher.research("What is quantum computing?")
time2 = time.time() - start

print(f"First run: {time1:.1f}s")
print(f"Second run (cached): {time2:.1f}s")
print(f"Speedup: {(time1/time2):.1f}x")
```

**Priority**: MEDIUM
**Estimated Time**: 2-3 hours
**Complexity**: Medium
**Impact**: Very High (near-instant for cached)

---

### **1.3 Semantic Source Deduplication** üéØ

**Current State**: URL-based deduplication only
```python
if url not in self.seen_urls:
    self.seen_urls.add(url)
```

**Problem**:
- Same content from different URLs (mirrors, copies)
- Wastes time processing duplicates
- Reduces diversity of evidence

**Proposal**: Semantic deduplication using content similarity

**Implementation**:
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class BasicDeepResearcherC:
    def __init__(self, ..., dedupe_threshold: float = 0.85):
        """
        Args:
            dedupe_threshold: Similarity threshold for deduplication (0.85 = 85% similar)
        """
        self.dedupe_threshold = dedupe_threshold
        self.content_fingerprints = []  # List of (url, tfidf_vector)
        self.vectorizer = TfidfVectorizer(max_features=100)

    def _is_duplicate_content(self, url: str, content: str) -> bool:
        """
        Check if content is duplicate of existing evidence

        Args:
            url: Source URL
            content: Text content to check

        Returns:
            bool: True if duplicate (similarity > threshold)
        """
        if not self.content_fingerprints:
            return False

        # Vectorize new content
        try:
            # Combine with existing content for consistent vocabulary
            all_content = [content] + [fp[2] for fp in self.content_fingerprints]
            vectors = self.vectorizer.fit_transform(all_content)
            new_vector = vectors[0]
            existing_vectors = vectors[1:]

            # Calculate similarities
            similarities = cosine_similarity(new_vector, existing_vectors)[0]

            # Check if any similarity exceeds threshold
            max_similarity = np.max(similarities)
            if max_similarity > self.dedupe_threshold:
                duplicate_idx = np.argmax(similarities)
                duplicate_url = self.content_fingerprints[duplicate_idx][0]

                if self.debug:
                    logger.info(
                        f"‚ö†Ô∏è Duplicate content detected: {url} "
                        f"(similar to {duplicate_url}, {max_similarity:.2%})"
                    )
                return True

            return False

        except Exception as e:
            logger.warning(f"Deduplication failed: {e}")
            return False  # Fail open (allow if error)

    def _analyze_source_relevance(self, url: str) -> Optional[SourceEvidence]:
        """
        Enhanced with semantic deduplication
        """
        # Skip if URL already seen
        if url in self.seen_urls:
            return None

        # Fetch content
        try:
            content = fetch_url(url, timeout=self.fetch_timeout)
        except Exception as e:
            return None

        # Check for semantic duplicates
        if self._is_duplicate_content(url, content):
            self.seen_urls.add(url)  # Mark as seen
            return None  # Skip duplicate

        # Process normally (existing code)
        assessment = self.llm.generate(prompt, response_model=SourceRelevanceModel)

        # If relevant, store fingerprint
        if assessment.relevance_score >= self.grounding_threshold:
            self.content_fingerprints.append((url, assessment, content))

        # ... rest of existing code ...
```

**Lightweight Alternative** (no sklearn dependency):
```python
def _simple_content_fingerprint(self, content: str) -> set:
    """
    Simple fingerprint using word set

    Returns:
        set: Set of significant words
    """
    # Tokenize and filter
    words = content.lower().split()
    # Keep words 4+ characters
    significant = {w for w in words if len(w) >= 4}
    return significant

def _is_duplicate_simple(self, url: str, content: str) -> bool:
    """
    Simple Jaccard similarity check
    """
    new_fingerprint = self._simple_content_fingerprint(content)

    for existing_url, existing_fp in self.content_fingerprints:
        # Jaccard similarity
        intersection = len(new_fingerprint & existing_fp)
        union = len(new_fingerprint | existing_fp)
        similarity = intersection / union if union > 0 else 0

        if similarity > self.dedupe_threshold:
            if self.debug:
                logger.info(f"‚ö†Ô∏è Duplicate: {url} ~ {existing_url} ({similarity:.2%})")
            return True

    return False
```

**Expected Impact**:
- **Quality**: 10-15% better source diversity
- **Speed**: 10-20% faster (skip duplicate processing)
- **Uniqueness**: Higher quality evidence set

**Risks & Mitigation**:
- **Risk**: False positives (marking unique as duplicate)
  - **Mitigation**: Conservative threshold (0.85 = 85% similar)
  - **Mitigation**: User-configurable threshold
- **Risk**: Computation overhead
  - **Mitigation**: Lightweight fingerprinting (word sets)
  - **Mitigation**: Only compute for relevant sources

**Testing**:
```python
# Test deduplication
researcher = BasicDeepResearcherC(llm, dedupe_threshold=0.85)

# Add mirrors/copies of same content
urls = [
    "https://original.com/article",
    "https://mirror.com/same-article",  # Should be detected as duplicate
    "https://different.com/other-topic"  # Should pass
]

for url in urls:
    evidence = researcher._analyze_source_relevance(url)
    print(f"{url}: {'‚úÖ Added' if evidence else '‚ö†Ô∏è Skipped (duplicate)'}")
```

**Priority**: MEDIUM
**Estimated Time**: 2-3 hours
**Complexity**: Medium
**Impact**: Medium (10-15% quality improvement)

---

## Priority 2: Advanced Features

### **2.1 Link Exploration (Follow Embedded Links)** üîó

**Current State**: Single-level source fetching
```python
# Only fetch URLs from search results
search_results = self._perform_search(query)
```

**Problem**:
- Misses valuable linked resources
- Wikipedia ‚Üí cited papers
- Tutorials ‚Üí official docs
- Blog posts ‚Üí source code repos

**Proposal**: Recursive link exploration with depth control

**Implementation**:
```python
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

class BasicDeepResearcherC:
    def __init__(self, ..., explore_links: bool = False, max_link_depth: int = 1):
        """
        Args:
            explore_links: Enable link exploration
            max_link_depth: Maximum depth for link following (1 = direct links only)
        """
        self.explore_links = explore_links
        self.max_link_depth = max_link_depth
        self.link_depth_map = {}  # url -> depth

    def _extract_relevant_links(self, url: str, html_content: str, max_links: int = 5) -> List[str]:
        """
        Extract relevant links from HTML content

        Args:
            url: Source URL (for resolving relative links)
            html_content: HTML content
            max_links: Maximum links to extract

        Returns:
            List[str]: Relevant links (prioritized)
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []

        # Priority link patterns
        priority_patterns = [
            'docs', 'documentation', 'tutorial', 'guide', 'github',
            'paper', 'arxiv', 'research', 'official'
        ]

        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']

            # Resolve relative URLs
            full_url = urljoin(url, href)

            # Filter out non-http(s) links
            if not full_url.startswith(('http://', 'https://')):
                continue

            # Skip if already seen
            if full_url in self.seen_urls:
                continue

            # Priority scoring
            link_text = a_tag.get_text().lower()
            score = sum(1 for p in priority_patterns if p in full_url.lower() or p in link_text)

            links.append((full_url, score))

        # Sort by priority and return top N
        links.sort(key=lambda x: x[1], reverse=True)
        return [url for url, score in links[:max_links]]

    def _explore_links_recursive(self, url: str, depth: int = 0):
        """
        Recursively explore links from source

        Args:
            url: Source URL
            depth: Current exploration depth
        """
        if not self.explore_links:
            return

        if depth >= self.max_link_depth:
            return

        if url in self.seen_urls:
            return

        # Fetch and analyze source
        evidence = self._analyze_source_relevance(url)
        if not evidence:
            return

        # Extract embedded links
        try:
            html_content = fetch_url(url, timeout=self.fetch_timeout)
            embedded_links = self._extract_relevant_links(url, html_content)

            if self.debug:
                logger.info(f"üîó Found {len(embedded_links)} links in {url}")

            # Explore each embedded link (recursive)
            for link in embedded_links:
                self.link_depth_map[link] = depth + 1
                self._explore_links_recursive(link, depth + 1)

        except Exception as e:
            logger.warning(f"Link exploration failed for {url}: {e}")
```

**Expected Impact**:
- **Quality**: 20-30% more high-quality sources
- **Coverage**: Find official docs, papers, repos
- **Depth**: Access resources not in search results

**Risks & Mitigation**:
- **Risk**: Exponential explosion (too many links)
  - **Mitigation**: max_link_depth parameter (default: 1)
  - **Mitigation**: max_links per page (default: 5)
  - **Mitigation**: Priority filtering (docs, tutorials, official)
- **Risk**: Slower execution
  - **Mitigation**: Combine with parallel fetching
  - **Mitigation**: User-configurable (default: disabled)

**Testing**:
```python
# Test link exploration
researcher = BasicDeepResearcherC(
    llm,
    explore_links=True,
    max_link_depth=1,  # Only follow direct links
    max_parallel_fetches=5
)

result = researcher.research("How to write a LangGraph agent")

# Check metadata
print(f"Sources from search: {result.research_metadata.get('search_sources', 0)}")
print(f"Sources from links: {result.research_metadata.get('link_sources', 0)}")
print(f"Total sources: {len(result.sources_selected)}")
```

**Priority**: LOW
**Estimated Time**: 4-5 hours
**Complexity**: High
**Impact**: Medium-High (20-30% more sources)

---

### **2.2 Incremental Research (Continue from Previous)** üìà

**Current State**: Fresh start every time
```python
researcher.research(query)  # Always starts from scratch
```

**Problem**:
- Can't continue from previous results
- Can't add more sources to existing research
- Can't refine with additional queries

**Proposal**: Support incremental research continuation

**Implementation**:
```python
class BasicDeepResearcherC:
    def research(
        self,
        query: str,
        focus_areas: Optional[List[str]] = None,
        continue_from: Optional['ResearchOutput'] = None
    ) -> ResearchOutput:
        """
        Conduct research with optional continuation

        Args:
            query: Research question
            focus_areas: Optional focus areas
            continue_from: Previous research to continue from

        Returns:
            ResearchOutput: Combined results
        """
        # Restore state from previous research
        if continue_from:
            self._restore_from_previous(continue_from)
            if self.debug:
                logger.info(f"üì• Continuing from previous research ({len(self.evidence)} existing sources)")

        # Run research (existing logic)
        # ...

        return output

    def _restore_from_previous(self, previous: ResearchOutput):
        """
        Restore state from previous research

        Args:
            previous: Previous research output
        """
        # Restore evidence
        for source in previous.sources_selected:
            # Reconstruct SourceEvidence
            evidence = SourceEvidence(
                url=source['url'],
                title=source.get('title', ''),
                relevance_score=source.get('relevance_score', 0.8),
                credibility_score=source.get('credibility_score', 0.8),
                key_facts=source.get('facts', []),
                timestamp=source.get('timestamp', datetime.now().isoformat())
            )
            self.evidence.append(evidence)
            self.seen_urls.add(source['url'])

        # Restore gaps
        for gap in previous.knowledge_gaps:
            if gap not in self.active_gaps:
                self.active_gaps[gap] = {
                    "status": "active",
                    "source_urls": [],
                    "related_findings": [],
                    "dimension": "synthesis"
                }

        # Restore query intent if available
        if previous.research_metadata.get('query_intent'):
            # Would need to serialize/deserialize QueryIntent
            pass
```

**Usage**:
```python
# Initial research
result1 = researcher.research("What is quantum computing?")

# Continue with more specific query
result2 = researcher.research(
    "What are quantum error correction codes?",
    continue_from=result1  # Builds on previous
)

# result2 includes:
# - All sources from result1
# - New sources from result2
# - Combined synthesis
```

**Expected Impact**:
- **Flexibility**: Iterative research workflows
- **Efficiency**: Reuse previous work
- **Quality**: Build deeper understanding

**Priority**: LOW
**Estimated Time**: 3-4 hours
**Complexity**: Medium
**Impact**: Medium (new capability)

---

### **2.3 Multi-Query Batching** üì¶

**Current State**: One query at a time
```python
result = researcher.research(query)
```

**Proposal**: Batch multiple related queries, share evidence

**Implementation**:
```python
def research_batch(
    self,
    queries: List[str],
    share_evidence: bool = True
) -> List[ResearchOutput]:
    """
    Research multiple queries with optional evidence sharing

    Args:
        queries: List of research questions
        share_evidence: Share evidence across queries

    Returns:
        List[ResearchOutput]: Results for each query
    """
    results = []

    if share_evidence:
        # Collect evidence for all queries first
        combined_evidence = []

        for query in queries:
            # Phase 1-3: Context, Tasks, Evidence
            self._initialize_context(query)
            self._decompose_query()
            self._react_loop()
            combined_evidence.extend(self.evidence)

        # Deduplicate combined evidence
        self.evidence = self._deduplicate_evidence(combined_evidence)

        # Phase 4: Synthesize for each query
        for query in queries:
            self.context.query = query
            final_report = self._synthesize_with_grounding()
            results.append(self._build_output(final_report))

    else:
        # Process each independently
        for query in queries:
            results.append(self.research(query))

    return results
```

**Expected Impact**:
- **Efficiency**: Shared evidence collection
- **Quality**: More comprehensive evidence pool

**Priority**: LOW
**Estimated Time**: 2-3 hours
**Complexity**: Low-Medium
**Impact**: Medium (batch workflows)

---

## Priority 3: User Experience Enhancements

### **3.1 Progress Callbacks** üìä

**Current State**: Silent execution (only debug logs)

**Proposal**: Optional progress callbacks

**Implementation**:
```python
from typing import Callable

class BasicDeepResearcherC:
    def __init__(
        self,
        ...,
        progress_callback: Optional[Callable[[str, float], None]] = None
    ):
        """
        Args:
            progress_callback: Callback for progress updates (message, progress_pct)
        """
        self.progress_callback = progress_callback

    def _report_progress(self, message: str, progress: float):
        """Report progress if callback provided"""
        if self.progress_callback:
            self.progress_callback(message, progress)

    def research(self, query: str, ...) -> ResearchOutput:
        self._report_progress("Analyzing query intent...", 0.1)
        # ...
        self._report_progress("Fetching sources...", 0.3)
        # ...
        self._report_progress("Synthesizing report...", 0.9)
        # ...
        self._report_progress("Complete!", 1.0)
```

**Usage**:
```python
def progress_handler(message: str, progress: float):
    print(f"[{progress*100:.0f}%] {message}")

researcher = BasicDeepResearcherC(llm, progress_callback=progress_handler)
result = researcher.research("What is quantum computing?")
```

**Priority**: LOW
**Estimated Time**: 1-2 hours
**Complexity**: Low
**Impact**: Medium (better UX)

---

## Implementation Roadmap

### **Phase 1: Performance** (5-7 hours)
1. Parallel Evidence Extraction (3-4 hours) - HIGH IMPACT
2. Synthesis Caching (2-3 hours) - HIGH IMPACT
3. Semantic Deduplication (2-3 hours) - MEDIUM IMPACT

**Expected Result**: 30-50% faster execution, near-instant cached queries

### **Phase 2: Advanced Features** (7-10 hours)
1. Link Exploration (4-5 hours) - NEW CAPABILITY
2. Incremental Research (3-4 hours) - NEW CAPABILITY
3. Multi-Query Batching (2-3 hours) - NEW CAPABILITY

**Expected Result**: Deeper research, flexible workflows

### **Phase 3: UX Enhancements** (1-2 hours)
1. Progress Callbacks (1-2 hours) - BETTER UX

**Expected Result**: Transparent execution

---

## Testing Strategy

### **Performance Benchmarks**

```python
# Benchmark suite
def benchmark_optimizations():
    queries = [
        "How to write a LangGraph agent",
        "Compare BAML and Outlines",
        "What is quantum computing?"
    ]

    configurations = [
        {"name": "Baseline", "parallel": 1, "cache": False, "dedupe": False},
        {"name": "Parallel", "parallel": 5, "cache": False, "dedupe": False},
        {"name": "Cached", "parallel": 1, "cache": True, "dedupe": False},
        {"name": "Full", "parallel": 5, "cache": True, "dedupe": True},
    ]

    for config in configurations:
        researcher = BasicDeepResearcherC(
            llm,
            max_parallel_fetches=config["parallel"],
            enable_cache=config["cache"],
            dedupe_threshold=0.85 if config["dedupe"] else None
        )

        times = []
        for query in queries:
            start = time.time()
            result = researcher.research(query)
            times.append(time.time() - start)

        print(f"{config['name']}: {sum(times)/len(times):.1f}s average")
```

**Expected Results**:
```
Baseline: 60.0s average
Parallel: 40.0s average (-33%)
Cached: 5.0s average (second run, -92%)
Full: 25.0s average (first run, -58%)
```

---

## Configuration Management

**Recommended Defaults**:
```python
researcher = BasicDeepResearcherC(
    llm,
    max_sources=30,
    max_parallel_fetches=5,         # ‚Üê NEW (Optimization 1.1)
    enable_cache=True,               # ‚Üê NEW (Optimization 1.2)
    cache_dir=".cache/synthesis",    # ‚Üê NEW (persistent cache)
    dedupe_threshold=0.85,           # ‚Üê NEW (Optimization 1.3)
    explore_links=False,             # ‚Üê NEW (Optimization 2.1, default: off)
    max_link_depth=1,                # ‚Üê NEW (if explore_links=True)
    progress_callback=None,          # ‚Üê NEW (Optimization 3.1)
    debug=False
)
```

**Production Recommendations**:
- **Parallel Fetches**: 5 (balance speed vs rate limiting)
- **Caching**: Enabled (huge speedup for repeated queries)
- **Deduplication**: 0.85 threshold (85% similar = duplicate)
- **Link Exploration**: Disabled by default (user opt-in)

---

## Success Metrics

| Optimization | Metric | Target | Measurement |
|--------------|--------|--------|-------------|
| Parallel Fetching | Speed improvement | 30-40% | Benchmark suite |
| Caching | Cache hit rate | 20-30% | Production usage |
| Caching | Cached query speed | <1s | Benchmark |
| Deduplication | Duplicate rate | 10-15% | Log analysis |
| Link Exploration | Additional sources | 20-30% | Source count |

---

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Rate limiting (parallel) | Medium | Medium | Configurable max_parallel, backoff |
| False cache hits | Low | Medium | Include query_intent in fingerprint |
| False deduplication | Low | Medium | Conservative threshold (0.85) |
| Link explosion | Medium | High | max_link_depth, priority filtering |
| Thread safety issues | Low | High | Use locks for shared state |

---

## Conclusion

These optimization proposals provide:

1. **30-50% faster execution** through parallelization and caching
2. **Near-instant results** for repeated queries
3. **Higher quality sources** through deduplication and link exploration
4. **Flexible workflows** through incremental research and batching
5. **Better UX** through progress transparency

**Recommended Priority**:
1. **Phase 1** (Performance) - Immediate, high-impact improvements
2. **Phase 3** (UX) - Quick win for user experience
3. **Phase 2** (Advanced Features) - When needed for specific use cases

**Total Estimated Time**: 13-19 hours for full implementation

**Status**: Ready for implementation - all proposals are actionable with clear implementation paths.

---

**Document Version**: 1.0
**Last Updated**: 2025-10-26
**Next Review**: After Phase 1 implementation
