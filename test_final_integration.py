#!/usr/bin/env python3
"""
Final integration test: Verify enhanced tools work with real LLM calls.

This test demonstrates that the migrated sophisticated tools work correctly
with the enhanced metadata system and provide better LLM guidance.
"""

from abstractllm import create_llm
from abstractllm.tools.common_tools import list_files, search_files, read_file, write_file
from abstractllm.tools.handler import UniversalToolHandler


def test_enhanced_prompt_generation():
    """Test that enhanced metadata generates richer prompts."""
    print("üîç Testing Enhanced Prompt Generation")
    print("=" * 50)

    # Create tool handler
    handler = UniversalToolHandler("qwen3-coder:30b")

    # Generate enhanced prompt with migrated tools
    tools = [list_files, search_files, read_file, write_file]
    enhanced_prompt = handler.format_tools_prompt(tools)

    print(f"üìä Enhanced prompt length: {len(enhanced_prompt)} characters")

    # Check for enhanced features
    features = {
        "when_to_use guidance": "When to use" in enhanced_prompt,
        "tag information": "Tags:" in enhanced_prompt,
        "concrete examples": "EXAMPLES:" in enhanced_prompt,
        "multiple examples per tool": enhanced_prompt.count("description") > 10,
        "tool call formats": enhanced_prompt.count("<|tool_call|>") > 5,
        "rich descriptions": enhanced_prompt.count("file") > 20
    }

    print(f"\nüìã Enhanced Features Detected:")
    for feature, present in features.items():
        status = "‚úÖ" if present else "‚ùå"
        print(f"   {status} {feature}")

    # Show sample of the prompt
    print(f"\nüìÑ Prompt Sample (first 800 chars):")
    print("-" * 40)
    print(enhanced_prompt[:800] + "..." if len(enhanced_prompt) > 800 else enhanced_prompt)

    return enhanced_prompt


def test_real_llm_with_enhanced_tools():
    """Test real LLM calls with enhanced tools."""
    print(f"\nü§ñ Testing Real LLM with Enhanced Tools")
    print("=" * 50)

    try:
        # Create LLM instance
        llm = create_llm("ollama", model="qwen3-coder:30b", base_url="http://localhost:11434")

        # Test complex multi-tool scenario
        prompt = """I need to analyze the current project structure. Please:
1. List all Python files in the current directory recursively
2. Search for any function definitions containing 'tool' in the codebase
3. Read the first 10 lines of the abstractllm/tools/core.py file

Use the available tools to help me understand the project structure."""

        print(f"üì§ Complex prompt: {prompt[:100]}...")

        # Generate with enhanced tools
        response = llm.generate(
            prompt,
            tools=[list_files, search_files, read_file],
            stream=False
        )

        print(f"\nüì• Response Analysis:")
        print(f"   ‚Ä¢ Content length: {len(response.content)} chars")

        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"   ‚Ä¢ Tool calls made: {len(response.tool_calls)}")
            for i, call in enumerate(response.tool_calls, 1):
                print(f"     {i}. {call.name}({list(call.arguments.keys())})")

            # Check if tools were used appropriately
            tool_names = [call.name for call in response.tool_calls]
            appropriate_usage = {
                "list_files used": "list_files" in tool_names,
                "search_files used": "search_files" in tool_names,
                "read_file used": "read_file" in tool_names,
                "multiple tools used": len(set(tool_names)) > 1
            }

            print(f"\nüìä Tool Usage Analysis:")
            for usage, success in appropriate_usage.items():
                status = "‚úÖ" if success else "‚ùå"
                print(f"   {status} {usage}")

        else:
            print(f"   ‚ùå No tool calls detected")

        # Check response quality
        quality_indicators = {
            "mentions files": any(word in response.content.lower() for word in ["file", "python", "directory"]),
            "mentions tools": any(word in response.content.lower() for word in ["tool", "function", "definition"]),
            "structured response": any(word in response.content for word in ["1.", "2.", "3.", "‚Ä¢", "-"]),
            "technical content": any(word in response.content.lower() for word in ["core.py", "abstractllm", "class", "def"])
        }

        print(f"\nüìà Response Quality Analysis:")
        for indicator, present in quality_indicators.items():
            status = "‚úÖ" if present else "‚ùå"
            print(f"   {status} {indicator}")

        return True

    except Exception as e:
        print(f"‚ùå LLM test failed: {e}")
        return False


def test_comparison_with_basic_tools():
    """Compare enhanced tools vs basic tool usage."""
    print(f"\n‚öñÔ∏è  Enhanced vs Basic Tools Comparison")
    print("=" * 50)

    try:
        llm = create_llm("ollama", model="qwen3-coder:30b", base_url="http://localhost:11434")

        # Create a basic tool definition (no enhanced metadata)
        from abstractllm.tools.core import ToolDefinition

        basic_list_tool = ToolDefinition.from_function(list_files)
        basic_list_tool.name = "list_files"
        basic_list_tool.description = "List files in a directory"
        # No tags, when_to_use, or examples

        prompt = "Find all Python test files in the current directory"

        print("  üß™ Testing with ENHANCED tools...")
        enhanced_response = llm.generate(
            prompt,
            tools=[list_files],  # Enhanced with metadata
            stream=False
        )

        print("  üìÑ Testing with BASIC tools...")
        basic_response = llm.generate(
            prompt,
            tools=[basic_list_tool],  # Basic without metadata
            stream=False
        )

        # Compare results
        enhanced_calls = len(enhanced_response.tool_calls) if hasattr(enhanced_response, 'tool_calls') and enhanced_response.tool_calls else 0
        basic_calls = len(basic_response.tool_calls) if hasattr(basic_response, 'tool_calls') and basic_response.tool_calls else 0

        print(f"\nüìä Comparison Results:")
        print(f"   Enhanced tool calls: {enhanced_calls}")
        print(f"   Basic tool calls: {basic_calls}")

        # Check if enhanced version used better arguments
        if enhanced_calls > 0 and enhanced_response.tool_calls:
            enhanced_args = enhanced_response.tool_calls[0].arguments
            recursive_used = enhanced_args.get('recursive', False)
            pattern_used = enhanced_args.get('pattern', '*')

            print(f"   Enhanced used recursive: {'‚úÖ' if recursive_used else '‚ùå'}")
            print(f"   Enhanced used test pattern: {'‚úÖ' if 'test' in pattern_used.lower() else '‚ùå'}")

        accuracy_comparison = "Enhanced BETTER" if enhanced_calls > basic_calls else "EQUAL" if enhanced_calls == basic_calls else "Basic BETTER"
        print(f"   Tool usage accuracy: {accuracy_comparison}")

        return True

    except Exception as e:
        print(f"‚ùå Comparison test failed: {e}")
        return False


def main():
    """Run final integration tests."""
    print("üéØ FINAL ENHANCED TOOLS INTEGRATION TEST")
    print("=" * 60)

    results = []

    # Test 1: Enhanced prompt generation
    try:
        enhanced_prompt = test_enhanced_prompt_generation()
        results.append(("Enhanced prompt generation", True))
    except Exception as e:
        print(f"‚ùå Enhanced prompt test failed: {e}")
        results.append(("Enhanced prompt generation", False))

    # Test 2: Real LLM integration
    llm_result = test_real_llm_with_enhanced_tools()
    results.append(("Real LLM integration", llm_result))

    # Test 3: Enhanced vs basic comparison
    comparison_result = test_comparison_with_basic_tools()
    results.append(("Enhanced vs basic comparison", comparison_result))

    # Final summary
    print(f"\n" + "=" * 60)
    print("üéâ FINAL INTEGRATION TEST SUMMARY")
    print("=" * 60)

    successful_tests = sum(1 for _, success in results if success)
    total_tests = len(results)

    print(f"üìà Success Rate: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")

    for test_name, success in results:
        status = "‚úÖ" if success else "‚ùå"
        print(f"   {status} {test_name}")

    if successful_tests == total_tests:
        print(f"\nüèÜ COMPLETE SUCCESS!")
        print("‚úÖ Enhanced tool system fully operational")
        print("‚úÖ Rich metadata improves LLM tool usage")
        print("‚úÖ Sophisticated tools from legacy system integrated")
        print("‚úÖ Production ready with comprehensive examples and guidance")
    elif successful_tests > 0:
        print(f"\nüéØ PARTIAL SUCCESS!")
        print("‚úÖ Core functionality working")
        print("‚ö†Ô∏è  Some advanced features may need connectivity")
    else:
        print(f"\n‚ùå INTEGRATION ISSUES")
        print("Check connectivity and configuration")

    print("=" * 60)


if __name__ == "__main__":
    main()